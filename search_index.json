[
["index.html", "Statistics with R Colophon", " Statistics with R Douwe Molenaar April 2018 Colophon Course code: X_418156 Coordinator: Douwe Molenaar (d.molenaar@vu.nl), O|2 room 01W09, telephone 020 59 87196. Syllabus version: 2018.02 This syllabus was made using bookdown version 0.7. Some of the exercises in the introductory part were taken from the manual “Using R for scientific computing” by Karline Soetaert (https://www.nioz.nl/en/about/organisation/staff/karline-soetaert). "],
["studyguide.html", "CHAPTER 1 Study guide 1.1 Introduction 1.2 Entry conditions 1.3 Goal of the course 1.4 Workload 1.5 Setup and content of the course", " CHAPTER 1 Study guide 1.1 Introduction In the past decades the development of computers and software has been moving hand in hand with developments in statistics and data analysis as well as techniques of data data generation. Data analysis techniques that were impossible to carry out in the 1980’s, in particular with respect to voluminous data, are available on your PC now. At the same time, not accidentally, high-throughput and high-dimensional (multiplex) measurement technologies were developed in biological research (Schena et al. 1995; G. Gibson 2003). These techniques depend on relatively cheap computers and software to control the instruments and to collect the large amounts of data. This development has stimulated research in statistics and data analysis, leading to a strong inter-dependence between laboratory techniques and data analysis. Together, they have yielded many new insights in biological questions, and are also becoming essential in practical applications like medical diagnostics (Van ’t Veer et al. 2002). The data sets are often large and have a complex structure. The appropriate handling and analysis of such data sets has, therefore, become essential in modern biological research, both in academic and industrial settings. Biology students should learn the potential use of these techniques and should also have the basic skills to handle and analyze the resulting data sets. This course will introduce you to R as a general and flexible tool that can be used to analyze large and complex data sets. Furthermore, as you gain skills and learn to interpret the results in terms of biology, you will experience that data analysis can be fun. And that is the most important message that we want to convey! 1.2 Entry conditions The course “Statistics with R” is intended for students at Master level with backgrounds in Biology, Biomedical Sciences, Bioinformatics, Physics or Mathematics. To follow and complete the course successfully you will need a basic knowledge of statistics. 1.3 Goal of the course The goal of the course is to obtain practical skills in analyzing data sets. The reasons to introduce R are: it is a widely used tool in data analysis, most newly published analytical techniques are available as R-packages and, unlike many available “press button” tools, the use of R requires and stimulates acquisition of knowledge of the statistical concepts behind the analysis. As a bonus you will be come acquainted with some concepts of higher order programming. At the end of the course you should have the following skills: You know and can apply the basic concepts of R You know and can apply a number of “Statistical Learning” techniques You can use R to analyze (biological) data sets You are able interpret the result of the analyses in terms of their (biological) meaning You can write well-documented, re-usable R-code You can write documents explaining the data analyses procedures and results, supported with figures and tables generated in R. 1.3.1 This is not a statistics course Although we do treat the basics of some statistical learning techniques, the course does not aim to give a fundamental mathematical background of the statistical and data-analysis techniques that are used here. We approach these techniques more from a practical point of view, that is when and how do you apply them. You can learn the theoretical background elsewhere, for example in the books by James et al. (2013), Hastie, Tibshirani, and Friedman (2009) and Faraway (2005). The course was born from the need to obtain hands-on experience with data analysis in R. After successfully finishing this course you will be able to perform data analysis in an independent manner during internships or in a job. 1.4 Workload The course corresponds to 6 European Credit (EC) points or a workload of approximately 180 hours and will be given in a period of 8 weeks. 1.5 Setup and content of the course Learning to apply R in boils down to practicing, and practicing a lot. Most of the work that you need to do for this course can be done in any place where you can concentrate. In principle you can work out most of material individually, but in my experience, and I can not stress this enough, you learn faster when interacting with a teacher and peers every now and then. The lectures give a quick and intuitive introduction into a few important technical and statistical concepts The walk-in hours give the opportunity to get individual feedback from the teachers. It is very common to experience hurdles that are easily solved when discussing these with peers or with a teacher. The assignments simulate real-life problems that can be solved with R. They test your practical knowledge of R. 1.5.1 Walk-in hours The purpose of the walk-in hours is to provide individual help and to discuss problems with teachers and peers. If you find yourself in a situation where you have been trying to solve an apparently simple problem (an annoying error message keeps appearing, you can’t get the graph statement to display the right information, etc.) for more than 20 minutes, then you know it’s time to go for help. The locaction of the walk-in hours in the O|2 building also is a pleasant working-space. Alternatively, you can use the online discussion system in Canvas. 1.5.2 Syllabus The syllabus consists of two parts: an introduction to the technical aspects of R and tools to improve your productivity. You should actively carry out the exercises in the syllabus, which means that you should not read the answers before trying hard. The syllabus also contains detailed descriptions of the material, and can be used for self-study. 1.5.3 Assignments Your grade will be determined by assignments to be made individually during the practical course. Important The assignments must be handed in through Canvas. Assignments handed by email will not be considered! The assignments must be handed in before the deadline, see the course schedule in section 1.5.6. In addition to a document in which you describe your conclusions, allways hand in a document with R-code that can be run from Rstudio, i.e. either: an RMarkdown (*.Rmd) file an R-script (*.R) file The assignments and data files will be available on Canvas. 1.5.4 Assessment Assessment will be based on practical assignments. The practical assignments have to be made and handed in individually. The weights of each of these items for the final grade will be: Assignment 1: 25% Assignment 2: 30% Assignment 3: 45% 1.5.5 Evaluation of the course To be able to improve the course, we need your feedback. Therefore, your comments during the course will be highly appreciated. Please also fill in the online evaluation form after the course. 1.5.6 Course schedule Statistics with R, April–May 2018 Coordinator: Douwe Molenaar, d.molenaar@vu.nl Teachers: Chrats Melkonian, Sebastián Mendoza, Eunice van Pelt Walk-in hours: Thursday and Friday 10:00 - 12:00 in room 0E39, ground floor, east side of the O|2 building day date time location subject finish 1 Thu 5-Apr 13:30–15:15 F123 1. The R-language 2 Fri 7-Apr 3 Wed 11-Apr 4 Thu 12-Apr 13:30–15:15 F123 2. Reproducible research 5 Fri 13-Apr Chapter 10 6 Thu 19-Apr 13:30–15:15 F123 3. Linear modelingAssignment 1 7 Fri 20-Apr Assignment 1(deadline 18:00) 8 Wed 25-Apr 9 Thu 26-Apr 13:30–15:15 F123 4. Linear modeling, model selection – Fri 27-Apr King’s day 10 Thu 3-May 13:30–15:15 F123 5. Distributions 11 Fri 4-May 12 Wed 9-May – Thu 10-May Ascension – Fri 11-May University closed 13 Wed 16-May Chapter 17 14 Thu 17-May 13:30–15:15 F123 6. ClassifiersAssignment 2 15 Fri 18-May Assignment 2(deadline 18:00) 16 Thu 24-May 13:30–15:15 F123 7. Classifiers 17 Fri 25-May Chapter 22 18 Wed 30-May Assignment 3 19 Thu 31-May 20 Fri 1-Jun Assignment 3(deadline 18:00) References "],
["quick-start.html", "CHAPTER 2 A quick start 2.1 What is R? 2.2 Resources for learning R 2.3 Other sources on the use of R 2.4 Installation of R 2.5 Starting R 2.6 Obtaining help 2.7 An R-session", " CHAPTER 2 A quick start This manual is supposed to be read in an interactive manner. Do not just read it, but also type the examples in the console window of R. In this way you will learn the commands and structures in R more readily. Creativity begins with copying. 2.1 What is R? In the late 1970’s the S language was developed by John Chambers and colleagues at Bell Labs as a language for programming with data. It combined ideas from a variety of languages to provide an environment for manipulating, statistically analyzing, and visualizing data. S-plus was the commercial version of this language, the plus standing for S-plus-graphics. R was developed as an independent open source implementation of S (R comes before S), originally developed by Ross Ihaka and Robert Gentleman at the University of Auckland in the mid-1990s. It is well known for its use in statistical analysis, but also for is extremely flexible and powerful graphical capabilities. However, R is also a high-level programming language in which one can perform complex calculations, implement new methods, and make high-quality figures. R has high-level functions to operate on matrices, perform numerical integration, advanced statistics, you name it. These properties make it ideally suited for data-visualization, statistical analysis and mathematical modeling. R is maintained and developed by a small team of programmers, called The R Development Core Team. 2.2 Resources for learning R In the following lectures we will make you acquainted with the R language. There are many excellent sources for learning the R language. R comes with several manuals that can be accessed from the R user interface. Many other good introductions to R are available, some freely on the web, and accessible via the R web site (see the contributed documentation under the Manual link on the R-website). Two good ones are by Kuhnert and Venables (https://cran.r-project.org/doc/manuals/R-intro.pdf and https://cran.r-project.org/doc/contrib/Kuhnert+Venables-R_Course_Notes.zip). A nice reference website for R is Quick-R. Some introductory books on the use of R that I can recommend are (Crawley 2007) for general use of R as well as a little statistics, (Dalgaard 2008) for doing statistics with R and (Spector 2008) for data manipulation in R (in the sense of scientifically and legally permitted manipulation). 2.3 Other sources on the use of R Below I listed a few sources for news on R, often with advanced topics, but inspiring for the beginner too. R-bloggers with news, interesting applications, tutorials: (http://www.r-bloggers.com) Cookbook for R Several “R recipes” (http://www.cookbook-r.com) R Journal announcements on new versions of R and new packages from package authors (http://journal.r-project.org) Learning R mostly about making graphs using the R-package ggplot2 (http://learnr.wordpress.com) Advanced R An online book on R by Hadley Wickham: (http://adv-r.had.co.nz). Hard copy also available. R Style Guides You may find different stronger and less strong opinions on coding style. The bottom line is: be consistent. Google’s R coding Style Guide Hadley Wickham’s Style Guide (Chapter from “Advanced R”) Programming Before you dive deep into R-programming, first learn the R language, otherwise it will be a frustrating experience. There are the General Best practices in scientific computing Online version of a book on writing R Packages by Hadley Wickham: (http://r-pkgs.had.co.nz) 2.4 Installation of R You can find the latest binaries of R and the R-source code on the R-project web site: (http://www.r-project.org). Unless you want to compile the executables yourself, you will not need the source code. To install R, click on the CRAN (Comprehensive R Archive Network) link, and choose one of the mirror locations (in the Netherlands for example (http://cran-mirror.cs.uu.nl). Click on the link for your operating system, and follow the instructions. 2.5 Starting R You can now start R in the usual way for your OS. For windows it will be accessible from the START button and will start a simple graphical user interface (R-Gui). For linux and Mac you start a console (Figure 2.1). This is the screen in which you have control over the R-machinery by typing commands to be executed by R. For example, type 1+1 and press the return key. The answer will look like [1] 2. You will learn below why you get a [1] in brackets in front of the answer. Figure 2.1: The R consoles under Linux and Mac (left) and the R-Gui under Windows (right) with an R-console window inside 2.6 Obtaining help This is a good moment to introduce you to the R help system. There are different ways to find help about a topic: By pointing your browser to the help documents. There are two ways to do this: If you are working in the R-Gui, click the menu point “Help|HTML help”. If you work in the console type help.start(). Also take a look at the general manuals available under Help|Manuals. To get information about the standard operators, click the “Packages” link and then the “base” link. “base” is a package that provides, as its name says, the basic functions of R, from primitive functions like addition and subtraction to simple statistical functions. The “base” package is loaded during start-up of R, because you will always need some of its functions. To see these functions, in the browser on the “base” package page, find the entry “Arithmetic”, and click on its link. You will now get the page with information about the arithmetic functions/operators available in R. Also try to find the help page describing the basic R-function called mean. The help pages for functions have a standard layout: Description gives a one-or two sentence description of the function Usage gives brief examples of standard ways to execute the function. Also gives default values of function arguments Arguments gives the type and number of arguments you need to supply to the function Value gives information about the type of data that the function returns Examples can often be found at the end of the page. It contains very useful illustrations of what the function(s) can do. To quickly get to know a function it is often best to copy parts of this section and execute them in the R console From the command line: in the console inside the R-Gui type help(topic), where topic is the R-function about which you want to learn something. For example, if you want to learn something about the mean function, type help(mean). A shortcut is to type the question mark in front of the topic name, so ?mean gives you the same help page as help(mean). Accessing help from the command line gives you basically the same help text as the browser interface. Using the reference card. The R-reference card lists many of the basic R functions and includes brief descriptions. It’s mainly there to quickly find the right command. Complete descriptions of R functions can only be obtained from the original help pages. 2.7 An R-session When you have installed R you can start the R-console by clicking the R-icon, or by choosing it from the START button. You get command line access to the R machinery, a so-called session. The command line is indicated by the so-called “prompt” which looks like &gt;. You can work with the R-console as with a sophisticated calculator. Let’s do some simple operations to get you acquainted with the command line interpreter: At the prompt type 1, and then press the Enter key on your keyboard. Pressing the Enter key is the signal to R to interpret your input. Since your input consisted of the simple command 1, it returns a simple answer, which is 1. You see, however, that R returns something a little more complicated than 1, namely [1] 1. Actually, R returned a vector with one entry, and it reported the position of that entry by the number between brackets. Since the vector is the most primitive data type in R, a vector is what many R functions return as their result. To illustrate this, at the prompt, type rep(1,100). rep is the name of a function in R. The rep() function expects two so-called arguments inside the brackets. The rep function tells R to repeat the entry given by the first argument (1, in this case) for the number of times given by the second argument (100, in this case). After pressing the Enter key, you will see that R returns a vector, now consisting of 100 positions, each position with the value 1. The vector is so large that it doesn’t fit on one line, and R divides it over several lines. Each line starts with a number between brackets which reports the position of the first entry in that line. By the way, the first argument of the rep function can also be a character or a string (two or more concatenated characters). For example, type rep(‘a’,100) and press the Enter key, and then type rep(‘ab’,100), and again press the Enter key. Very often you will want to repeat a command with a slight modification. There is a quick way to do this in the R console: using the \\(\\uparrow\\) and \\(\\downarrow\\) buttons you can scroll through the history of your commands. Use these buttons to retrieve the rep(‘ab’,100) command and, moving in the line using the \\(\\leftarrow\\), \\(\\rightarrow\\) buttons, modify it to rep(‘abc’,100), for example. The modified command will be appended to the end of the command history. Table 2.1: Some of the common and less common mathematical functions in R Function Meaning log(x) log to base e of x exp(x) antilog of x (\\(e^{x}\\)) log(x,n) log to base n of x log10(x) log to base 10 of x sqrt(x) square root of x factorial(x) \\(x!\\) choose(n,x) binomial coefficients \\(\\frac{n!}{(n-x)!x!}\\) floor(x) greatest integer \\(&lt;x\\) ceiling(x) smallest integer \\(&gt;x\\) trunc(x) closest integer to x between x and 0 trunc(1.5)=1, trunc(-1.5)=-1. trunc is like floor for positive values and like ceiling for negative values round(x, digits=0) round the value of x to an integer signif(x, digits=6) give x to 6 digits in scientific notation runif(n) generates n random numbers between 0 and 1 from a uniform distribution cos(x) cosine of x in radians sin(x) sine of x in radians tan(x) tangent of x in radians acos(x), asin(x), atan(x) inverse trigonometric transformations of real or complex numbers acosh(x), asinh(x), atanh(x) inverse hyperbolic trigonometric transformations of real or complex numbers abs(x) the absolute value of x, ignoring the minus sign if there is one R knows all the mathematical operators, functions, and constants which you can find on a standard scientific calculator (and many more, see table 2.1): Type 1+1 at the prompt and press the Return key. You see that R yields the expected answer, again in a vector of size 1. Type 2^2 to calculate the square of 2. To calculate the area of a circular surface with radius 2, type pi*2^2 and press Return. Again, you get the expected answer. You can find the standard operators and functions in the help files which were installed together with R. Use the R command line to calculate the following expressions. Also look up the function definitions for the log() function. Formula Expected value \\((\\frac{4}{6}\\times8-1)^{\\frac{2}{3}}\\) 2.657958 \\(\\ln(20)\\) 2.995732 \\(\\log_{2}(4096)\\) 12 \\(2\\times\\pi\\times3\\) 18.84956 \\(e^{2+\\cos(0.5\\times\\pi)}\\) 7.389056 \\(\\sqrt{2.3^{2}+5.4^{2}-2\\times 2.3 \\times 5.4 \\times \\cos \\left( \\frac{\\pi}{8} \\right) }\\) 3.391288 The session is closed when you exit the program. References "],
["elementary-datatypes.html", "CHAPTER 3 Elementary data types and operations 3.1 The atomic data types 3.2 The basic data object classes 3.3 Assigning names to R-objects 3.4 Arithmetic with vectors, arrays, and data frames 3.5 Extracting and replacing parts of data objects 3.6 Removing elements from objects 3.7 Data class conversion or coercion 3.8 Exercises", " CHAPTER 3 Elementary data types and operations Every higher order programming language (like R, but also C, Perl, Python, etc.) has a classification system for data, a so-called “typing system”. The reason to classify data is that it simplifies the programming of functions that work with those different types of data. Many functions can be applied only to certain types of data. For example, you can calculate the mean of a set of numbers, but not of a set of characters. So, in R the function mean() can only be applied to a set of numbers and not to characters. If you do apply this function to a set of characters R returns a warning message, and the special value called NA, for Not Available (see below). The core of the typing system of R consists of the so-called atomic data types. You can not actually create an object of an atomic data type in R, but you can create vectors filled with atomic data, since a vector is the simplest data object in R, as we will see below. The atomic data types are numeric, character, logical, complex and raw. You will use only the first three of these. 3.1 The atomic data types Numeric This atomic type represents anything that you would in daily life call a number. It also comprises two special numeric values, namely Inf, representing extremely large or infinite numbers and NaN, representing “Not a Number”. Inf (or -Inf) is the result of a calculation leading to a number that is so large that it cannot be represented in R. type 1/0 (or -1/0) at the prompt, and press return. But also 2^2000 will lead to an Inf value. Although it is not infinite, it cannot be represented in computer memory by R, at least not in the usual way. NaN is the result of calculation with an undefined result. For example, type 0/0 at the prompt and press return. Or type 2^2000 - 2^4000. The advantage of these “values” in R is that they prevent your code from getting stuck in calculations leading to such results. Other programming languages will stop when such undefined results occur unless you make explicit programmatic precautions. Character Character type atomic elements can be any single letter or a string of letters. To indicate that your entry is of character type, you put it between single or double quotes, like 'a', or '123', or &quot;a&quot; and &quot;123&quot;. Logical This atomic type can have two values, FALSE and TRUE, and they are the outcome of comparison operators, like &gt;, &lt;, == (smaller than, bigger than, equal to, see the help under the base package for all logical operators, or functions) or logical operators like ! (“not”). At the prompt, type 1 &gt; 2, and then press the Enter key. This asserts that 1 is bigger than 2, and is clearly a false assertion, so the R interpreter returns FALSE, Or type !(1 &gt; 2). Also type TRUE at the prompt and press Enter. You get back a vector of length one with the value TRUE as its single entry. Notice that TRUE and FALSE have to be written in capitals; true and false are not known as logical constants in R. Complex This atomic type represents the complex numbers. We will not use complex numbers in this course. Raw This type holds binary data, i.e. raw bytes. We will not use it. A special value common to all five atomic types is the value NA, which indicates a missing value (Not Available). As you can imagine, NA is a very useful element in the analysis of real-life data, since measurements are very often incomplete. As with Inf and NaN, NA offers a way to not get stuck with missing data when you do calculations on data sets. At the prompt type NA and then press the Enter button. Also try to type N, or A separately. You see that in the latter two cases R complains that it does not know objects with those names. So R recognizes specifically the combination NA. You have seen before that it also recognizes pi. 3.2 The basic data object classes In science data never come alone, and therefore all data in R are minimally represented as a vector, i.e. a tuple of things1. Atomic elements do not exist as individual objects in R. The vector object class in R is actually an abstract type, representing anything that you can think of as a tuple. In R you can not create a vector object directly (see figure 3.1. Therefore, the vector class is called a “virtual class”, which means that you can only create objects of one of the derived, so-called child classes of the vector class. So, if you enter a single number, character, logical value or complex number in the command line, then you will get an object that has numeric, character or logical class and has length 1. Let’s discuss each of the classes in figure 3.1, create objects if possible, and look at the properties. Figure 3.1: Basic R data class hierarchy. This is probably the most important figure of the introductory chapters. Only objects of classes depicted in shaded boxes can be created, meaning that, although the class vector is defined in R, you cannot actually create objects of this class. It is a virtual class, serving to define common properties in its child classes. The dark shaded classes correspond to vectors of the atomic data types. vector A vector object is a tuple of other objects. By “tuple” we mean that each of the sub-objects has a defined and fixed position characterized by a single number, the index-position, in the super-object. The whole range of occupied positions is called the index. Sub-objects can be retrieved from the parent vector object by using a selector operator (explained below). We have already seen a way to construct vectors with several elements, namely by the rep() function. Another important way is to use the function c() (for concatenate or combine). Take a look at the help file for this operator: help(c) or ?c. An important consequence of the hierarchy in figure 3.1 is that all data objects in R, no matter how complex, are derived from the vector class. This is even true for the data objects not listed in the figure. It implies that all data objects in R have vector-like properties. In practice this means that the methods or functions defined for the vector class can be applied to any data object in R. atomic vectors Atomic vectors are vectors consisting of objects of a single atomic type, basically “character”, “logical”, and “numeric”. The atomic type of a vector is called its “mode”. The mode of a vector can be queried with the mode() function. numeric Numeric-mode vectors contain numbers, either integer or floating point numbers. Floating point numbers contain digits behind the decimal point, whereas integers don’t. At the prompt type c(1.1,2.1,3) and press Enter. This will create a numeric-mode vector Type 1:3 and press Enter. What is the function of the : operator? (See help(colon)). Type rep(1:3,3). What is happening here? Take a look at the help page for rep. The c() operator can also be used to concatenate two or more vectors into one vector. To see this effect type c(c(1,2),c(3,4)) and press Enter. integer This is a child class of numeric, containing only integers. You can create an integer-mode vector by using the commands integer(), or as.integer(). Also a command like 1:10 generates an integer class vector. Type mode(1:3) and class(1:3) to find out the mode and class of these vectors. Note that although the class of the numeric child class integer is what its name says, the mode of this child class is “numeric”. The same holds for the child classes factor and ordered, below. factor This is a child class of integer (and thereby of numeric too) that is used for discretely leveled data having a limited number of levels. For example, the variable “gender” has two levels, either “Male” or “Female”, or you could abbreviate them as “M” and “F”. The levels “Male” and “Female” are internally represented as the integers 1 and 2, but displayed with their given names, using a translation table that is an additional attribute of the factor object. Create a factor vector by typing factor(c(‘M’,‘F’,‘M’)). You can use the levels() function to find out which levels a factor has: levels(factor(c(‘M’,‘F’,‘M’))) To see that the representation is numeric, translate the factor-type vector back to a numeric vector by the command as.numeric(factor(c(‘M’,‘F’,‘M’)). The as.numeric() function is a so-called coercion function (to coerce = using force to change something). There are many such functions starting with as.. Above you saw the as.integer() coercion function, which converts vectors to an integer-type vector. ordered This is a child class of factor (and thereby of integer and numeric). Whereas for the “gender” factor in the previous example there is no order relation between the “M” and “F” elements (they have equivalent rank), you can imagine that a factor registering the size of things, and having for example the discrete levels “small”, “medium”, “large”, the order relation between these levels is meaningful. The ordered data class registers not only the levels, but also this order relation. To create an ordered factor, type ordered(c(‘large’, ‘medium’, ‘small’, ‘large’), levels=c(‘small’, ‘medium’, ‘large’)), where the position in the “levels” argument defines the order relation between the levels. character A character-mode vector contains character entries consisting of strings of one or more letters. Type c(‘a’,‘b’,‘c’) and press Enter. Type c(‘abc’,1,22) and press Enter. At first, you might think that this should generate an error because we have entered two different atomic types (numeric and character), whereas a vector can have only one type of atomic elements. However, R converts the numeric type of the second and third positions to a character type. This is again an example of coercion, but now automatically performed. You will see that it has created a character vector with three strings, being ‘abc’, ‘1’ and ‘22’. Can you see how R indicates that the numeric elements were coerced to character elements? Type rep(c(‘A’,‘B’),c(2,3)). What is happening here? Type rep(c(‘A’,‘B’),each=3). What happens here? logical A logical-mode vector contains only logical entries, i.e. the special values TRUE or FALSE or NA. Type c(TRUE, FALSE, NA). Type c(1,2,NA) and press Enter. Also try c(1, 2, TRUE), c(1, 2, FALSE), and c(‘a’, ‘b’, TRUE). To what are the logical elements coerced? The logical value NA, which indicates a missing value, has a special behavior in coercion, in that it is not changed to a character type or numeric type where the other logical values (TRUE and FALSE) are coerced. Again, if you want to find out which type of atomic elements your vector contains you can use the function mode(). Type, for example mode(c(1,2,3)), and it will give you numeric as the answer. list A list is a vector of R-objects. list objects inherit vector-like behavior from the parent vector class. In contrast to atomic vectors, however, lists are very flexible data objects because they can contain non-atomic objects as their entries, and can also contain different types of objects at different positions. So, lists can contain vectors, matrices, data frames and lists and any mixture of these. Type list(1,‘a’), and press Enter. R prints the list on the screen, each entry indicated by its index position (the double bracketed number, like [[1]] for the first position) and its value, in this case a numeric vector of length 1 at position [[1]] and a character vector of length 1 at index position [[2]] Type list(‘John’, ‘Doe’, c(‘Carpenter’, ‘Teacher’), 34, ‘M’) and press Enter. This illustrates that lists can be very handy to keep different types of related data together, in this case information about a person. For example, we could agree that index 1 gives the first name, index 2 the last name, index 3 the professions that the person has had, index 4 the age of the person , and index 5 gives the gender of the person. We could even make a small database of persons by putting several different “person”-lists in a list again. Think about this data object for a while, and realize that it consists of a set of five vectors of which the first three have the mode character the fourth the mode numeric, and the last the mode character and they have lengths 1, 1, 2, 1, and 1 respectively Having to remember the index position where a specific piece of information can be found is not very user-friendly. Therefore, R offers the possibility of labeling index positions with names. To illustrate this, type list(firstname=‘John’, lastname=‘Doe’, professions=c(‘Carpenter’, ‘Teacher’), age=34, gender=‘M’). Press Enter, and you will see that in the screen print of the list, the index numbers have been replaced by the labels, preceded by a dollar sign. We will see later how to retrieve data from lists and other objects using such named index positions. data frame This is a very important R data object that corresponds to the notion of a table. You can see in figure 3.1 that data frame objects inherit from the list class. The behavior that it inherits from the list class is that it can contain different types of elements at the index positions. However, there are restrictions relative to the parent class: all elements in a data frame must have the same length and they have to be atomic vectors. You can see why this corresponds to a table, because each row in a table represents an instance of a set (a person, for example, as an instance of “human being”), and each column represents some type of data about those instances (age, height, gender, etc.). The columns correspond to the data frame entries, and each has to have the same number of rows. Often, tables are made in other programs, for example in Excel, and then imported in R in a data frame object for analyses. Let’s make a data frame with the data.frame() function (notice the dot): type data.frame(age = c(10, 45, 20, 31), height = c(1.24, 1.78, 1.68, 1.66), gender = factor(c(‘M’, ‘M’, ‘F’, ‘F’))) and press Enter. Take some time to think carefully about the structure of this data frame. It is a vector with three elements. The elements in their turn each contain a vector of length 4, two of which have mode numeric, and one has mode character. As said, a data frame is less flexible than a list, because the lengths of the columns (the list elements) have to be equal and atomic. So, something like different persons having different number of “professions”, as in the list above, is not very easy to implement in a data frame. On the other hand, statistical calculations on data frames are often more easily performed. array An array consists of an atomic vector that contains the array’s elements, together with a dimension property. The number of dimensions in an array are unlimited, but often you will work with two-dimensional arrays which in R have a special name (and data sub-type class, see figure 3.1), namely matrix. The fact that the elements of an array are contained in a vector (let’s call it the “content vector”) implies that all elements in an array or matrix are of the same atomic type, and that the mode of an array equals the mode of its content vector. Find the help page for the array function for creating arrays in R Type array(1:10, dim=c(2,5)) and press Enter. What do the first and second integer in the dim argument correspond to? Find the help page for the matrix() function for creating a matrix in R Type matrix(1:10, nrow=2) and press Enter Make a 3-dimensional 2 x 2 x 2 array with the elements 1:8 3.3 Assigning names to R-objects You have just used R as a calculator. As with a calculator, the data that you have typed above were volatile, meaning that after printing, they are gone. Re-creating data every time you want to do some calculation cannot be the purpose of a high-level programming language. And so, R offers the possibility of storing data in memory. The way to do that is by attaching a name to data. This process is called “assignment”, because you assign a name to a data object. During this assignment, space is made available in memory to store the data. Assignment occurs through the assignment operator &lt;-. Try the following: At the prompt type v, and press Return. R complains about not knowing v. At the prompt type v &lt;- c(1,2,3). press Return. No complaints! Now type v and press Return. This possibility of naming and storing makes handling of the data much more efficient. We will demonstrate this in the next section. By the way, there are two equivalent versions of the assignment operator, namely &lt;- and =. I prefer the &lt;- operator, because due to its asymmetry it shows explicitly which is the name and which is the data. But remember that v &lt;- c(1,2,3) and v = c(1,2,3) are equivalent. 3.3.1 Listing objects in memory If you lose track of all the objects that you have put into the memory of our PC, there are two useful commands that give an overview of those objects. They are the functions ls() and its variant ls.str(). Type these commands at the prompt now, and don’t forget to type the empty brackets behind them. The ls() command only displays the names of the objects currently in memory, whereas ls.str() is much more informative about the object: it shows class, size, a few values, etc. In case of complex objects like data frames it also shows information about the individual sub-objects. 3.4 Arithmetic with vectors, arrays, and data frames Perhaps you know vector arithmetic from linear algebra. In R you can execute vector arithmetic too, like inner, outer, and cross products etc., but this is not the subject of this chapter. This chapter shows the effect of applying simple unary or binary operators like taking the square root (unary) or addition, multiplication etc. (binary) to vectors. These operators are applied element-by-element on the vector or vectors. Such operations may not be formally defined in linear algebra, and are a typical part of R (This is part of an important subject called “vectorization” which is covered in chapter 8). make the vector a by typing a &lt;- c(1,2,3). Then type a/2, and study the result Every element in a was divided by 2! The same operation with the same effect, dividing by a scalar, exists in linear algebra. However, the element by element procedure will also be applied for any other operator, like the unary function sqrt(). Type sqrt(a) and look at the result. This operation does not formally exist in linear algebra. You can also apply binary operators (like *, /, etc.) to pairs of vectors having lengths greater than 1. The result of these operations do not correspond to their equivalents (if they exist) in linear algebra. For example, you can multiply two vectors of the same length element by element. Make a new vector b &lt;- c(4,5,6) and then type a*b You will see that the result is a vector of the same length which is the result of multiplying a and b element by element, which does not correspond to the inner or outer product from linear algebra. Also make a new vector c &lt;- c(4,5) and type a * c. This yields a warning message: “longer object length is not a multiple of shorter object length”. However, a result is produced. The message actually suggests that if the length of the longer object (a) were a multiple of that of the shorter, the operation would not have issued a warning. To see this, make a vector d &lt;- c(1,2,3,4) and type d * c The result is the element by element multiplication of d by the re-cycled vector c. In the former case, a * c, the shorter vector c is also recycled, but only partially in the last cycle. Check this assertion! Notice that recycling will never give a warning when one of the vectors has length 1 (is a 1-tuple or singleton). The single scalar element is recycled for every element of the longer vector. These are very handy properties of R vectors, as you will notice in the examples following below. Now you will also understand the advantage of having the Inf and NaN numeric values, and the NA value: type a &lt;- c(1,2,3) and b &lt;- c(2,0,3) at the prompt. Then type a/b. Division of the elements at the second index position (2/0) leads to an infinite value, but the other two divisions are perfectly valid. In R such a calculation will not lead to an error message and halting of the program, as it would in most other programming languages. It will just perform the operations on the elements for which it is valid, and fills in the appropriate special result (Inf, NaN, NA) at the positions for which it led to invalid answers. The following example shows that the individual columns in data frames behave as vectors. Type d &lt;- data.frame(inhabitants=c(1000,200000,500), area=c(200,5000,NA)). which lists the number of inhabitants and area (in square kilometers) of three imaginary countries. We do not know the area of the third country because it wasn’t provided. At the prompt press d to see whether you have typed in the correct data. Now we want to calculate the population density in each of these countries. First, let’s retrieve the inhabitants column from the data frame. To do that, type d$inhabitants and press return. The dollar operator $ in this case selects the element (column) with the name “inhabitants” from the data frame, and returns it as a vector. Now you may find out for yourself (and it’s obvious) how to calculate the population density of each of these countries. The answer looks like [1] 5 40 NA. Can you make an additional column in d called “density” that contains the calculated result? Hint: use the $ selector operator to create the new column and to assign the density result to it Arrays (and 2-dimensional arrays, which are called matrices in R) are generalizations of vectors. As stated above, an array in R is defined as a vector with a dimension attribute (2 x 2, 2 x 2 x 4, etc.). If you apply arithmetic operators to an array then you actually apply them to the vector part of the array. As an illustration, type a &lt;- array(1:8, dim=c(2,4)) and press return. As you see, the values have been filled in column-wise, filling the first column from top to bottom, then the second, etc. Now multiply a with b (equal to c(4,5)). This will not issue a warning because the length of the vector part of a equals 8, a multiple of 2, and b can be recycled completely. This results in the upper row of a being multiplied by 4 and the lower row by 5, since recycling of b takes place in the same order as the array was filled. Difficult? Just work it out by hand on a piece of paper. You can also multiply, divide, take roots etc. with arrays in this way, since the operators are applied to the vector parts, regardless of the dimension properties. By the way, matrix multiplication (from linear algebra) is also possible in R. Look up what the the matrix multiplication operator looks like in R (Hint: search with the term “crossproduct”. Note that there are two ways to calculate it). 3.5 Extracting and replacing parts of data objects Very often, you want to select a part of an object to do calculations with. You already saw how to select a whole column from a data frame using the dollar ($) operator. We know that every data object inherits from the vector class (see figure 3.1). The basic way to extract a single sub-object from an element of any data object, is to use the double bracketed index selector operator [[...]]. This selector operator accepts a single index position (a positive integer) or a single name of an index position as an argument. We will illustrate the selection of a single sub-object now. More help on this subject can be found in the R help pages: type help(Extract). 3.5.1 Extracting a single sub-object with the double bracket index selector [[ Make a vector a &lt;- c(1,2,3,4). Select the second element using a[[2]]. To find out the class of this extracted part, type class(a[[2]]). Does it differ from the class of the original object? Make a list v &lt;- list(firstname=‘John’, lastname=‘Doe’, professions=c(‘Carpenter’, ‘Teacher’), age=34, gender=‘M’). Select its third sub-object using the command v[[3]]. What is the class of v and what is the class of v[[3]]? Also select the third sub-object by name, using the command v[[‘professions’]]. Why do we use the quotes and not plainly v[[professions]] (study the error message when you type the last command)? 3.5.2 For data frames and lists $ and [[ selectors behave similarly Above, we have seen the use of the dollar selector to select single named columns from a data frame. In data frames the columns correspond to its sub-elements. Therefore, the use of the dollar selector for data frames is identical to the use of the double bracket selector when using a named index. To illustrate this: Type d &lt;- data.frame(inhabitants=c(1000,200000,500), area=c(200,5000,NA)). Then type d$area and d[['area']] and study the type of objects they return. Also select the second column of d using the [[ selector and a numerical index A difference between the $ and [[ selectors is that, in case of the [[ selector you can enter a variable holding the name, whereas this does not work for the dollar selector: Type mycolumn &lt;- 'inhabitants'. Now try to select the inhabitants column from d using the $ and [[ selectors on the variable mycolumn (Why would using quotes around the variable name yield the wrong result here?). This behavior of the [[ selector can be very handy if the column to be selected from a data frame is the result of some calculation. The behavior with respect to $ and [[ selectors of the data frame class is actually inherited from the list class, its parent class (see figure 3.1). Therefore, the things said here about the data frame class also hold for the list class. 3.5.3 Selecting portions with the single bracket selector [ Another way to select a portion of an object, and not necessarily sub-objects, is by using the single bracket selector [...]. Selection using numeric index vectors You can fill in single integers or vectors of integers between the single brackets to select portions of an object: Make a vector a &lt;- c(‘a’,‘b’,‘c’,‘d’). Select the second element using a[[2]]. Select the second until fourth elements using a[2:4] (remember 2:4 is identical to c(2,3,4)). Try to do the same using the double bracket selector: a[[2:4]]. What is the error message? Select the fourth and first element, in that order, using a[c(4,1)]. What are the classes of the portions that you have selected from a? The selection vectors c(2), 2:4, c(1,4) etc. are called “numeric index vector” if they are used to select elements from another vector. Apart from this name, there is nothing special about them! Make a list v &lt;- list(firstname=‘John’, lastname=‘Doe’, professions=c(‘Carpenter’, ‘Teacher’), age=34, gender=‘M’) Select the portion consisting of its third element using the command v[3]. What is the class of v and what is the class of v[3]? Do you notice the difference with the double bracket selector [[? Select the portion of v consisting of its third and second elements using the single bracket operator. What is the class of this selection? Try to perform nested selection (remember the error message from a[[2:4]]?) using the double bracketed command v[[c(3,2)]]. Do you understand what is happening here? Remember that you can only select single sub-objects with the double bracket selector. What is the class of this selected object? Also try v[c(3,2)]. What is the difference with the previous command? What is the class of the selected portion? Selection using logical index vectors You can also use logical vectors (vectors containing TRUE and FALSE elements) to select elements from another vector. These vectors are then called logical index vectors. Make a vector a &lt;- 1:4 and select the first and fourth element using a[c(TRUE, FALSE, FALSE, TRUE)]. Logical vectors really come in handy if they are the result of a calculation. Type a&gt;2, and look at the result. Now select all elements from a that are larger than 2 using a[a&gt;2]. Select all elements from a that are larger than 1 and smaller than 4 using a calculated logical index vector (use the logical and operator: &amp;). Select all elements from a that are smaller than 2 or larger than 3 using a calculated logical index vector (use the logical or operator: |). Selection using the complement of an index vector You can use index selection to select the complement of a set of numerical indices using complement syntax: Select the second and fourth element from a by typing a[-c(1,3)] This is of course especially handy if you have a very long vector from which you want to extract all but a few elements. Selection using index names The single bracket selector also works with named indices. Select “firstname” and “lastname” from the list object v defined above using the [ selector and a character vector with the names of these sub-objects. What is the class of the selected portion. Can you think of a way to create a character vector containing first and last names using a bracket selector? Create a data frame d &lt;- data.frame(age = c(10, 45, 20), height = c(1.24, 1.78, 1.68), sex = factor(c(‘M’, ‘M’, ‘F’))). Now select the “age” and “sex” columns using the [ selector and named selection. What type of object is the output of this selection? 3.5.4 Changing parts of a vector Index selection also works if you want to change parts of an object, i.e. assign different values to parts of an object. For this, you use a combination of the selection and assignment operators. Set the third and fourth element of a to be 5 by typing a[3:4] &lt;- 5. Look at the resulting a. This syntax actually invokes a separate so-called replacement function named [&lt;-, see help(Extract). 3.5.5 Effect of the selector operators on data frames Make a data frame d &lt;- data.frame(age=c(20,30,40), weight=c(55,81,76)). Select the weight column with the $ operator. Select the weight column using the [ operator. Try both using the numeric index of that column, or the name of its index position. Select the weight column using the [[ operator, either with the numeric index of that column, or with its name string. What is the difference with the [ operator? (tip: use the class() function to find out what type of object is coming out of these operators) 3.5.6 Effect of the selector operators on arrays On arrays the index selection operators work a little different: Make an array a &lt;- array(1:9, dim=c(3,3)) Remember, an array in R is represented by a vector with a dimension attribute. So, you can select single elements or ranges of elements from an array using the [] operator exactly as from a vector, completely ignoring its dimension property. Try a[1] and a[2:4], a[c(1,3)]. An ordered, comma-separated list of vectors (each with 1 or more elements) within [] is interpreted as a selection from each of the dimensions of the array to which it is applied. So you can select the element in the second row and third column by the command a[2,3]. You can select the elements in the first and third column of the third row by typing a[3,c(1,3)]. And you can select the entire third row by the command a[3,]. Of course, the number of vectors in the list between [] brackets should exactly equal the number of dimensions in a. By the way, you can use the same “dimension” trick to a data frame. Select the third row of d from the previous paragraph by the command d[3,]. What is the class of the selected part? 3.6 Removing elements from objects Sometimes you have to remove elements from an object because subsequent data analysis requires it. Removing elements from a list or data frame can be done by using a selector and assigning the value NULL to the selected elements. Elements can be selected with any of the three types (numeric, logical, character) of index vectors. Make a list v &lt;- list(name=‘John’, children=c(‘John jr.’, ‘Nancy’)). Remove the “children” element by typing v[[‘children’]] &lt;- NULL. Now examine v again Make a data frame d &lt;- data.frame(age=c(20,30,40), weight=c(55,81,76)). Remove the “weight” column by typing d$weight &lt;- NULL. Examine d again. In vectors and arrays this “nullification” doesn’t work. In this case you can use, for example, the complement syntax within a selector operator: Make a vector a &lt;- 1:4. Type a[-c(2,3)] and see what you get. Now remove the second and third element from a by reassigning the previous result to a: a &lt;- a[-c(2,3)]. Reexamine a. Suppose you have a logical index vector having a TRUE value for the elements that you would like to remove. For example, you want to remove all elements smaller or equal to 2 using the logical index vector resulting from a &lt;= 2. Remove these elements from a using assignment, the logical index vector, and the logical not operator !. Which other logical operation (apart from a &lt;= 2) could you have used to do this? In arrays you can remove entire (ranges of) rows and columns. Make an array a &lt;- array(1:9, dim=c(3,3)). Remove the second row using complement syntax: a &lt;- a[-2,]. You could also have used a &lt;- a[c(1,3),]. 3.7 Data class conversion or coercion Sometimes it is necessary to convert objects of one type to another. The following examples demonstrate the use of functions that do this: Make a vector d &lt;- c(‘male’,‘female’,‘male’,‘male’). Which mode does this vector have? Suppose you realize that these gender indications should be organized in a factor-type vector. You can simply convert the character-type vector to a factor-type vector by using the function as.factor(): d &lt;- as.factor(d). Make a vector a &lt;- 1:4. Which mode does this vector have? Suppose you want to change it to a character vector. This can be done using a &lt;- as.character(a). Make a logical vector o &lt;- c(FALSE,TRUE,FALSE). Change it to a numeric vector by using as.numeric(o). How are the logical TRUE and FALSE elements translated? Suppose you have a logical vector (here test a short example: c(TRUE, FALSE, FALSE, TRUE)), and want to know the number of times that it contains TRUE. Use the sum() function to calculate this number. Use the logical negation operator ! (i.e. not) to calculate the number of FALSE elements. This is again an example of automatic coercion. The function sum() expects a numeric vector. Before caclulating the sum, the as.numeric() coercion function is automatically applied if the input is not numeric. 3.8 Exercises Selection Create a vector a &lt;- 1:100. Select its 20 to 25-th elements using the [ selector. Find out what type of object this subselection is by applying the function class() to the sub-selection. Show solution a &lt;- 1:100 a[20:25] class(a) Select the elements from vector a that are smaller than 20, using a logical index vector. Show solution a[a&lt;20] Select the even elements from a using a logical index vector that you calculate using one of the functions ceiling(), round(), or trunc(). Show solution a[round(a/2)==a/2] Create a variable n &lt;- 200. Then create a data frame: d &lt;- data.frame(age = round(12 + 70*runif(n)), gender = factor(ifelse(runif(n)&gt;0.5, &#39;male&#39;, &#39;female&#39;)), smoker = ifelse(runif(n)&gt;0.8, TRUE, FALSE)) Look at its structure and, from their help pages, find out what the functions runif() and ifelse() did. What are the minimum and maximum ages possible using this data construction method? Show solution Make a sub-selection from d containing the columns “gender” and “smoker” using the [ selector. What is the class of the object created by this selection? Show solution subselection &lt;- d[c(&#39;gender&#39;,&#39;smoker&#39;)] Select all rows from d containing women. All columns should be included. What type of object is the result? Show solution d[d$gender==&#39;female&#39;, ] Select all rows from d containing women younger than 50 that smoke. Show solution d[d$gender==&#39;female&#39; &amp; d$smoker==TRUE &amp; d$age&lt;50, ] In the previous question you probably have used the syntax d$smoker==TRUE, but you could also just have used d$smoker without the logical == operator and TRUE argument. Try it. Why does this work? Which two options do you have if you want to collect all non-smokers? Show solution d[d$gender==&#39;female&#39; &amp; d$smoker &amp; d$age&lt;50, ] # to collect all smokers: d[d$smoker==FALSE, ] # or d[!d$smoker, ] Select the “smoker” column from d using the $ selector, the [ selector with a numeric index vector and the [ selector with an index vector of names (here only one name). For all cases find out what class of object the selected part is. Show solution smokers1 &lt;- d$smoker smokers2 &lt;- d[3] smokers3 &lt;- d[&#39;smoker&#39;] class(smokers1) class(smokers2) class(smokers3) Applying a logical function and the function length() on the vector representing the smoker column from d, find out how many smokers there are. Show solution length(d$smoker[d$smoker]) # or length(d$smoker[d$smoker==TRUE]) Many basic R-functions automatically try to coerce input that is not of the right type to the type they expect. For example the sum() function expects numeric type. If you give it a logical vector as input then it automatically first applies as.numeric() to this vector (see section 3.7 for the effect that this conversion has on a logical vector). Use this property to re-calculate the number of smokers using a logical statement and the sum() function. Show solution sum(d$smoker) # or sum(d$smoker==TRUE) # Since sum() implicitly performs as.numeric() on its argument, the sum is taken of as.numeric(d$smoker) Find out what the minimum and maximum ages are. Also find out the mean and median ages. Use the functions min(), max(), range(), and mean() and median(). Show solution min(d$age) max(d$age) range(d$age) mean(d$age) median(d$age) Many R-objects have their own summary() function encoded with the object. Apply the summary() function to d. Are your previous calculations correct? Show solution summary(d) Functions having specific implementations for a class of objects are called class methods. So, many object classes have their own summary() method, but for example also their own plot() method. Try plot(d). The plot method for data frames makes by default a pairplot of all variables (columns) against all others (available for other objects too as the graphical function pairs()). In this case not very insightful, but in case of a data frame with multiple numeric columns, it often gives immediate insight in the correlations. Try, for example to plot the standard iris data set: plot(iris). Using the complement syntax (3.5.3), select the rows 11-100 from d. You will have to use the c() function too. Show solution d[-c(1:10,101:200),] Create a factor with three levels: servedsteaks &lt;- factor(c(&#39;rare&#39;,&#39;medium&#39;,&#39;done&#39;)[trunc(runif(30)*3)+1]) Now create a new vector steaks1 selecting only the elements with “rare” and “medium” in servedsteaks. What is the most efficient way to do this? Show solution steaks1 &lt;- servedsteaks[servedsteaks != &#39;done&#39;] Query the levels of steaks1 using the levels() function. If you did the right thing in the previous question, you will see that the new factor still encodes the level “done” although it is not present in the vector anymore. Find out how you can most efficiently drop the unused levels from a factor: In R versions 2.12 or above there is a function droplevels especially made for this. In lower versions (but also above 2.12) you can use the “drop” argument of the [ selector method for objects of the factor class. Type help('[.factor') to retrieve the help page for this specific method. Show solution levels(steaks1) # yields: [1] &quot;done&quot; &quot;medium&quot; &quot;rare&quot;&quot; steaks1 &lt;- droplevels(steaks1) # help for the &#39;[ function shows we can use the &#39;drop&#39; argument ?&#39;[&#39; steaks2 &lt;- servedsteaks[servedsteaks != &#39;done&#39;, drop=TRUE] levels(steaks2) Create a list of lists: flintstones &lt;- list(person1=list(name=&#39;Fred&#39;, kids=c(&#39;Pebbles&#39;), married=TRUE, pets=c(&#39;Dino&#39;,&#39;Baby Puss&#39;,&#39;Doozy&#39;)), person2=list(name=&#39;Barney&#39;, kids=&#39;Bamm-Bamm&#39;, married=TRUE, pets=&#39;Hoppy&#39;)) Select person2 using the [ and the [[ operators, both with numeric and named index vectors. What are the classes of the selected data objects (they should be the same class)? Can you understand why the objects are, nevertheless, different? Show solution person2.singlebracket &lt;- flintstones[&#39;person2&#39;] person2.doublebracket &lt;- flintstones[[&#39;person2&#39;]] class(person2.singlebracket) class(person2.doublebracket) These are different because the single bracket selector takes a container from the original list, whereas the double bracket selector extracts the object out of the container. Actually, the [, [[, and $ operators are just functions which are recognized with a special syntax. But the normal function syntax (functionname(argument1, argument2, …)) is recognized for these operators too. Try '['(flintstones, 'person2'), '[['(flintstones, 'person2'), and '$'('[['(flintstones, 'person2'), 'kids') (notice the quotes around the names of the functions). Random draws, set operations Make a vector d containing the numbers 101…200. Use the sample() function to take a random sample of size 50 from d, and put that sample in the vector s. What do you get if you just type sample(d)? Also look it up in the help function: this selection has a special name. Use the %in% and logical not ! functions to select all remaining numbers from d that are not in s and put those in the vector t. Verify, using Set Operations from R (see help files), that the intersection of sets s and t equals the empty set (\\(s\\cap t=\\oslash\\) ) and that their union equals the set d (\\(s\\cup t=d\\)). You might want to use the function all(). Show solution d &lt;- 101:200 s &lt;- sample(d, 50) # sample(d) gives a random permutation of the original vector d t &lt;- d[!(d %in% s)] length(intersect(t,s)) == 0 all(d %in% union(t,s)) More selection, vectors and sequences Vector v Create a vector, called v, with even numbers between 16 and 56, using the function seq(). Display this vector. What is the sum of all elements of v? Display the first 4 elements of v Calculate the product of the first 4 elements of v Display the 4th, 9th and 11th element of v using the c() function. Show solution v &lt;- seq(from=16, to=56, by=2) sum(v) v[1:4] prod(v[1:4]) v[c(4,9,11)] vector w Create a new vector, w, which is equal to v multiplied by 3; display its content. How many elements of w are smaller than 100? Create two solutions using either the length() or the sum() function. Create a vector that contains the values (1, 1/2, 1/3, 1/4, …, 1/10) without typing each of the entries. (use the seq() or : functions as well as element-by-element vector division) Compute the square root of each element of this vector Compute the square of each element Compute the sum of all squared elements. Leonhard Euler proved, by solving the Basel problem, that this sum converges to \\(\\frac{\\pi^2}{6}\\). Can you support this numerically? Create a sequence with values (0/1, 1/2, 2/3, 3/4, …, 9/10) Show solution w &lt;- 3*v w sum(w&lt;100) # or length(w[w&lt;100]) x &lt;- 1/1:10 sqrt(x) x^2 sum(x^2) sum(x^2)/(pi^2/6) # = 0.942 sum((1/1:100)^2)/(pi^2/6) # = 0.994 sum((1/1:1000)^2)/(pi^2/6) # = 0.9994: This ratio seems to approach 1 which means that # the sum 1/x^2 x -&gt; inf possibly approaches pi^2/6. 0:9/1:10 vector u Create a vector, u, with 100 random numbers, uniformly distributed between -1 and 1. Check the range of u using the range() function; all values should be between -1 and +1. Calculate the sum and the product of the elements of u. How many elements of u are positive? Make all negative values of u equal to 0. Sort u. Show solution u &lt;- runif(100, min=-1, max=1) range(u) sum(u) prod(u) sum(u &gt;= 0) u[u&lt;0] &lt;- 0 sort(u) vectors x, y Create two vectors: vector x, with the elements: 2,9,0,2,7,4,0 and vector y with the elements 3,5,0,2,5,4,6 (in that order). Divide all the elements of y by the elements of x. Type in the following commands; try to understand: x&gt;y x==0 (Why can’t you use x=0? Or better: what does x=0 do?) Select all values of y that are larger than the corresponding values of x Select all values of y for which the corresponding values of x are 0. Remove all values of y for which the corresponding values of x equal 0. Make all elements of x that are larger or equal than 7 equal to 0. Display x. Show solution x &lt;- c(2,9,0,2,7,4,0) y &lt;- c(3,5,0,2,5,4,6) y/x x&gt;y # logical vector with number of index positions equal to x and y x==0 # logical vector x=0 would be an assignment y[y&gt;x] y[x==0] y[x!=0] x[x &gt;= 7] &lt;- 0 x Matrices Use function matrix() to create a matrix with the following contents: \\[ \\left[ \\begin{array}{cc} 3 &amp; 9\\\\ 7 &amp; 4 \\end{array} \\right] \\] Display it to the screen. Use function matrix() to create a matrix called A (you do not have to type each entry individually): \\[ \\left[ \\begin{array}{ccc} 1 &amp; \\frac{1}{2} &amp; \\frac{1}{3}\\\\ \\frac{1}{4} &amp; \\frac{1}{5} &amp; \\frac{1}{6}\\\\ \\frac{1}{7} &amp; \\frac{1}{8} &amp; \\frac{1}{9} \\end{array} \\right] \\] Take the transpose of A. Create a new matrix, B, by extracting the first two rows and first two columns of A. Display it to the screen. Use function diag() to create the following matrix, called D: \\[ \\left[ \\begin{array}{ccc} 1 &amp; 0 &amp; 0\\\\ 0 &amp; 2 &amp; 0\\\\ 0 &amp; 0 &amp; 3 \\end{array} \\right] \\] Use functions cbind() and rbind() to augment this matrix, such that you obtain: \\[ \\left[ \\begin{array}{cccc} 1 &amp; 0 &amp; 0 &amp; 4\\\\ 0 &amp; 2 &amp; 0 &amp; 4\\\\ 0 &amp; 0 &amp; 3 &amp; 4\\\\ 5 &amp; 5 &amp; 5 &amp; 5 \\end{array} \\right] \\] It is simplest to do this in two statements (but it can be done in one!) Remove the second row and second column of the previous matrix. Show solution Internally, matrices in R are represented as vectors with additional dimension attribute. Type attributes(A) or dim(A) to show its dim attribute and as.vector(A) to show its content vector. Do you see that the content vector runs column by column? selecting the first three elements of the content vector by A[1:3] gives you the entries of the first column. In an tuple every element is uniquely associated with an index position. This in contrast to a set which is just a bag of things in which the order of elements doesn’t matter.↩ "],
["interacting-with-r.html", "CHAPTER 4 Interacting with R 4.1 Controlling R from a script 4.2 Other editors 4.3 Working with packages 4.4 Where your packages are installed 4.5 Reproducible research 4.6 Easy organizing with RStudio 4.7 Displaying information about your R session 4.8 Citing R and R-packages in reports and papers 4.9 Exercise: diversity of deep-sea nematodes", " CHAPTER 4 Interacting with R 4.1 Controlling R from a script There are two basic ways to tell R what you want it to do: by typing in commands one by one in the R console as we did in chapter 2, or by writing a script that can contain multiple commands and send the whole script or parts of the script to R for interpretation. If you want to be able to save your work, which is highly desirable for future reference or for repeating the same procedures on different data, the easiest way to control R is by writing a script and by saving this. The MS-Windows R-GUI (Graphical User Interface) contains a very simple editor, which, as far as editing properties are concerned is not much more sophisticated than notepad. You invoke this editor from the R-GUI menu by choosing “File|New script”. 4.2 Other editors The Windows R-GUI internal editor is a very simple editor. There are several other more sophisticated editors, with syntax highlighting and all kinds of other handy stuff making writing and managing scripts much easier. The best development in this area was the creation of the RStudio IDE (Integrated Development Environment, http://www.rstudio.com). It is currently the best editor for data analysis, scripting, reporting, as well as package development in R, and it is available for all major operating systems. In section 4.6 you will find some additional information about the possibilities of this IDE. If you program a lot in other languages too, then you might be using eclipse as an editing and managing environment. You can use this editor for R as well if you think of serious programming in R (meaning lots of files and package construction). The eclipse plug-in for R called “StatET” can be found at http://www.walware.de/it/statet. Using eclipse to the full benefit and configuring it for use with R is much more complex than using RStudio and it only pays off when you are already using eclipse. RStudio is the right choice for ordinary data analysis tasks, even for the advanced programmer! 4.3 Working with packages Scientists inventing new or more efficient data analysis methods often make these available for the scientific community by creating an R-package. An R-package is a coherent set of functions and new data structure definitions (classes) that are developed to solve a specific class of problems. By this mechanism you can use other people’s work by installing their packages on your computer. Since almost every new statistical tool is quickly implemented in R, either by the scientists themselves or by some other enthusiastic R-programmer, these tools are at your disposal much earlier than with any commercial software package for statistical analyses. An example is the novel correlation measure called “distance covariance” (Székely and Rizzo 2009). It was implemented by the authors in the R-package “energy” concomitantly with their publication. Suppose you have a problem that you want to solve, and you suspect that other people might have solved a similar problem before. You wonder whether anybody wrote an R-package for the problem. To find this package, if it exists, go to the CRAN website (Comprehensive R Archive Network, http://cran.r-project.org/mirrors.html). Choose one of the mirror sites and on the left hand menu click on the “Search” link. Type a few keywords in the search field and see whether you can find a package that meets your needs (notice that above the search field alternative search methods are given). Another way is to follow to the link “Contributed extension packages”. You will then get a full list of available packages. If you have found one or more suitable packages, you have to download and install them. You can do that from the console command line. Suppose you want to install packages with the names “pkg1” and “pkg2” as well as its dependencies. “Dependencies” are other packages which a package that you want to install needs to function properly. Then this command can be used to install all those packages: install.packages(c(&quot;pkg1&quot;,&quot;pkg2&quot;), dependencies=TRUE) Packages can also be managed and installed from RStudio, and this is probably the preferred way when you start working with Rstudio. In the background Rstudio will call the same command as we did above. However Rstudio presents a nice little menu with options. The menu can be started from the lower right panel under the “Packages” tab, by pressing the “Install Packages” button. You should know in advance the names of the packages that you want to install. Of course, to use the new packages in an R-session you will first have to call them into that session with the library() or require() functions: library(&quot;pkg1&quot;) require(&quot;pkg2&quot;) The dependencies will be called in automatically with these calls. The difference between library() and require() is merely cosmetic and only visible if they fail to locate a package. Whereas require() returns FALSE and issues a warning message upon this event, library() will halt and issue an error statement. 4.4 Where your packages are installed If you are the administrator of the PC that you work on you can follow the normal procedure for package installation. Otherwise you may have to tell R that it has to install (and read) packages from a directory that you have write-rights for. However, before you do that, you should check whether a default user package directory is already defined. In the R-console type Sys.getenv(&quot;R_LIBS_USER&quot;) If this yields a path like “~/R/x86_64-pc-linux-gnu-library/3.4” then that will be the directory where packages that you install will be stored. R will automatically also search in this directory if you try to load a package in an R-session. You can just install packages as described in the previous section. If the previous commands did not yield a directory then you can make a subdirectory “Rlibraries” in your home directory. First create that directory. In case your working directory is located on a network, please make sure that your home directory maps to a virtual drive (“U:&quot;, for example), and not to some pointer like”////file/…“, because R will not understand the latter syntax. Then, in the R console type the following commands: lib.dir &lt;- choose.dir() This will give you a pop-up screen. Select the newly created “Rlibraries” directory in it. Then type: .libPaths(lib.dir) This command adds the newly created directory to the search path for R libraries. Type .libPaths() to check whether the new path is indeed present in the set of library search paths. Now you can follow the normal procedure for package installation. The new package will be installed in your “Rlibraries” directory. After closing R, your addition to the library search path is lost. So, if you install a library in a non-standard directory you will have to add it to the search path again after re-starting R. To do that, just use the previous two commands again. Alternatively, you can specify the location of a non-standard library tree with the lib.loc parameter when attempting to load it; see the help page for the library() and require() functions. 4.5 Reproducible research Experimental research should be reproducible, otherwise the results can not be trusted. Reproducibility is a very important issue in computational sciences and data analysis as well, as demonstrated by the increasing attention for “Reproducible Research” (Peng 2011; Ince, Hatton, and Graham-Cumming 2012). The “Reproducible Research” concept is born from the fact that many computer scientists or data analysts experience how difficult it can be to reproduce the results of others, or even their own results. To improve this situation there are a few rules and tools that help making data analysis reproducible: Organize files and folders in a fixed manner Document your R-code with comments Make an analysis document with extensive comments and “literate programming” Provide a bibliography Mention versions of software used (R, package versions) Save data sources in an open format and protect files from mutation Make generic code (that can be executed on other machines, other data, test data) 4.5.1 Staying organized The goal of organizing your work is to be able to find, interpret and reproduce your results easily. Keeping all files of a project together in a properly named folder is one of basic and most easy ways to stay organized (figure 4.1). One of the ideas in reproducible research is to weave executable code and explanatory text together in the final report or paper of the work, using so-called “literate programming”. Literate programming means that you mix programming code with extensive descriptions, or even with the text of the final report. This guarantees availability of code used for calculations to any future reader of the document. For readability, the document containing code and text can be converted to a nicely typeset document in which the output of the code is presented as numbers, tables, figures etc. To facilitate this process, a number of computational tools have been created, also for R. One of the modern tools for literate programming in R is the R-package “knitr”. An extensive listing of these tools can be found on the CRAN website (https://cran.r-project.org/web/views/ReproducibleResearch.html). Figure 4.1: One way to organize your project directory, in this case using Rstudio. The R-Markdown source documents (.Rmd extension) and their output (.HTML in this case) are kept in the root folder of the project. The ‘scripts’ folder holds scripts not present in the R-Markdown file as well as those sourced by the R-Markdown file as external scripts (see below). The ‘output data’ folder holds all file output from the scripts. Keep the source data in read-only mode, if possible. Of course, if you want to reproduce results, you also need to know which versions of programs and packages were used to obtain earlier results. And, if you have tried different analyses, or change something after publishing or reporting you also need different “frozen” versions of your analysis. Let’s define a few desirable features for organizing work with R: Work should be gathered in folders that correspond to well-defined projects. In a project folder we should keep: A copy of the original raw data as well as sample annotations etc. in a format readable using R functions. The raw data files preferably have a read-only status. The R scripts with comments (Comments are essential: you will be surprised how quickly you forget why you did certain things). Documents describing the interpretation of the analysis, and in which output from R, like figures and tables, is integrated. We should also note the version numbers of R and packages that were used for an analysis. In case of complex and long-term projects we should keep frozen versions of R-scripts and other documents. With these rules we should land somewhere right of the center of the “reproducibility spectrum” as depicted in figure 4.2 Figure 4.2: The reproducibility spectrum for computational sciences. Reproduced from Peng (2011) 4.6 Easy organizing with RStudio RStudio has several mechanisms built in that help you keeping the requirements just defined above. In RStudio the natural way to organize your work is in Projects. A project has an associated base folder, the workdirectory (see Chapter 6) in which documents, scripts and data can be organized in subfolders, if desired. RStudio has a few options available for weaving R code and R output with documentation together in a source document. Such a source document can be converted to different final output formats, like HTML (web-page like), PDF, or Word. These documents (source and output) are perfect for saving your analysis and for communicating about it with your colleagues. RStudio provides an interface for two popular versioning systems, the older “Subversion” and the highly flexible distributed versioning system “Git” (http://git-scm.com). For either tool you need a working installation (subversion or git) on your OS. There are standard packages available for most operating systems, including Windows. You should always use the first two possibilities. The third, Git, is an advanced feature that pays back especially in group projects, and when the focus lies on programming rather than data analysis. Figure 4.3: An example of an R Markdown document. It is a simple text file with markup elements that are instructions for document formatting. It is interspersed with R code chunks between the triple backtick delimiters ```, displayed in grey in Rstudio. The elements between the curly brackets in the code chunks are parameters for the knitr package. See http://yihui.name/knitr/options for a list of knitr options. Rmarkdown documents R-Markdown documents are a type of source document in which code and text can be weaved together. Using R-Markdown syntax is one of the easiest ways to make source documents. R-Markdown documents are text documents with Markup commands that that can be interpreted as instructions for output document formatting. For example, a single hash in front of a text, # My Title, will produce the text that looks like a title (large font, etc.). You can find other markup commands under the menu ‘Help | Markdown Quick Reference’. To start an R-Markdown document in RStudio, click on File | New | R Markdown. A new example Markdown document pops up (figure 4.3), to which you can make changes. To generate the HTML output you save the RMarkdown file (in your project folder) and click on the “knit” button. The functions from the knitr package will execute the R code parts and generate HTML output from the R-output (figure 4.4). They will also translate the Markdown commands to HTML tags, and weave this together with the R-output to a valid HTML document. Figure 4.4: The HTML output of the default R Markdown document. Other types of output for R-Markdown documents are PDF and Word. The output type can be chosen from the dropdown menu under the “knitr” button. The translation of an R-Markdown file (extension “Rmd”) into one of these final formatted documents is done in a two-step process (figure 4.5). In the first step, the code chunks in the document are executed, figures are made, and the output is translated into Markdown code, and produces a Markdown (extension “Md”) file. For example, when the code produced a figure, that figure is stored in a (temporary) file and at the original position of the R-code, Markdown commands are placed that import the figure. In the second step, the program Pandoc translates the Markdown file to a final output file. Figure 4.5: The two-step processing of an R-Markdown document called ‘analysis.Rmd’ to a HTML, PDF or other format. The Knitr package takes care of the execution of R-code (in blue) and translation of its output to Markdown syntax (pink). Pandoc creates a formatted document (HTML or PDF) from a Markdown document. Another type of source document is the “R Sweave” type. R-sweave documents are just LaTeX documents intercalated with R-code. The second phase of the processing in figure 4.5 is taken care of by the latex program. LaTeX markup syntax is much more complex than Markdown. The main reason to use Sweave in some cases is that it allows much more flexibility in the output format. In principle all the flexibility of LaTeX itself. In this course you should use R-Markdown, unless you are already proficient in LaTeX and insist on using that system. LaTeX markup is much more prone to the introduction of errors, and the generation of output, particularly the analysis of errors, can be tedious if you are not an experienced user. It is a bad idea to combine the goal of learning of R with that of learning LaTeX. Even for experienced LaTeX users it may not be a good idea to make life more difficult. Using LaTeX does not yield extra points in this course … Externalizing code It is not always convenient to experiment with code in a Markdown or Sweave document, because you need to generate the document to see the effects of code changes, and R-error messages are difficult to retrieve. That is why I often put the R-code in a separate document (code externalization), and include that R-code via the read_chunk() function from the knitr package in the Markdown or Sweave document. The procedure is explained on the knitr website. It also has some clarifying demo files. 4.7 Displaying information about your R session Although most R-code and package writers take care to maintain backward compatibility of algorithms, it can sometimes happen that a script that previously worked suddenly fails. In many cases this is due to deprecation of a function. In such a case you might want to restore the original environment. You can in principle run multiple versions of R in parallel on the same PC. You will need to print version numbers in your original report though. A simple way to do this is by using the sessionInfo() function. sessionInfo() ## R version 3.4.3 (2017-11-30) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 16.04.4 LTS ## ## Matrix products: default ## BLAS: /usr/lib/openblas-base/libblas.so.3 ## LAPACK: /usr/lib/libopenblasp-r0.2.18.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] Rcpp_0.12.16 rstudioapi_0.7 knitr_1.20 magrittr_1.5 ## [5] munsell_0.4.3 colorspace_1.3-2 rlang_0.2.0 highr_0.6 ## [9] stringr_1.3.0 plyr_1.8.4 tools_3.4.3 grid_3.4.3 ## [13] gtable_0.2.0 xfun_0.1 png_0.1-7 htmltools_0.3.6 ## [17] yaml_2.1.18 lazyeval_0.2.1 rprojroot_1.3-2 digest_0.6.15 ## [21] tibble_1.4.2 bookdown_0.7 ggplot2_2.2.1 evaluate_0.10.1 ## [25] rmarkdown_1.9 stringi_1.1.7 compiler_3.4.3 pillar_1.2.2 ## [29] scales_0.5.0 backports_1.1.2 The first two lines show the R version number and the platform on which it was run. The locale section shows local settings of the platform. The attached base packages and other attached packages show which packages from the base installation and custom packages are loaded and attached in the session (attached means that you can access their functions from the console. The loaded via a namespace (and not attached) section shows which packages make functions available to attached packages without being attached themselves, which means that their functions can not be directly accessed from the console. 4.8 Citing R and R-packages in reports and papers It is important to cite the use of R and of specific packages when writing a report or a paper. First of all, The R Development Core Team and the package programmers have invested a huge amount of work and we, the R user community, should pay them credit for it. Second, others have to be able to reproduce your data analyses, and therefore have to know exactly which tools you used. To obtain general citation details for the use of R use the command citation(): citation() ## ## To cite R in publications use: ## ## R Core Team (2017). R: A language and environment for ## statistical computing. R Foundation for Statistical Computing, ## Vienna, Austria. URL https://www.R-project.org/. ## ## A BibTeX entry for LaTeX users is ## ## @Manual{, ## title = {R: A Language and Environment for Statistical Computing}, ## author = {{R Core Team}}, ## organization = {R Foundation for Statistical Computing}, ## address = {Vienna, Austria}, ## year = {2017}, ## url = {https://www.R-project.org/}, ## } ## ## We have invested a lot of time and effort in creating R, please ## cite it when using it for data analysis. See also ## &#39;citation(&quot;pkgname&quot;)&#39; for citing R packages. Citation details for specific packages can usually be found in the package description in its help pages. For many packages the citation() function also works. For example for the nlme package: citation(&quot;nlme&quot;) ## ## Pinheiro J, Bates D, DebRoy S, Sarkar D and R Core Team (2018). ## _nlme: Linear and Nonlinear Mixed Effects Models_. R package ## version 3.1-137, &lt;URL: https://CRAN.R-project.org/package=nlme&gt;. ## ## A BibTeX entry for LaTeX users is ## ## @Manual{, ## title = {{nlme}: Linear and Nonlinear Mixed Effects Models}, ## author = {Jose Pinheiro and Douglas Bates and Saikat DebRoy and Deepayan Sarkar and {R Core Team}}, ## year = {2018}, ## note = {R package version 3.1-137}, ## url = {https://CRAN.R-project.org/package=nlme}, ## } 4.9 Exercise: diversity of deep-sea nematodes For this exercise you should use Rstudio. Open Rstudio and become acquainted with its 4 panels: The upper left panel is the script panel, where you write R-scripts. You can execute parts of this code by selecting it and pressing the “Run” button. The lower left panel is a console, where parts of your script are executed, You can also directly enter commands in the console. The upper right screen is the workspace management area. it shows the objects in the workspace, and it allows you to delete specific objects The lower right panel is an interface to the file system, the help system, package management, as well as an area where plots are displayed. Also find the help-pages for Rstudio on the internet. Before starting the exercise, make a project directory where you will keep your script and data (menu “File | New Project”). We will work on a data set consisting of nematode species densities, found in Mediterranean deep-sea sediments, at depths ranging from 160 m to 1220 m. The densities are expressed in number of individuals per 10 cm2. Nematodes are small (&lt; 1 mm) worms, and they are highly abundant in all marine sediments. The data (from Soetaert, Heip, and Vincx (1991)) have been deposited as a table in a comma-separated-values (csv) format on the course website: http://www.few.vu.nl/~molenaar/courses/statR/data/nemaspec. Download the file “nemaspec.csv” to a “source data” subdirectory of your project directory and open it in RStudio. Check its structure. You may also open the file in Excel, but do not forget to close it before proceeding. Excel is very territorial, and will not allow another program, such as R, to access a file that it is open in itself. On the first line is the heading (the names of the stations), the first column contains the species names. Before importing the file in R change the working directory (see chapter 6 for the concept of working directory). Make sure that your project directory is open on the Files tab in the lower right panel in Rstudio. Then set this to the working directory by pressing the “More” button and choosing “Set As Working Directory”. In the console you should see that Rstudio has sent a command like the following to the console: setwd(&#39;C:/Rwork/nematodes&#39;) Now open a new R-script tab in Rstudio, and at the top write a few comments: Title and purpose of the script, date, your name, R-version etc. Note that in an R-script, everything one a line behind a # is regarded as a comment. Now save the script as an R-script to the wording directory. Execute the following commands, submitting each line from the script to R to check its correctness. Read the comma-delimited file, using R-command read.csv. Type ?read.csv for help (you will learn more about file-IO in chapter 6). Specify that the first row of the data file is the heading (header=TRUE) and that the first column contains the row names (row.names=1). Put the data in data frame Nemaspec. Nemaspec &lt;- read.csv(&quot;nemaspec.csv&quot;, header=TRUE, row.names=1) Check the contents of Nemaspec, for example by clicking on its name in the Workspace management area. You can also display part of it in the console using the head() function: head(Nemaspec). head(Nemaspec) ## M160a M160b M280a M280b M530a M530b ## Acantholaimus 0 6.580261 0.000000 1.120782 1.315487 1.727387 ## Acantholaimus elegans 0 0.000000 1.439706 0.000000 3.956836 0.000000 ## Acantholaimus iubilus 0 0.000000 0.000000 0.000000 0.000000 1.193131 ## Acantholaimus M1 0 5.919719 0.000000 3.628518 0.000000 1.193131 ## Acantholaimus M10 0 0.000000 0.000000 1.120782 0.000000 0.000000 ## Acantholaimus M11 0 0.000000 0.000000 0.000000 0.000000 0.000000 ## M820a M820b M990a M990b M1220a ## Acantholaimus 3.417313 3.748096 2.447545 3.728838 4.369345 ## Acantholaimus elegans 0.000000 2.198407 5.080900 5.330997 3.644567 ## Acantholaimus iubilus 0.000000 0.000000 1.270450 1.372789 0.000000 ## Acantholaimus M1 1.155307 0.000000 5.052036 6.225598 0.000000 ## Acantholaimus M10 0.000000 0.000000 0.000000 0.000000 1.115981 ## Acantholaimus M11 0.000000 0.000000 0.000000 2.285694 0.000000 ## M1220b ## Acantholaimus 4.787512 ## Acantholaimus elegans 3.494481 ## Acantholaimus iubilus 0.000000 ## Acantholaimus M1 1.166825 ## Acantholaimus M10 0.000000 ## Acantholaimus M11 0.000000 The rest is up to you, but do not forget to save your script, because you will need it later in an exercise to make modifications and do additional calculations: Select the data from station M160b (the 2nd column of Nemaspec); put these data in a vector dens. (Remember: to select a complete column, you select all rows by leaving the first index blank, or you can use the $ operator). Show solution dens &lt;- Nemaspec$M160b Remove from vector dens, the densities that are equal to 0 (tip: you could use the logical operator != (not equal), or a combination of the logical == (equal) and ! (not) operators). Display this vector on the screen. Show solution dens &lt;- dens[dens!=0] Calculate \\(N\\), the total nematode density of this station. The total density is simply the sum of all species densities (i.e. the sum of values in vector dens). What is the value of \\(N\\) ? (Answer :699). Show solution N &lt;- sum(dens) Divide the values in vector dens by the total nematode density \\(N\\). Put the results in vector p, which now contains the relative proportions for all species. The sum of all values in p should now equal 1. Check that. Show solution p &lt;- dens/N Calculate \\(S\\), the number of species: this is simply the length of p or dens; call this value \\(S\\). (Answer: 126) Show solution S &lt;- length(p) Estimate the values of diversity indices \\(N1\\) and \\(N2\\) and \\(N3\\), given by the following formulas: \\[ \\begin{align*} N1 &amp;= -\\sum p_{i}\\cdot\\ln(p_{i}) &amp; &amp; \\text{Shannon-Wiener index} \\\\ N2 &amp;= \\frac{1}{\\sum p_{i}^{2}} &amp; &amp; \\text{Inverse Simpson index} \\\\ N3 &amp;= \\frac{1}{max(p_{i})} &amp; &amp; \\text{Renyi diversity of infinite order} \\end{align*} \\] where \\(\\sum\\) means summation over the index \\(i\\). You can calculate each of these values using only one R statement! (Answers: N1=4.502, N2=66.78, N3=22.56). Show solution N1 &lt;- -sum(p*log(p)) N2 &lt;- 1/sum(p*p) N3 &lt;- 1/max(p) The 126 nematode species per 10 cm2 were obtained by looking at all 699 individuals. Of course, the fewer individuals are determined, the fewer species will be encountered. Some researchers determine 100 individuals, other 200 individuals. To standardize their results, the expected number of species in a sample can be recalculated based on a common number of individuals. The expected number of species in a sample of size \\(n\\), drawn from a total population of size \\(N\\), which has \\(S\\) species is given by \\[ ES(n) = \\sum_{i=1}^{S} \\left[ 1 - \\frac{ \\binom{N-N_{i}}{n}}{\\binom{N}{n}} \\right] \\] where \\(N_{i}\\) is the number of individuals in the i-th species in the full sample and \\(\\binom{N}{n}\\) is the so-called “binomial coefficient”, the number of different sets with size n that can be chosen from a set with total size N. In R, binomial coefficients are calculated with the statement choose(N,n). What is the expected number of species per 100 individuals? (\\(n\\)=100, \\(N\\)=699, Answer: ES(100)=60.69) Show solution ES &lt;- sum(1-choose(N-dens,100)/choose(N,100)) Put all the diversity indices (N, S, N1, N2, N3, ES) in a vector called diversity, name its elements using the names() function, and print it to the screen. It should look like: ## N S N1 N2 N3 ES ## 699.000000 126.000000 4.501515 66.778414 22.561569 60.689713 Show solution diversity &lt;- c(N,S,N1,N2,N3,ES) names(diversity) &lt;- c(&#39;N&#39;,&#39;S&#39;,&#39;N1&#39;,&#39;N2&#39;,&#39;N3&#39;,&#39;ES&#39;) References "],
["graphics.html", "CHAPTER 5 Graphics 5.1 The basics of R graphics 5.2 X-Y plots 5.3 Histograms 5.4 Boxplots 5.5 Images and contour plots 5.6 Plotting a mathematical function 5.7 Multiple figures in one graphical device 5.8 Saving graphs 5.9 Different graphical systems and packages 5.10 Exercises", " CHAPTER 5 Graphics The importance of intelligent and comprehensible graphical representation of data can not be overestimated. In the exploratory phase of data analysis it is often essential to get an impression of data quality, like bias, experimental error etc. but also of putative relations between variables that you might want to investigate. After analyzing data and hypothesis testing, a clear presentation of the evidence is important. A graph can be much more convincing than just a table with p-values. In contrast to tables of numbers, a good plot can be interpreted in a few seconds, and appeals to the “common sense” feeling of the reader. Perhaps even more importantly, graphs can show properties of data that are difficult to describe in a few words. An example, originating from the book “Introductory statistics with R” by Dalgaard (2008), is shown below. The amount of the vitamin folate was measured in red blood cells after incubation of the cells under different “ventilation” conditions: “N2O and O2 for 24 hours”, “N2O and O2 during operation”, and “only O2 for 24 hours”. The question was whether there was a significant difference between any of the treatments, and if so, between which of the treatments. This question is typically solved by a one-way ANOVA (see Chapter 12). The outcome of this analysis (an F-test) showed that, indeed, there was a significant difference with p-value 0.044. To find out where the difference lies, a pairwise t-test with adjustment for multiple testing was performed: pairwise.t.test(red.cell.folate$folate, red.cell.folate$ventilation) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: red.cell.folate$folate and red.cell.folate$ventilation ## ## N2O+O2,24h N2O+O2,op ## N2O+O2,op 0.042 - ## O2,24h 0.310 0.408 ## ## P value adjustment method: holm This showed that the difference lies in the treatments “N2O and O2 for 24 hours” and “N2O and O2 during operation”. You might leave it with that, but the following graphical representation, a so-called boxplot, is quite illustrative of a weak spot in this analysis. boxplot(folate~ventilation, data=red.cell.folate, col=&#39;grey&#39;, las=2) Although it suggests that there might be a difference between the groups, it shows that the assumption implicitly underlying the previous ANOVA and t-tests, namely equal variance between the groups, is perhaps not justified. The hight of the boxes seems to differ! So, the tests have to be re-done without this assumption. The outcome of a pairwise t-test would then actually reject the alternative hypothesis! pairwise.t.test(red.cell.folate$folate, red.cell.folate$ventilation, pool.sd=FALSE) ## ## Pairwise comparisons using t tests with non-pooled SD ## ## data: red.cell.folate$folate and red.cell.folate$ventilation ## ## N2O+O2,24h N2O+O2,op ## N2O+O2,op 0.087 - ## O2,24h 0.321 0.321 ## ## P value adjustment method: holm To conclude, you have seen that a single well-chosen graph tells much more than p-values from a few statistical tests. 5.1 The basics of R graphics One of the great joys of R is its graphical capabilities. Many people use R to generate high-quality custom graphs for publication purposes, because of the incredible flexibility and high-quality output in the main graphical formats. You can control almost any aspect of graphical output. And often you don’t have to go into the details of graphical output, because many complex graph types can be created with single commands. Take a tour of the graphical capabilities by starting the demos and examples of a number of graphics packages and commands: demo(graphics) demo(image) demo(persp) example(plot) example(pairs) example(hist) Graphics are plotted in the figure window which floats independently from the other windows. Such a window is called a graphical device, and it is opened or refreshed automatically by the high-level plotting commands listed below. You can also launch one or more graphical windows manually by writing: windows() or the equivalent command on Mac and linux operating systems X11() This is convenient if you want to have multiple graphical windows on a screen, for example to compare plots. When you make a figure, it is plotted in a graphical device. As said, this can be a special window on your computer screen, but it can also be a file with a specific graphical format, like a PDF or JPEG file. A figure consists of a plot region surrounded by 4 margins, which are numbered clockwise, from 1 to 4, starting from the bottom. R distinguishes between: high-level commands. By default, these create a new figure, e.g. hist, barplot, pie, boxplot, … (1-D plot) plot, curve, matplot, pairs, … ((x-y)plots) image, contour, filled.contour, … (2-D surface plots) persp, scatterplot3d (3-D plots) low-level commands that add new objects to an existing figure, e.g. lines, points, segments, polygon, rect, text, arrows, legend, abline, locator, rug, … These add objects within the plot region box, axis, mtext (text in margin), title, … which add objects in the plot margin graphical arguments that control the appearance of plotting objects: cex (size of text and symbols), col (colors), font, las (axis label orientation), lty (line type), lwd (line width), pch (the symbol type), … graphic window: mar (the size of the margins), mfrow (the number of figures on a row), mfcol(number figures on a column), … To obtain some idea of the possibilities you should look at the help files: ?plot.default ?par ?plot.window ?points or the examples (to go through the examples, press enter): example(plot.default) example(points) 5.2 X-Y plots Figure 5.1: The three iris species from the iris dataset. Petals are the three smaller leafs in the flower, sepals are the three larger leafs. In many flowering plants sepals are green and support the flower leafs. In iris species they are coloured. To demonstrate the different standard plot types, we will use the famous “iris” data set, which is available at start-up of R. It was introduced by the statistician Ronald A. Fisher. More information on this data set can be obtained from the help page: ?iris. It gives measurements, in centimeters, for lengths and widths of sepals and petals of 50 flowers of each of the three species. Type head(iris) to see the first 6 entries of the data frame. ?iris gives a description of the data set (or see the Wikipedia Lemma with photographs of the iris species: http://en.wikipedia.org/wiki/Iris_flower_data_set). Let us make an (x,y) plot of two of the four measured flower dimensions, namely Petal.Length and Sepal.Length. plot(x=iris$Petal.Length, y=iris$Sepal.Length) As plot is a high-level command, it starts a new figure. By default, R adds axes, and labels, and represents the (x,y) data as open circles. Although it is acceptable for data-gazing, you shouldn’t want to publish such a figure in a report or paper. We will change a few things by changing the default values of a number of arguments of the plot() function. Rather than open circles, the points should be something else (change function argument pch, for point character. See example(points) for a list of symbols). The x- and y-axes labels should have proper text (change function arguments xlab, ylab) The units of all measurements were cm. Hence, we may want the axes to have equal metrics, i.e. the \\(x/y\\) aspect ratio should equal \\(1\\) (function argument asp). plot(x=iris$Petal.Length, y=iris$Sepal.Length, pch=19, asp=1, xlab=&quot;Petal length (cm)&quot;, ylab=&quot;Sepal length (cm)&quot;) To this figure, we can now add low-level objects. Note that low-level commands only work on an open graphical device. For example, using linear regression, we have calculated that a straight line \\(y=a+bx\\), with \\(a=4.31\\) and \\(b=0.41\\) fits the data (we will learn later how to perform such a regression). We use the lines() function to draw a line, and we make it a red line (col) with line-width 2 (lwd): plot(x=iris$Petal.Length, y=iris$Sepal.Length, pch=19, asp=1, xlab=&quot;Petal length (cm)&quot;, ylab=&quot;Sepal length (cm)&quot;) lines(x=c(1,7),y=c(4.31+0.41*1, 4.31+0.41*7), lwd=2, col=&#39;red&#39;) Now, we would also like to indicate the correspondence between iris species and the position in the graph, for example by using different symbol types or colors for the different species. This means that the plot symbol or color of the symbol should be made conditional on the iris$Species factor variable. For the application here, it is essential to realize again that factors are stored as integers (remember that the factor class is a subclass of the integer class), starting from \\(1\\). In the R-statement below, we simply use different symbols (pch) and colors (col) for each species: pch=(15:17)[iris$Species] means that, depending on the value of iris$Species (i.e. the species), the symbol (pch) will take on the value 1 (first species = setosa), 2 (second species = versicolor) and 3 (third species = virginica). col=(1:3)[iris$Species] does the same for the point color. The final statement adds a legend, positioned at the top left. See the reference card for a display of the available graphical symbols. plot(x=iris$Petal.Length, y=iris$Sepal.Length, pch=(15:17)[iris$Species], asp=1, xlab=&quot;Petal length (cm)&quot;, ylab=&quot;Sepal length (cm)&quot;, col=c(&#39;red&#39;,&#39;green&#39;,&#39;blue&#39;)[iris$Species]) lines(x=c(1,7),y=c(4.31+0.41*1, 4.31+0.41*7), lwd=2, col=&#39;red&#39;) legend(1.2,8,pch=15:17, col=c(&#39;red&#39;,&#39;green&#39;,&#39;blue&#39;), legend=levels(iris$Species)) 5.3 Histograms Histograms are used to display distributions of variables. In R we can use the hist() function for making histograms. This function calculates by default a suitable, equally-spaced binning of the data, and calculates frequencies of the data per bin. Let us display the distribution of the iris petal length measurements, using the default settings: h &lt;- hist(iris$Petal.Length, xlab=&quot;Petal length (cm)&quot;) Note that we use the assignment function in this plotting statement. This is not necessary for generating the plot, but it allows us to examine and re-use the results of the binning calculations performed by the hist() function. The plot is a so-called “side-effect” of the hist() function. In fact, plots are always side-effects of some calculation, i.e. the output of plotting functions itself is always an R-object, although for many, like plot() this is just NULL, and its side-effect is actually its main purpose. To see the output of the hist() function in the calculation above, just type h in the console window and press return. The modifications to the default arguments you will most often make are a change in the number of bins (nclass), the variable displayed on the y-axis (either relative frequency or absolute counts), and perhaps the coloring or hatching (using the density argument, a confusing name I admit) of the bars. Like this: layout(matrix(1:2,nrow=1)) hist(iris$Petal.Length, xlab=&quot;Petal length (cm)&quot;, col=&quot;lightblue&quot;, main=&quot;&quot;, probability=TRUE) hist(iris$Petal.Length, xlab=&quot;Petal length (cm)&quot;, density=10, main=&quot;&quot;, probability=FALSE) The hist() function has somewhat limited capabilities. It is, for example, not easy to overlay multiple histograms in one plot. If you need such capabilities it is better to use another graphical package, like the versatile graphical package “ggplot2” (see section 5.9). In the first plot the bars for different species falling at the same position were stacked, and in the second they were or overlayed with semi-transparent colors. Side-by side of bars would have been another option (by setting the position=&quot;dodge&quot; parameter in the geom_histogram() function). # require(ggplot2) # Note that the &quot;multiplot&quot; function used below has to be defined by the user (you) first. # This function puts multiple ggplot functions in one graph. It can be obtained from # http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2) p &lt;- ggplot(iris, aes(x=Petal.Length, fill=Species)) p1 &lt;- p + geom_histogram(position=&quot;stack&quot;, binwidth=0.5) + xlab(&quot;Petal length (cm)&quot;) p2 &lt;- p + geom_histogram(position=&quot;identity&quot;, binwidth=0.5, alpha=0.65) + xlab(&quot;Petal length (cm)&quot;) multiplot(p1, p2, cols=2) ## Loading required package: grid 5.4 Boxplots Boxplots are used to plot continuous data as a function of categorical data. They are very useful in multivariate data analysis as you saw in the example at the beginning of this chapter, where a boxplot was used as a graphical support for a one-way ANOVA. Even if you decide not to publish the boxplot, it is always a good idea to use it for examining the data. For example, we could analyze the distribution of petal lengths in the three different iris species. We will use an R-formula in the boxplot function. The formula syntax is discussed in chapter 10. The R-formula Petal.Length ~ Species in the boxplot command can be interpreted as “(plot) the variable Petal.Length as a function of the variable Species”. We saw before that Petal.Length is a continuous variable and Species is a categorical variable, represented by a factor object in R: boxplot(Petal.Length~Species, data=iris, col=&#39;grey&#39;, xlab=&#39;Iris species&#39;, ylab=&#39;Petal length&#39;) Based on this plot you might suspect that there is a significant difference between the petal lengths in the different species, a hypothesis that could be tested using a one-way ANOVA. However, you also notice from this plot that the variance of the petal length in the three species is probably different, a condition that is not compatible with a traditional ANOVA. You could now easily try some data transformations to find one that could meet the equal-variance condition. For example, a logarithmic transform of the petal lengths might be a good choice. We can simply modify the previous R-formula to make this plot: boxplot(log(Petal.Length)~Species, data=iris, col=&#39;grey&#39;, xlab=&#39;Iris species&#39;, ylab=&#39;log(Petal length)&#39;) which looks already better from an “equal variance” perspective. You should read the help file of the boxplot command to know what the different graphical elements, like the whiskers and boxes in the boxplot represent, or how to change the default plot behavior. 5.5 Images and contour plots R has some very nice functions to create images and contour plots. For example, the data set “Bathymetry” from the “marelac” package can be used to generate the bathymetry (and hypsometry) of the world oceans (and land). Before you run this example, you need to instal the “marelac” package. # require(marelac) image(Bathymetry$x, Bathymetry$y, Bathymetry$z, col=femmecol(100), asp=TRUE, xlab=&quot;&quot;, ylab=&quot;&quot;) contour(Bathymetry$x, Bathymetry$y, Bathymetry$z, add=TRUE) Note the use of asp=TRUE, which maintains the aspect ratio, meaning that x- and y- distances have the same scale on the graphic device. 5.6 Plotting a mathematical function Plot curves for mathematical functions are generated with R-command curve: curve(sin(3*pi*x)) will plot the curve for \\(y=sin(3\\pi x)\\), using the default settings (figure left), while: curve(sin(3*pi*x), from=0, to=2, col=&quot;blue&quot;,xlab=&quot;x&quot;, ylab=&quot;f(x)&quot;, main=&quot;curve&quot;) curve(cos(3*pi*x), add=TRUE, col=&quot;red&quot;, lty=2) first draws a graph of \\(y=sin(3\\pi x)\\), in blue (col), and for x values ranging between 0 and 2 (from, to), adding a main title (main) and x- and y-axis labels (xlab, ylab) (1st sentence). The 2nd R-sentence adds the function \\(y=cos(3\\pi x)\\), as a red (col) dashed line (lty). Note the use of parameter add=TRUE, as by default curve creates a new plot. The final statements adds the x-axis, i.e. a horizontal, dashed (lty=2), line (abline) at \\(y=0\\) and a legend. abline(h=0,lty=2) legend(&quot;bottomleft&quot;, c(&quot;sin&quot;,&quot;cos&quot;),text.col=c(&quot;blue&quot;,&quot;red&quot;),lty=1:2) 5.7 Multiple figures in one graphical device There are several ways in which to arrange multiple figures on a plot. The simplest is by specifying the number of figures on a row (mfrow) and on a column (mfcol): par(mfrow=c(3,2)) will arrange the next plots in 3 rows, 2 columns. Graphs will be plotted row-wise. par(mfcol=c(3,2)) will arrange the plots in 3 columns, 2 rows, in a column-wise sequence. Note that both mfrow and mfcol must be inputted as a vector. Try: par(mfrow=c(2,2)) for (i in 1:4) curve(sin(i*pi*x),0,1,main=i) The R-function layout() allows more complex plot arrangements. 5.8 Saving graphs Using the R console, you will obtain separate graphics windows when plotting a graph. When a graphics window is active (you have to click on it to become active) the main menu will look different. On the File menu there are options available for saving a graph in formats like PNG (portable network graphics), BMP (bitmap), JPEG (joint photographic experts group), PDF (portable document format) and others. Another option to save a graphic is to simply right mouse click on the graphic, which will produce a pop up menu with options to copy or save the graphic in a few formats as well as to directly print the graphic. In a Windows environment the copy options are as a bitmap or metafile, and the save options are as metafile or postscript. Remember that PDF and postscript formats are lossless, and as long as they do not contain bitmaps you can enlarge the figures without losing resolution. This is very convenient for posters and journal publications. In Rstudio the plots as well as options to export them as pdf or images are available in the lower right “plots” panel. If you want to save graphs for publication, and need full control over size, margins, fonts etc., it is a good idea to plot directly to a file instead of plotting on screen and then saving to file. The disadvantage is that if you plot directly to a file, the file itself is the graphical device, and you will not see the result on screen. However, you can play with some of the settings while plotting to screen first, and subsequently use these settings to plot to file. Note however, that many settings concerning size, font size etc. will not properly map on a screen graphical device. There are different types of file-based graphical devices for the different graphical formats, like PDF, BMP, PNG, JPEG and TIFF, invoked by commands like pdf(), jpeg(), etc. See the help file ?device to obtain help on these different file-based graphical devices. 5.9 Different graphical systems and packages There are two basic graphical systems (implemented in standard packages) in R. The default system we have used above is encoded in the “graphics” package. The alternative system is encoded in the “grid” package. Since they are so fundamental, and form a basis for other graphical packages, we will refer to them as systems rather than packages. Both the “graphics” and “grid” system take care of the very basic drawing functions, like making a coordinate system, drawing lines, text, and points. These functions need to be called by other high-level plotting functions like plot(), hist(), boxplot(), etc. to be useful to an end-user. An end-user does not usually use graphics or grid functions directly. As an end-user, the only reason to know that there are two basic graphical systems in R is the fact that functions based on these packages can not be mixed in the same graphical device ( i.e. a graphical window on your computer, or a graphical file like a PDF, PNG, TIFF, or whatever format). In addition to the default graphical system and functions that we used above, two other packages are worth mentioning, namely the “lattice” and “ggplot2” packages. Both are based on the “grid” system, and hence cannot be mixed with traditional graphics. These packages enable substantial flexibility and complexity in producing graphs. My impression is that the recent “ggplot2” is one of the best innovations in graphical flexibility. For more literature on producing graphs, see (Murrell 2006) (general basic graphics, grid and lattice), (Sarkar 2008) (by the author of the lattice package) and (Wickham 2009) (by the author of the ggplot2 package). The author of ggplot2 has also set up a web manual with examples: http://docs.ggplot2.org. A good recipe book for the ggplot2 package is Chang (2012). Below I show three examples, including the R commands, of plotting the same data (“diamonds” from the ggplot2 package) with the default graphics package (plot), the lattice package (xyplot) and the ggplot2 package (qplot). plot(price~carat, data=diamonds, log=&#39;xy&#39;, col=rainbow(length(levels(diamonds$color)))[diamonds$color]) # require(lattice) xyplot(price~carat, data=diamonds, scales=list(x=list(log=10),y=list(log=10)), group=color) qplot(carat, price, data=diamonds, colour=color) + scale_x_log10() + scale_y_log10() Finally, to see how easy it is to produce complex but intelligible graphs with ggplot2, we re-create the final plot of the iris data, this time including an on the fly fitted regression line: ggplot(data=iris, mapping=aes(x=Petal.Length, y=Sepal.Length)) + # what to plot stat_smooth(method=lm, se=FALSE, colour=&quot;red&quot;, size=1) + # draw a fitted line through the data geom_point(mapping=aes(shape=Species, color=Species), size=3) + # plot points, adapt shape/color using Species variable theme(legend.position=c(0.05,0.95), legend.justification=c(0,1)) # where to put the legend 5.10 Exercises Simple curves Create a script that draws a curve of the function \\(y=x^{3}\\sin^{2}(3\\pi x)\\)in the interval \\([-2,2]\\). Plot at a reasonable resolution (number of points) to obtain a smooth curve. Show solution f &lt;- function(x) {x^3*(sin(3*pi*x)^2)} curve(f, -2, 2, n=200) Make a curve of the function \\(y=1/\\cos(1+x^{2})\\) in the interval\\([-5,5]\\). Show solution f &lt;- function(x) {1/(cos(1 + x^2))} curve(f, -5, 5, n=200) Human population growth The human population (\\(N\\), millions of people) at a certain time \\(t\\), can be described as a function of time, the initial population density at \\(t=t0\\) (\\(N_{t0}\\)), the carrying capacity \\(K\\) (maximum possible population) and the growth rate \\(a\\) by the logistic growth equation: \\[ N(t)=\\frac{K}{1+\\left[\\frac{K-N_{t0}}{N_{t0}}\\right]\\cdot e^{-a(t-t0)}} \\] For the US, the population density in 1900 (\\(N_{0}\\)) was 76.1 million people; the population growth can be described with parameter values: \\(a\\)=0.02 yr-1, \\(K\\)= 450 million. Actual population values were (in millions of people): \\[ \\begin{array}{ccccccccc} 1900 &amp; 1910 &amp; 1920 &amp; 1930 &amp; 1940 &amp; 1950 &amp; 1960 &amp; 1970 &amp; 1980\\\\ 76.1 &amp; 92.4 &amp; 106.5 &amp; 123.1 &amp; 132.6 &amp; 152.3 &amp; 180.7 &amp; 204.9 &amp; 226.5 \\end{array} \\] Tasks: Plot the population density curve as a thick line, using the US parameter values (watch out with the units). Show solution a &lt;- 0.02 K &lt;- 450 N0 &lt;- 76.1 x0 &lt;- 1900 f &lt;- function(x) {K/(1 + ((K-N0)/N0)*exp(-a*(x - x0)))} curve(f, 1900, 1980, lwd=2) Add the measured population values as points. Finish the graph with titles, labels etc. Show solution a &lt;- 0.02 K &lt;- 450 N0 &lt;- 76.1 x0 &lt;- 1900 censusdata &lt;- data.frame( year=c(1900, 1910, 1920, 1930, 1940, 1950, 1960, 1970, 1980), population=c(76.1, 92.4, 106.5, 123.1, 132.6, 152.3, 180.7, 204.9, 226.5) ) plot(population ~ year, data=censusdata) curve(f, 1900, 1980, lwd=2, add=TRUE) Toxic ammonia Nitrogen in the form of ammonia is present in two forms: the ammonium ion (\\(NH_{4}^{+}\\)) and unionized ammonia (\\(NH_{3}\\)). As ammonia can be toxic at sufficiently high levels, it is often desirable to know its concentration. The relative importance of ammonia, (the contribution of ammonia to total ammonia nitrogen, \\(p_{[NH_{3}]}=[NH_{3}]/\\left([NH_{3}]+[NH_{4}^{+}]\\right)\\)) is a function of the proton concentration \\([H^{+}]\\) and a parameter \\(K_{N}\\), the so-called stoichiometric equilibrium constant: \\[ p_{[NH_{3}]}=\\frac{K_{N}}{K_{N}+[H^{+}]} \\] Tasks: Plot the relative fraction of toxic ammonia to the total ammonia concentration as a function of pH, where \\(pH=-\\log{}_{10}([H^{+}])\\) and for a temperature of 30°C. Use a range of pH from 5 to 11. The value of \\(K_{N}\\) is 8x10-10 at a temperature of 30°C. Show solution KN &lt;- 8E-10 pNH &lt;- function(x) {KN/(KN + 10^(-x))} curve(pNH, 5, 11) Add to this plot the relative fraction of ammonia at 0°C; the value of \\(K_{N}\\) at that temperature is 8x10-11 mol/kg. Show solution KN &lt;- 8E-11 curve(pNH, 5, 11, col=&#39;red&#39;, add=TRUE) The iris data set Have a look at the data: What is the class of the data set? why? Show solution It’s a data frame. A matrix would not have been suitable because there are both numerical and factor columns What are the dimensions of the data set? (number of rows, columns) Show solution dim(iris) yields 150 rows and 5 columns Produce a scatter plot of petal length against petal width; produce an informative title and labels of the two axes. Show solution plot(Petal.Length~Petal.Width, data=iris, main=&#39;Two variables from the iris data set&#39;, xlab=&#39;Petal width (cm)&#39;, ylab=&#39;Petal length (cm)&#39;) yields 150 rows and 5 columns Repeat the same graph, using different symbol colors for the three species. (tip: make use of the fact that the Species column is a factor vector, which is internally represented by an integer vector) Show solution See main text of this chapter Add a legend to the graph. Copy the result to an Powerpoint, MS-Word or OpenOffice document. Show solution See main text of this chapter Create a box-and whisker plot for sepal length where the data values are split into species groups; use as template the first example in the boxplot help file. Show solution See main text of this chapter Now produce a similar box-and whisker plot for all four morphological measurements, arranged in two rows and two columns. Use the layout() function to first specify the graphical parameter that arranges the plots two by two. Show solution layout(matrix(1:4, nrow=2, byrow=TRUE)) boxplot(Sepal.Length~Species, data=iris) boxplot(Sepal.Width~Species, data=iris) boxplot(Petal.Length~Species, data=iris) boxplot(Petal.Width~Species, data=iris) Automatic coercion by plot() Create a data frame exactly as stated here: d &lt;- data.frame(nr=factor(c(&#39;1&#39;,2:20)),value=c(1:20)^2) Check its content by printing d on the screen. Now make a plot: plot(d$nr, d$value). Something is going wrong here, because if you print d$nr and d$value to the screen they seem to be in the right order. Actually, the plot(x,y) command receives as its first argument a factor, which is not a number and it then tries to convert this to a numerical vector by applying as.numeric() to it. See what happens if you do that manually. Use the coercion functions as.character() and as.numeric() to convert d$nr to the right numerical type vector, and make the plot. References "],
["file-io.html", "CHAPTER 6 File input and output 6.1 Defining the path to a file or directory 6.2 The “working directory” 6.3 Using the read.table and write.table functions 6.4 Reading data from a webserver 6.5 Reading data from compressed files and archives 6.6 Input/output with Excel files and database management systems 6.7 Exercises", " CHAPTER 6 File input and output Often you will want to read data from or write data to a file. Perhaps you have data in an Excel file, or you want to use the calculations from R in another program. R provides basically two functions for this, that are (surprise!) called read.X and write.X, where the X stands for different formats. These functions can only read and write text files, i.e. files that contain ASCII characters. The two basic formats available are comma-separated and tab-delimited, but you can tweak the read and write functions to cope with all kinds of subtle variants of these formats. I often use the functions for the tab-delimited formats which are defined by read.table and write.table. Take a look at their help files to get a feeling of what you can do with them. 6.1 Defining the path to a file or directory The location or “path” of a file or directory under an operating system can be defined as a string. Under Windows, for example, the path of a file that you want to read from or write to could be defined as “C:files.txt”. R can interpret and use such path strings. If you have a complicated folder structure with long path names, it is often tedious to write these strings manually in R. Fortunately, there is a convenient function available that allows you to choose a file interactively in a widget, and that return path strings. It is called file.choose() for choosing one file. On Windows machines (Mac also?) we also have choose.dir() for choosing one directory, and choose.files() for choosing one or more files. At the R prompt, type each of these functions and choose some random files or directories. The strings returned in the console are the paths to these files. 6.2 The “working directory” To avoid having to type complicated paths or having to browse through your file system, you should use the R “working directory” concept. During any R-session there is always a working directory defined, which is the default directory in which output is written to files and in write-operations and which is the default directory in which R searches for files in read-operations, when no full path specification is given. You can find out what the current working directory is by using the function getwd(). Suppose you have your project data in a folder on your hard-disk. Then it may be useful to use that folder as your working directory if you are going to have a lot of input and output to that directory. You can set a working directory to, for example, “C:files&quot; (a Windows directory) by executing the command setwd('C:/My files') (yes, forward slashes /. They are automatically translated to the windows specific backward-slash syntax 'C:\\My files'). However, the directory must exist and be accessible (read/writeable). If the path is not fully specified, i.e. it does not start with the root of a file system, then read- and write operations are attempted relative to the working directory. The specification of a file system root depends on the operating system that you use. Suppose you want to read or write a file called ‘mydata.csv’ in a directory data subdirectory of the current working directory, then on any operating system you should use the file argument file='data/mydata.csv'. On unix-like systems, if you would write instead file='/data/mydata.csv' an attempt would be made to search or write a file in the data sub-directory of the root (what happens on Windows and Mac?). On Windows writing to a data subdirectory of the C-disk would need a file='C:/data/mydata.csv' argument. In Rstudio, when opening a project, the working directory is set to the project directory. This is the behaviour that you will need most often when working with file in- and output during a project. You can change it by moving into the directory of your choice using the file selector panel (by default the lower right) and then select from the menu in the same panel “More - Set As Working Directory”. It will execute the setwd() function with the directory filled in, as you will notice in the console panel. Often, your data directory will be a sub-directory of the project directory. You can read and write from and to a subdirectory by specifying a relative path, as mentioned above (like file=‘data/mydata.csv’). Under windows (I don’t know about Mac) there are also the choose.dir() and choose.files() functions available, that open OS-specific widgets to choose a directory or multiple files. The combination setwd(choose.dir()) may be useful to choose a working directory in a user-friendly manner. 6.3 Using the read.table and write.table functions The read.table() and write.table() functions are file input output functions tuned for working with tabular data. The usual procedure in which you will use them is that you note down the data in a spreadsheet program, like Excel. Let’s do that here. Open an Excel worksheet and write the following exactly as shown, including the comment at the top: # This table shows the scores of pupils on math in a dutch school pupil test 1 test 2 test 3 gender 1 1 5 7 m 2 8 7.5 8.5 m 3 5 7 6.5 f 4 9 8.5 10 f Save the spreadsheet file in a directory that you call “R file IO”, for example and call it “scores.xls” (or “scores.xlsx”). In the same directory also save the table as a tab-delimited file, and call it “scores.txt”. Now, using the setwd() command, change the working directory to the one in which these files are located, for example by using the commands shown above. Try to read the tab-delimited file with the following commands: read.table(&quot;scores.txt&quot;) read.table(&quot;scores.txt&quot;, sep=&quot;\\t&quot;) read.table(&quot;scores.txt&quot;, sep=&quot;\\t&quot;, header=TRUE) Explain what is happening here. What does '\\t' in sep='\\t' mean? What happens to the comment line in the file? Why? (Please look at the documentation of the read.table() function). What should you do to be able to do anything with these data in R? What happens to the names of the column headers? If you run into a problem reading a tab-delimited file (error messages like “line N does not contain M elements”) check whether it is necessary to explicitly state the field separator, the quote character (disable it with quote='') and the comment character (specify it with comment.char='#'). These are the most common causes of errors. Another useful argument to the read.X functions is na.strings. With this argument you can indicate which texts in your file should be interpreted as missing values. You can enter multiple values in na.strings. By default na.strings='NA', but you could, for example, specify na.strings=c('','not measured') if empty cells as well as the string 'not measured' in your file indicate missing values. Now put the table in an R-object called d. Find out what the dimensions of d are by using the dim() function. What is the object class of d? Find out the object classes of each of the individual columns in d. Can you explain what you observe here? In the documentation of the read.table function try to find out how you could change the object classes of the columns during the reading process. We want to add a column to the data frame and write the result to a new file in the working directory. Let’s create a column “test.4” and put some additional grades in it. There are two ways to do this manually: Type the following: d$test.4 = c(2.5, 8, 7, 8) Start the table editor by typing d &lt;- edit(d), and then type in the numbers in a new numerical column (why do we have to use the assignment operator &lt;-?). Try both. Now we will save the new table (data frame) in a file called “scores_nw.txt”. Open the help file of the write.table function, and then try each of these commands. After each command have a look at what is written in the new file by trying to open it in notepad or Excel. Close it again before executing the next command, otherwise R may not be able to overwrite the file write.table(d, file=&quot;scores_nw.txt&quot;) write.table(d, file=&quot;scores_nw.txt&quot;, quote=FALSE) write.table(d, file=&quot;scores_nw.txt&quot;, quote=FALSE, sep=&quot;\\t&quot;) write.table(d, file=&quot;scores_nw.txt&quot;, quote=FALSE, sep=&quot;\\t&quot;, row.names=FALSE) What is the effect of each of the additional parameter changes? Which file resembles most accurately the original “scores.txt” file? 6.4 Reading data from a webserver Open a web browser, and take a look at this URL: http://www.few.vu.nl/~molenaar/courses/statR/data/io. You will see that there are three files in this directory on the web server. The file called “weights.txt” is a tab-delimited file, so, we could try to read it into a data frame. Of course, using the right mouse button you could download the file to a directory on your PC, and then read the file from there using the technique discussed above. But that is not really necessary. R can also read directly from URLs. We will do this a lot in the exercises later, so let’s practice it here. Because most of the data for this syllabus are located in subdirectories of a common webserver directory, it saves typing if you define a base url in an R variable baseurl, which is just the common initial part of the complete URL. In this syllabus the base url is http://www.few.vu.nl/~molenaar/courses/statR. After defining this variable type the following commands. At the prompt type head(d) to see whether the data were loaded. f &lt;- file.path(baseurl, &#39;data&#39;, &#39;io&#39;, &#39;weights.txt&#39;) d &lt;- read.table(f, sep=&quot;\\t&quot;, header=TRUE) head(d) ## Gender Age Height Weight BFI ## 1 F 17 69.0 116.0 18.6 ## 2 F 18 59.5 110.2 22.0 ## 3 F 18 64.0 125.2 22.3 ## 4 F 18 63.0 115.4 35.4 ## 5 M 18 66.0 187.4 25.4 ## 6 M 18 66.0 156.2 14.8 The file.path() function creates a complete URL from parts of a path or URL, using the appropriate directory separator symbols. You can check that by looking at the content of the variable f. 6.5 Reading data from compressed files and archives Large data sets can take up large amounts of disk space. You can limit the use of disk space by keeping data in compressed formats: a factor of 3 to 5 compression of the text-type files that we use are not unusual. Furthermore, archive file formats like zip or tar are nice solutions to keep data together that belong together. There are a few functions available in R for reading compressed files and (compressed) archives. You can get help for these functions by typing, for example, help(unz). Let us practice reading a large data file from a zip archive. As you may know, zip files are compressed archives that contain one or more files. So, to be able to read from a zip file, you should not only indicate the location of the zip file itself, but also the name of the file that is located in the zip archive. On the server in the io directory there is a zip file called “arrays.zip” which contains data from two microarrays. Choose a working directory. Before you can use it, you will need to download the array.zip to your working directory. Either do that manually with the browser, or use the command download.file(): f &lt;- file.path(baseurl, &#39;data&#39;, &#39;io&#39;, &#39;arrays.zip&#39;) # In case of using a local data repository, no need to &quot;download&quot; # &#39;grepl&#39; looks for the pattern &#39;http:&#39; in the file path, and returns # a logical vector. if (grepl(&#39;http:&#39;, baseurl)) { # match! So, file is on a server # download to workdir with destination file name the same as the original download.file(f, destfile=&#39;arrays.zip&#39;) } else { # file is in a local repository, copy it to the workdirectory file.copy(f, getwd()) } The archive (zip) file contains two tab-delimited files, “array_1.txt” and “array_2.txt”, both with header. You have downloaded it with the previous command. Where is it located? Read the file “array_1.txt” into a data frame d, but first make a read connection to this file with the unz command: con &lt;- unz(&#39;arrays.zip&#39;, filename=&#39;array_1.txt&#39;) d &lt;- read.table(con, header=TRUE) Look whether you succeeded in reading the data into d: head(d) ## gene sample1 sample2 sample3 ## 1 gene1 0.9863501 0.5647454 0.03490377 ## 2 gene2 0.5761707 0.3034730 0.87093070 ## 3 gene3 0.3978490 0.8536779 0.18710406 ## 4 gene4 0.7331442 0.3848955 0.24674201 ## 5 gene5 0.1022730 0.9713885 0.17269618 ## 6 gene6 0.9519216 0.4982501 0.97074606 6.6 Input/output with Excel files and database management systems There are also packages that to read and write directly to Excel files, and other packages to read and write to relational databases like MSAccess, MySQL (mariaDB) and PostgreSQL, or graph databases like Neo4j. These require more advanced techniques, but you might want to use them in the future if your data sets become very large, have a complex structure, or are stored in such databases by the provider. To get a taste of the possibilities, have a look at the R manuals website (http://cran.r-project.org/manuals.html). There, under R Data Import/Export you will find more information. For full control over direct read and write access to Excel files, I can recommend the packages XLconnect (somewhat slow, but easy to use), xlsx (works under linux also), readxl (simple, works under any operating system, but for read-only with limited control) and RODBC (very fast but more complicated, works only under windows). 6.7 Exercises We would like to calculate the average score for each pupil in the “scores.txt” file that you created , and put these averages as a separate column in the table. This means that we want to calculate the mean of columns 2, 3, and 4 for each pupil, and put the result in a column that we will call “average.score”. We could, for every row, calculate the mean, and put it in the new column. However, there is a very useful procedure for applying a function (like mean()) to rows or columns to a whole table in one go. It is called “apply”, and you should learn its use as soon as possible (more on this in chapter 8). Read the documentation for the apply() function and use it to construct the new average.score column. Show solution You need to select the columns 2–4, and the MARGIN needs to have the value 1, which means that the function FUN will be applied over the first dimension = rows: d &lt;- read.table(&quot;scores.txt&quot;, sep=&quot;\\t&quot;, header=TRUE) d$average.score &lt;- apply(d[,2:4], 1, mean) Try to read the “geneexpr.txt” table from the server (“io” directory) into a data frame using the read.table() function. It is possible! It is a tab-delimited file with one comment line at the top and it has column headers. It also has a missing value, which should become NA automatically by using the right argument setting for read.table(). If you don’t succeed on the first attempt, study the error messages and adjust the arguments to the read.table() function. There is a useful explanation on the different options for the read.table() function on the R website under Documentation \\(\\rightarrow\\) Manuals \\(\\rightarrow\\) R Data Import / Export, paragraph “Variations on read.table”. Show solution Now you cannot use the default behavior where all code after a hash (#) will be interpreted as comments, and not read in the table, because some of the gene names contain a hash. However, the skip argument allows you to skip a number of lines at the start of the files before the interpreter starts reading data. The na.strings argument needs to be set to let the function recognize missing values, and insert NA in those positions. ## genes control salt ## 1 LF001 1.01 3.5 ## 2 LF002 2.30 4.7 ## 3 LF003 0.10 1.1 ## 4 LF004 0.10 1.0 ## 5 LF005 2.60 0.7 ## 6 LF006 10.40 NA ## 7 LF007 1.50 1.6 ## 8 LF008#1 0.30 0.4 ## 9 LF008#2 0.10 0.0 ## 10 LF009 6.00 5.5 ## 11 LF010 4.00 4.4 The excel file “malatedehydrogenase_activity.xlsx” gives a table of the activity of the enzyme malate dehydrogenase at different thyroxine concentrations. Thyroxine inhibits this enzyme. An excel file can not be read directly from the server. Therefore, you first have to download the file using the download.file() function. By default, it deposits your file in the working directory. Subsequently use the readxl package to read the file. Plot two different graphs in a single pane of the rate of the enzyme as a function of the malate concentration. Make one graph of the rate in the absence of thyroxine, and one of the rate in the presence of thyroxine. Use different symbols for the graphs. Note that the output of readxl is a so-called ‘tibble’, a data frame with additional attributes. You can just treat it as a data.frame: it is a child class of data.frame. Show solution library(readxl) filename &lt;- &#39;malatedehydrogenase_activity.xlsx&#39; url &lt;- file.path(baseurl, &#39;data&#39;, &#39;io&#39;, filename) download.file(url, destfile=filename) d &lt;- read_excel(filename, sheet=1L, col_names=TRUE, skip=4) # divide the data in two parts for different thyroxine concentrations d0 &lt;- d[d$thyroxine==0,] d1 &lt;- d[d$thyroxine==60,] plot(rate ~ malate, d0, pch=1, ylim=c(0,120)) points(d1$malate, d1$rate, pch=19) "],
["Programming-with-R.html", "CHAPTER 7 Programming with R 7.1 Defining a function 7.2 Program flow control 7.3 Literature 7.4 Exercises 7.5 Additional programming exercises", " CHAPTER 7 Programming with R 7.1 Defining a function R has many built-in functions. However, often you will encounter situations in which you want to make your own new function, for example because you will repeatedly do a similar, somewhat complicated calculation on one or more objects. In R you can define your own functions by using the function statement. The general structure of a function definition in R is: functionName &lt;- function (argument1, argument2, ...) { # here comes the function body doing things with the arguments and calculating some &#39;object&#39;. Then we return that object. return(object) } where functionName is the new name by which you call (use) your function, the argument list (one or more) is given in brackets, and the function body contains the calculations performed on the arguments. The calculated object is returned explicitly with return(object) or, alternatively, if it is the result of the last calculation in the function body then it is implicitly returned. Subsequently, you call your new function as follows: functionName(value1, value2, ...) Typically, complex functions are written in R-script files for re-use. These files can then be read into an R-session using R’s source() function, for example. Let’s define a very simple function called “Circlesurface” that calculates the surface of a circular disc (\\(\\pi \\cdot r^{2}\\)), given its radius (the only argument for this function): Circlesurface &lt;- function(radius) pi*radius^2 Here, since our function body contains only one statement, we don’t have to put it in a code block surrounded by curly brackets (see below). And you also see that we use the implicit return of the last calculated value in the function body. After submitting this function to R, we can use it to calculate the surfaces of discs with given radius: Circlesurface(10) ## [1] 314.1593 or Circlesurface(1:4) ## [1] 3.141593 12.566371 28.274334 50.265482 which will calculate the surface of discs with radii 1, 2, 3 and 4 (Why does your function also work on a vector with multiple elements?). More complicated functions may return more than one element: Sphere &lt;- function(radius) { v &lt;- 4/3*pi*radius^3 s &lt;- 4*pi*radius^2 return(list(volume=v, surface=s)) } Here we recognize The function heading (first line), specifying the name of the function (Sphere) and the input parameter or argument (radius) The function specification. As the function comprises multiple statements, the function specification now has to be embraced by curly braces “{ … }” in a so-called code block. The return values (last line in the block). Sphere will return the volume and surface of a sphere, as a list object. The earth has approximate radius 6371 km, so its volume (km3) and surface (km2) are: Sphere(6371) ## $volume ## [1] 1.083207e+12 ## ## $surface ## [1] 510064472 The next statement will only display the volume of spheres with radius 1, 2, … 5 Sphere(1:5)$volume ## [1] 4.18879 33.51032 113.09734 268.08257 523.59878 Sometimes it is convenient to provide default values for the input parameters. For instance, the next function estimates the density of “standard mean ocean water” (in kg/m3), as a function of temperature, T, (and for salinity=0, pressure=1 atm); the input parameter T is by default equal to 20°C: Rho_W &lt;- function(T=20) { 999.842594 + 0.06793952 * T - 0.00909529 * T^2 + 0.0001001685 * T^3 - 1.120083e-06 * T^4 + 6.536332e-09 * T^5 } Calling the function without specifying temperature, uses the default value: Rho_W() ## [1] 998.2063 Rho_W(20) ## [1] 998.2063 Rho_W(0) ## [1] 999.8426 Finally, we have now seen functions with one argument, but you can also build functions with multiple arguments by providing the arguments in a comma-separated list in the argument brackets. 7.2 Program flow control Complex functions or procedures often require the encoding of alternative computational paths or the repeated application of a single computational path (looping)2. For example, our Rho_W() function defined above is only valid for a certain range of temperatures, let’s say -5°C to 50°C. If the query temperature is outside this range, instead of extrapolating to a probably false density value, it is safer to return the NA value (not available). To implement this we need code in our function that tests whether we are in the correct temperature range and, based on this test, return either the interpolated density or NA. We can use the if/else construct. In general, an if/else construct looks like this: if (condition) { # do things } else { # do other things } Specifically, in our case we want to test whether T is between -5°C and 50°C, or both \\(T\\geqq-5\\) and \\(T\\leqq50\\) must be true. To see how these logical and comparison operators work, first have a look at their help pages help(Comparison) and help(Logic). Let us test them: T &lt;- -10 T &gt;= -5 ## [1] FALSE T &lt;= 50 ## [1] TRUE T &gt;= -5 &amp;&amp; T &lt;= 50 ## [1] FALSE T &lt;- 1 T &gt;= -5 &amp;&amp; T &lt;= 50 ## [1] TRUE So, in the if statement of our adapted Rho_W function we can use the condition T &gt;= -5 &amp;&amp; T &lt;= 50: Rho_W &lt;- function(T=20) { if (T &gt;= -5 &amp;&amp; T &lt;= 50) { 999.842594 + 0.06793952 * T - 0.00909529 * T^2 + 0.0001001685 * T^3 - 1.120083e-06 * T^4 + 6.536332e-09 * T^5 } else { NA } } The indentation that we use here is only for aesthetical purposes and is not interpreted by R. Also note that function statements automatically return the last value just before function exit, in this case either the result of the polynomial formula or NA. To make the return value more explicitly known to the reader of the code, we could have written: Rho_W &lt;- function(T=20) { if (T &gt;= -5 &amp;&amp; T &lt;= 50) { return(999.842594 + 0.06793952 * T - 0.00909529 * T^2 + 0.0001001685 * T^3 - 1.120083e-06 * T^4 + 6.536332e-09 * T^5) } else { return(NA) } } For simple cases of if/else decisions, there is the one-liner instruction ifelse. The special thing about ifelse is that it returns an object with the same shape as the condition object when it is coerced into logical mode (see help(ifelse)). This can be handy, but also confusing sometimes: a &lt;- c(0,2,5,2,3) ifelse(a&lt;3,0,1) ## [1] 0 0 1 0 1 The condition object is a &lt; 3, is coerced into a logical vector c(TRUE, TRUE, FALSE, TRUE, FALSE). Accordingly, either 0 (TRUE cases) or 1 (FALSE cases) are plugged in the output vector. However, for the usual programmatic if (condition) {do thing 1} else {do thing 2} construction this doesn’t work: if (a&lt;3) {0} else {5} ## Warning in if (a &lt; 3) {: the condition has length &gt; 1 and only the first ## element will be used ## [1] 0 As the warning message already says, the if/else construction can not use a logical vector of length greater than 1, and if the condition statement evaluates to a vector with more than one element only the first element will be used for the program flow decision. So, you should take care that in an if/else construction the condition statement always coerces to a logical vector of length 1, otherwise you can get surprising results. This is also the reason that you should preferably use the &amp;&amp; and || operators (logical AND and OR) instead of &amp; and | in these condition statements (see help(Logic)). Looping several times through the same set of statements is another desirable program flow feature. There are a few ways to do this, the most common being the for and the while loops. The for loop uses an iterator statement, which has the structure var in seq, where var is a variable that assumes, before each turn of the loop, the next value in the vector seq, starting from the first element. Looping stops after the last element is used. The while loop uses a condition statement that it tests before starting each turn of the loop. If the condition evaluates to TRUE, looping continues, whereas it stops if the condition evaluates to FALSE. Some examples: for (i in c(&#39;one&#39;, &#39;two&#39;, &#39;three&#39;)) { print(paste(&#39;Loop number&#39;,i)) } ## [1] &quot;Loop number one&quot; ## [1] &quot;Loop number two&quot; ## [1] &quot;Loop number three&quot; Here the variable i iterates through the sequence (vector) c('one', 'two', 'three'). Each loop, i has a different content which is used in the print(paste(...)) statement. i &lt;- 0 cube &lt;- 0 while (cube &lt; 100) { print(paste(&quot;The cube of&quot;, i, &quot;is smaller than 100&quot;)) i &lt;- i+1 cube &lt;- i^3 } print(paste(&quot;We stopped because the cube of&quot;, i, &quot;is larger than 100&quot;)) Notice that, in contrast to the case of for loops, it is easy to write infinitely continuing while loops. For example, if we had forgotten to write the statement i &lt;- i+1 in the looping block. If that happens, stop the calculation with ctrl-z or, if that doesn’t work, terminate the R-process with the task manager (ctrl-alt-delete). 7.3 Literature An additional introduction to programming in R can be found in the manual R Language Definition on the R website (http://www.r-project.org). Advanced programming issues and creation of packages are treated in the manual Writing R Extensions on the same website and in Chambers (2008). 7.4 Exercises The function “Sphere”. Extend the “Sphere” function on with the circumference of the sphere at the place of maximal radius. The formula for estimating the circumference of a circle with radius r is: \\(2\\cdot\\pi\\cdot r\\). What is the circumference of the earth near the equator? Show solution Sphere &lt;- function(radius) { v &lt;- 4/3*pi*radius^3 s &lt;- 4*pi*radius^2 c &lt;- 2*pi*radius return(list(volume=v, surface=s, circumference=c)) } The circumference of the Earth equals Sphere(6371)$circumference=4.003017410^{4} km. An R-function to estimate saturated oxygen concentrations. The saturated oxygen concentration in water (\\(\\mu\\)mol.kg-1), as function of temperature (T), and salinity (S in ppt, parts per thousand) can be calculated by SatOx=\\(e^{A}\\) where: \\[\\begin{multline} A = -173.9894 + \\frac{25559.07}{T} + 146.4813 \\cdot \\ln \\left( \\frac{T}{100} \\right) - 22.204 \\cdot \\frac{T}{100} \\\\ + S \\cdot \\left[ -0.037362 + 0.016504 \\cdot \\frac{T}{100} - 0.0020564 \\cdot \\left( \\frac{T}{100} \\right)^2 \\right] \\end{multline}\\] and \\(T\\) is temperature in Kelvin (\\(T_{Kelvin}=T_{Celsius}+273.15\\)). Tasks: Make a function that implements this formula; the default values for temperature and salinity are 20°C and 35 ppt, respectively. The temperature input should be in degrees Celsius. What is the saturated oxygen concentration at the default conditions? (Answer: 225.2346) Estimate the saturated oxygen concentration for a range of temperatures from 0°C to 30°C, and salinity 35 ppt. (Do not use loops!). Show solution SatOx &lt;- function(T=20, S=35) { T &lt;- T + 273.15 A &lt;- -173.9894 + (25559.07/T) + 146.4813*log(T/100)-22.204*(T/100) + S*(-0.037362 + 0.016504*(T/100)-0.0020564*(T/100)^2) return(exp(A)) } SatOx()\\(=225.2345729\\) SatOx(T=seq(0,30,by=5))\\(=349.6541533, 308.2286415, 274.9097853, 247.7234524, 225.2345729, 206.3833132, 190.3754653\\) 7.4.1 Loops Column averages. Create a matrix a with 10 columns a &lt;- matrix(1:100,ncol=10) Also create a vector avgs with 10 zeros. Now make a for loop that fills in the column average for every column in a at the corresponding position in the avgs vector. The result should look like avgs ## [1] 5.5 15.5 25.5 35.5 45.5 55.5 65.5 75.5 85.5 95.5 In chapter 8 you will learn a much more efficient way to do the same. Show solution avgs &lt;- vector(length=dim(a)[2], mode=&#39;numeric&#39;) # Or, alternatively avgs &lt;- rep(0,dim(a)[2]) for (i in 1:dim(a)[2]) { avgs[i] &lt;- mean(a[,i]) } Fibonacci numbers. The Fibonacci numbers are calculated by the following relation: \\(F_{n}=F_{n-1}+F_{n-2}\\), with \\(F_{1}=F_{2}=1\\). Tasks: Compute the first 50 Fibonacci numbers; store the results in a vector (use R function vector() to create it). You have to use a loop here. For large \\(n\\), the ratio \\(F_{n}/F_{n-1}\\) approaches the “golden mean”: \\(\\left(1+\\sqrt{5}\\right)/2\\). What is the value of \\(F_{50}/F_{49}\\); is it equal to the golden mean? When is \\(n\\) large enough? (i.e. sufficiently close (difference \\(&lt;1e-6\\)) to the golden mean). Show solution fibs &lt;- vector(length=50,mode=&#39;numeric&#39;) fibs[1] &lt;- 1 fibs[2] &lt;- 1 for (i in 3:50) { fibs[i] &lt;- fibs[i-1] + fibs[i-2] } fibs[50]/fibs[49]\\(=1.618034\\). (1 + sqrt(5))/2\\(=1.618034\\). So, yes at this level of accuracy they are equal. Since the ratio of two consecutive Fibonacci numbers can be larger or smaller than \\(&lt;1e-6\\) we have to test for the absolute difference between the Golden mean and the ratio: ratio &lt;- fibs[2:50]/fibs[1:49] which(abs(ratio - (1 + sqrt(5))/2) &lt; 1e-6) ## [1] 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 ## [24] 39 40 41 42 43 44 45 46 47 48 49 So, starting with the 16-th ratio (fibs[17]/fibs[16]) the difference with the Golden mean is smaller than \\(1e-6\\). Diversity of deep-sea nematodes Starting from your code from the exercise to estimate diversity indices for deep sea station M160b, now write a loop that does so for all the stations in Nemaspec. First create a matrix called div, with the number of rows equal to the number of deep sea stations, and with 6 columns, one for each diversity index. This matrix will contain the diversity values. The column names of div are: “N”, “S”, “N1”, “N2”, “N3”, and “ES”. The row names of matrix div are the station names (= the column names of Nemaspec). Tip: Use R-commands colnames(), rownames(). Now loop over all columns of data frame Nemaspec, estimate the diversity indices and put the results in the correct row of matrix div: for (i in 1:ncol(Nemaspec)) { # Here write your part of the code } Show matrix div to the screen. Diversity indices – a function (optional) Based on the results obtained in the previous section, make a function that will calculate the diversity indices for any data matrix. Rarefaction diversity (optional) In ecology, rarefaction is a technique to compare species richness computed from samples of different sizes. Here, try an alternative way of estimating the number of species per 100 individuals by taking random “subsamples” of 100 individuals with replacement and estimating the number of species from this sub-sample (this is a so-called bootstrap method). If the procedure is repeated often enough, the mean value should converge to the expected number of species, ES(100). This is the rarefaction method of Sanders (1968). You may need the following new R-functions3: round() (converting reals to integers), cumsum() (take a cumulative sum), sample() (take random selection of elements, set replace=TRUE), table() (to make a table of counts), as well as length(), mean(). Hurlbert (1971) showed that rarefaction generally overestimates the true estimated number of species. Can you corroborate this finding? 7.5 Additional programming exercises For programming enthusiasts. The factorial function. Create 3 implementations of the factorial function \\(n!=1\\cdot2\\ldots n\\). Test the speed of your factorial functions by using the system.time() function to measure the time it takes to calculate 10000 times \\(50!\\). Using a for loop Using recursion (i.e. the function calls itself in the function body) Using vectorized R-operators Show solution # The loop version fac1 &lt;- function(n) { ans &lt;- 1 for(i in seq(n)) ans &lt;- ans * i return(ans) } # The recursive version. fac2 &lt;- function(n) if (n &lt;= 0) 1 else n * fac2(n - 1) # The vectorized version fac3 &lt;- function(n) prod(seq(n)) # Measuring the speed of each function by repeating fac(50) N times # shows that the vectorized version is fastest in R N &lt;- 10000 sapply(c(&#39;fac1&#39;,&#39;fac2&#39;,&#39;fac3&#39;), function(x) { system.time(for (i in 1:N) do.call(x,list(50))) }) ## fac1 fac2 fac3 ## user.self 0.098 0.249 0.078 ## sys.self 0.000 0.000 0.000 ## elapsed 0.098 0.249 0.078 ## user.child 0.000 0.000 0.000 ## sys.child 0.000 0.000 0.000 The sieve of Eratosthenes. Create an implementation of the sieve of Eratosthenes. This algorithm, known in ancient Greece, finds all prime numbers smaller than a certain number \\(N\\). It works as follows: Make a list of all numbers \\(1\\ldots N\\) Set a test variable \\(x\\) equal to 2 Remove all multiples of \\(x\\) from the list If \\(x&lt;N\\) then increase \\(x\\) by \\(1\\) and go to step 3, otherwise stop The efficiency of the algorithm can be increased considerably by observing that you can start removing multiples of \\(x\\) from a value of \\(x^{2}\\), since a multiple of \\(x\\) below \\(x^{2}\\) must also have been a multiple of a number smaller than \\(x\\). Furthermore, you can now also see that you can stop the procedure in step 4 much earlier than when \\(x\\geqslant N\\), namely when \\(x&gt;\\sqrt{N}\\). To construct the implementation, use vectorized operators as much as possible. Check the correctness of your algorithm against the ``A000040’’ sequence, which lists the first 59 prime numbers. See http://oeis.org/A000040. Show solution # A simple sieve &#39;sieve1&#39; &lt;- function (n) { ans &lt;- 2:n for (i in 2:trunc(sqrt(n))) { r &lt;- i^2:n ans &lt;- ans[!(ans %in% r[r %% i == 0])] } return(ans) } # My most efficient one, in the sense that it avoids all double work. Is much faster for large n! &#39;sieve2&#39; &lt;- function (n) { # First set up a logical vector, length n, that will keep track of whether a number has been sieved out selection &lt;- c(FALSE, rep(TRUE,n-1)) for (i in 2:trunc(sqrt(n))) { # If i was sieved out before, because it was a multiple of a smaller number, then testing it is unnecessary if (selection[i]) { # set up r: the range of numbers to be tested r &lt;- (i^2:n) # remove from r the numbers that were already sieved out before r &lt;- r[selection[r]] # set the positions in &#39;selection&#39; corresponding to multiples of i to FALSE selection[r[r %% i == 0]] &lt;- FALSE } } return((1:n)[selection]) } # Recursive version. Because R doesn&#39;t like deep recursion it will not work for large n &#39;sieve3&#39; &lt;- function(n) { minlen &lt;- sqrt(n) # define the recursive function inside the calling function &#39;partsieve&#39; &lt;- function(l1) { if (length(l1) &lt; minlen) { return(l1) } else { i &lt;- l1[1] l2 &lt;- l1[-1] return(c(i, partsieve(l2[l2 %% i != 0]))) } } partsieve(2:n) } # One from Swanand https://saysmymind.wordpress.com/2013/02/06/sieve-of-eratosthenes-in-r-and-sql # Is approximately 4 x faster than my sieve2 sieve4 = function(n) { # a holds a sequence of numbers from 2 to n a = c(2:n) # we start from 2 since it is the beginning of prime numbers, # it is also the loop variable l = 2 # r holds the results r = c() #while the square of loop variable is less than n while (l*l &lt; n) { # the prime number is the first element in vector a r = c(r,a[1]) # remove elements from a which are multiples of l a = a[-(which(a %% l == 0))] # the loop variable is the first variable remaining in a l = a[1] } # the result is r and all the remaining elements in a c(r,a) } # Testing on the sequence &quot;A000040&quot; the first 59 prime numbers. See http://oeis.org/A000040 A000040 &lt;- c(2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229, 233, 239, 241, 251, 257, 263, 269, 271) # test whether the sets are of the same size length(sieve1(271)) - length(A000040) ## [1] 0 length(sieve2(271)) - length(A000040) ## [1] 0 length(sieve3(271)) - length(A000040) ## [1] 0 # length(sieve4(271)) - length(A000040) # test whether all elements of A000040 are in the sieved sets all(A000040 %in% sieve1(271)) ## [1] TRUE all(A000040 %in% sieve2(271)) ## [1] TRUE all(A000040 %in% sieve3(271)) ## [1] TRUE # all(A000040 %in% sieve4(271)) # these two tests together prove correctness of the sieve functions for n=271 # Testing calculation time on a large N (recursive version will not work) N &lt;- 500000 system.time(sieve1(N)) ## user system elapsed ## 5.848 0.000 5.849 system.time(sieve2(N)) ## user system elapsed ## 0.553 0.000 0.553 # system.time(sieve4(N)) # We can use the replicate function to repeat the same system.time call repeatedly. The output is automatically converted to a vector. Taking the average of the outcomes gives a more accurate restimation apply(replicate(5,system.time(sieve1(N))),1,mean) ## user.self sys.self elapsed user.child sys.child ## 5.1430 0.0000 5.1436 0.0000 0.0000 apply(replicate(5,system.time(sieve2(N))),1,mean) ## user.self sys.self elapsed user.child sys.child ## 0.6902 0.0000 0.6902 0.0000 0.0000 # apply(replicate(5,system.time(sieve4(N))),1,mean) Stratified partitioning. Create a function that performs a so-called stratified partitioning of a data set. Stratified partitioning is needed when doing cross-validations of classifiers, for example. Suppose you have the Iris data set and you have a classifier that predicts the iris species based on Sepal and Petal dimensions. In cross-validation you divide the data set in equal parts, for example 10, and then take 9/10-th of the data to train the classifier and use the remaining 1/10-th to test the classifier. Every 1/10-th partition is used once for testing, and hence you get a 10-fold cross-validation. To obtain representative data sets, it is important that the frequency of the different species in the partitions resembles that of the complete data set as close as possible. The partitions are then said to be stratified over the factor ``Species’’. The function is called partition(x, p, f), where x is a data frame, p is the number of desired partitions, and f is character string with the name of the factor over which stratification should take place. The output is the same data frame with an additional column that contains the label of the partition (1 … p). Show solution partition &lt;- function(x, p, f) { # The data frame x has N rows with classes documented in the factor column with name &#39;f&#39;. # We want p partitions stratified over f. # The code can be easily extended to cases where stratification takes place over multiple # factor columns # # Create a partition column x$partition &lt;- NA # Calculate the sizes of the stratification groups groupsizes &lt;- table(x[[f]]) groupnames &lt;- names(groupsizes) # Each group gets a random distribution of labels 1:p. If groupsize %% p is larger # than 0, then the remainder of the rows gets a random selection of labels 1:p. for (gn in groupnames) { # - the first call to &#39;sample&#39; randomly permutes the row index numbers in a group # - the second call to &#39;sample&#39; adds one random value from 1..p to the index number. This # prevents the first groupsize %% p partitions to always get the remaining rows. labels &lt;- (sample(1:groupsizes[gn]) + sample(1:p,1))%%p + 1 x$partition[x[[f]]==gn] &lt;- labels } return(x) } # # alternative one-liner using the &quot;data.table&quot; package, (x must be a data.table object): # partition &lt;- function(x, p, f) { # x[,partition := (sample(1:.N) + sample(1:p,1))%%p + 1, by=f] # } References "],
["vectorization.html", "CHAPTER 8 Vectorization 8.1 The function apply() 8.2 The function tapply() 8.3 The functions lapply() and sapply() 8.4 Exercises", " CHAPTER 8 Vectorization In section 7.2 we mentioned that R has very efficient alternatives for repetitive tasks that in other programming environments often require looping constructs. This is called vectorization, and it is an important technique in R. We have already seen cases of vectorization. Suppose we have a vector a &lt;- c(5,3,7). The following expression adds 1 to every element of a: a+1 ## [1] 6 4 8 Implicitly, it loops over every element of the longest vector a and adds to it the element of the shortest vector c(1), equivalent to the following implementation with looping: result &lt;- vector(mode=&#39;numeric&#39;,length=length(a)) for (i in 1:length(a)) { result[i] &lt;- a[i] + 1 } result ## [1] 6 4 8 Another example is the replacement of elements of a vector using a logical vector. Suppose we want to replace all elements of y &lt;- c(0,4,3,6,0,2) that are equal to 0 by 1. This can be done as follows: y &lt;- c(0,4,3,6,0,2) y[y==0] &lt;- 1 y ## [1] 1 4 3 6 1 2 Implicitly, this simple statement first loops over every element of y and creates a logical vector (say test), having the same length as y, in which the elements are TRUE when the corresponding element in y equals 0, and FALSE otherwise. Subsequently, it replaces the corresponding element in y if the element in test equals TRUE. In code it is equivalent to: test &lt;- vector(mode=&#39;logical&#39;, length=length(y)) for (i in 1:length(y)) { if (y[i]==0) { test[i] &lt;- TRUE } else { test[i] &lt;- FALSE } } for (i in 1:length(y)) { if (test[i]==TRUE) { y[i] &lt;- 1 } } y ## [1] 1 4 3 6 1 2 You will probably agree that the vectorized forms are, from a coding perspective, much more efficient, and therefore less prone to error! As a bonus, vectorized code often executes faster, especially when dealing with large data structures. So, vectorization is an important property of R, and you should use it as much as possible4 Below we will discuss a few functions that help you with the vectorization of slightly more complex tasks in R. They are the functions apply(), tapply(), sapply() and lapply(). Basically, these functions loop over a data structure, like a matrix, a data frame or a list, and apply a function to a sub-structure, like a row or column (which then turn into vectors), or a list element. 8.1 The function apply() Take a look at the help file for the function apply(X, MARGIN, FUN, ...). It tells you that as its first argument X, apply() takes an array, and if the input is not an array, but it does have a dimension property (like matrix and data.frame have), it will try to coerce it to an array. Its third argument FUN is the name of a function to be applied to X, and the second argument MARGIN indicates over which dimension the function is to be applied, where, for a 2-dimensional array 1 represents rows and 2 represents columns. Suppose you have the following data frame containing scores of 5 pupils on two different exams: d &lt;- data.frame(exam1=c(7,6,8,9,5), exam2=c(6,7,7,7,4), row.names=c(&#39;pupil1&#39;,&#39;pupil2&#39;,&#39;pupil3&#39;,&#39;pupil4&#39;,&#39;pupil5&#39;)) d ## exam1 exam2 ## pupil1 7 6 ## pupil2 6 7 ## pupil3 8 7 ## pupil4 9 7 ## pupil5 5 4 You want to know the mean scores of each of the pupils (rows) and on each of the exams (columns). Instead of programming a loop over the rows or columns, we can simply state: apply(d,1,mean) ## pupil1 pupil2 pupil3 pupil4 pupil5 ## 6.5 6.5 7.5 8.0 4.5 or apply(d,2,mean) ## exam1 exam2 ## 7.0 6.2 (What type of object is the output of apply() here?). Do you understand the connection between the value of the second MARGIN argument (1 or 2) and the outcome? You are not limited to applying pre-defined functions over arrays. You can also define your own function and put its name in the FUN argument. Suppose we want to know for each pupil whether it has passed all exams, using the rule that the average over the exams has to be 5.5 or larger. We can define a function named “passfunction” that returns “pass” or “fail” for a vector of exam scores of a pupil: passfunction &lt;- function(scores) { if(mean(scores)&gt;=5.5) { return(&#39;pass&#39;) } else { return(&#39;fail&#39;) } } Now we can apply our function “passfunction” to the rows of d: apply(d,1,passfunction) ## pupil1 pupil2 pupil3 pupil4 pupil5 ## &quot;pass&quot; &quot;pass&quot; &quot;pass&quot; &quot;pass&quot; &quot;fail&quot; Suppose we want to make our “passfunction” function a little more sophisticated by being able to enter the pass limit. We also want this limit to have a default value of 5.5, in case the user of our function does not enter a pass limit as an argument. We can re-define the function as follows: passfunction &lt;- function(scores, limit=5.5) { if(mean(scores)&gt;=limit) { return(&#39;pass&#39;) } else { return(&#39;fail&#39;) } } We can apply this function as before to the rows of d, and it will show which pupils had an average score above or below 5.5. But suppose we want the pass limit to be 7.0, then we can add that limit as a fourth argument to apply(). This is what the ... indicates in the definition of apply(X, MARGIN, FUN, ...). For ... you can enter any number of (named, or name inferred from position) arguments to the function FUN: apply(d,1,passfunction,limit=7.0) ## pupil1 pupil2 pupil3 pupil4 pupil5 ## &quot;fail&quot; &quot;fail&quot; &quot;pass&quot; &quot;pass&quot; &quot;fail&quot; To continue on this passing of arguments to the function applied by apply(), suppose we have a slight change of our exam data, where the first pupil did not show up at the first exam: d[&#39;pupil1&#39;,&#39;exam1&#39;] &lt;- NA Now, apply(d,1,mean) and apply(d,2,mean) will yield NA for the first row and column, which may be undesirable (try it). But of course, we could let the apply() function pass on the argument na.rm=TRUE to the mean() function: apply(d,1,mean, na.rm=TRUE) ## pupil1 pupil2 pupil3 pupil4 pupil5 ## 6.0 6.5 7.5 8.0 4.5 apply(d,2,mean, na.rm=TRUE) ## exam1 exam2 ## 7.0 6.2 and obtain averages again for the first row and column. 8.2 The function tapply() Suppose we have the following data frame: d &lt;- data.frame(exam1=c(7,6,8,9,5), exam2=c(4,5,5,7,3), gender=c(&#39;female&#39;,&#39;female&#39;,&#39;male&#39;,&#39;male&#39;,&#39;female&#39;), row.names=c(&#39;pupil1&#39;,&#39;pupil2&#39;,&#39;pupil3&#39;,&#39;pupil4&#39;,&#39;pupil5&#39;)) d ## exam1 exam2 gender ## pupil1 7 4 female ## pupil2 6 5 female ## pupil3 8 5 male ## pupil4 9 7 male ## pupil5 5 3 female We want to know the average score for exam1 by gender, so separately for male and female. Also in this case, we could program a loop structure, testing gender, adding the score to either male or female sum, and finally taking the averages. But there is a much simpler solution that uses vectorization. It uses the function tapply(): tapply(d$exam1, d$gender, mean) ## female male ## 6.0 8.5 What did we do here? Take a look at the definition of tapply() in its help page: tapply(X, INDEX, FUN = NULL, ...). as input X we need an atomic object, typically a vector (here d$exam1). The third argument FUN was again the name of the function that we want to use (mean), and the second argument INDEX should be one or more factors (if more than one, they are provided as a list) of the same length as X that we want to use to separate the data in X. In our case we used one factor: d$gender. 8.3 The functions lapply() and sapply() The function lapply() (list-apply) applies a selected function to each element of a list. The answer is returned as a list of the same length (the same number of elements) as the list to which it was applied. Suppose you have a list of numeric vectors of different length: mylist &lt;- list(v1=runif(12),v2=runif(100),v3=runif(23)) and you would like to know the mean of each of these lists. Then, instead of making a for-loop construct looping over each element, you can just type lapply(mylist,mean) ## $v1 ## [1] 0.4914725 ## ## $v2 ## [1] 0.5284289 ## ## $v3 ## [1] 0.4352408 This gives a list of the three means. The function sapply() does the same as lapply(), but if possible, returns a vector or matrix as a result. This is only possible if the function that was applied is guaranteed to return a fixed number of results for each element to which it was applied. The function mean() always returns a single number, or NA, so sapply() should work here: sapply(mylist, mean) ## v1 v2 v3 ## 0.4914725 0.5284289 0.4352408 Verify, using the class() function that the classes of the results from both functions are indeed as was stated above. 8.4 Exercises Instead of using a for loop, use the apply() function to calculate the column averages of the matrix a section 7.4.1 Show solution a &lt;- matrix(1:100, ncol = 10) apply(a, 2, mean) ## [1] 5.5 15.5 25.5 35.5 45.5 55.5 65.5 75.5 85.5 95.5 Use apply() to calculate a) the mean scores per pupil and b) per exam on the data frame in section 8.2. Tip: use a sub-selection of the columns of d, selected with a bracket operator, to apply the function mean() to. Show solution d &lt;- data.frame(exam1=c(7,6,8,9,5), exam2=c(4,5,5,7,3), gender=c(&#39;female&#39;,&#39;female&#39;,&#39;male&#39;,&#39;male&#39;,&#39;female&#39;), row.names=c(&#39;pupil1&#39;,&#39;pupil2&#39;,&#39;pupil3&#39;,&#39;pupil4&#39;,&#39;pupil5&#39;)) apply(d[c(&#39;exam1&#39;,&#39;exam2&#39;)], 1, mean) ## pupil1 pupil2 pupil3 pupil4 pupil5 ## 5.5 5.5 6.5 8.0 4.0 # this column selection works because a data frame is a child class of list (columns are list elements). Alternatively treat a data frame as a matrix: apply(d[,c(&#39;exam1&#39;,&#39;exam2&#39;)], 1, mean) apply(d[c(&#39;exam1&#39;,&#39;exam2&#39;)], 2, mean) ## exam1 exam2 ## 7.0 4.8 Add a second factor hair to the data frame in section 8.2 and calculate the mean score on exam1 by hair color. Also calculate the mean score by gender and hair color. Tip: in the last case use as an index the list composed of d$gender and d$hair. This exercise will probably confirm all your prejudices. d$hair &lt;- factor(c(&#39;brunette&#39;,&#39;blonde&#39;,&#39;blonde&#39;,&#39;brunette&#39;,&#39;blonde&#39;)) Show solution tapply(d$exam1, d$hair, mean) ## blonde brunette ## 6.333333 8.000000 tapply(d$exam1, list(d$gender,d$hair), mean) ## blonde brunette ## female 5.5 7 ## male 8.0 9 Preparing for the next exercise: if you calculate the mean of a numeric vector containing an NA element, then the result will be NA, unless you provide the argument na.rm=TRUE to the mean() function. Calculate the mean of a &lt;- c(0:10, NA, 11:20), disregarding the NA element (it should equal 10). Show solution a &lt;- c(0:10, NA, 11:20) mean(a, na.rm=TRUE) ## [1] 10 Create the data frame u and, using the apply() function, calculate the row- and column means, disregarding the NA elements. Look up in the help page of the apply() function how to pass the na.rm=TRUE argument to the mean() function in an apply() statement. Also calculate the standard deviations of the columns. u &lt;- data.frame(var1=rnorm(100,0,1), var2=rnorm(100,10,1)) u$var1[runif(length(u$var1))&gt;0.90] &lt;- NA u$var2[runif(length(u$var2))&gt;0.90] &lt;- NA Show solution apply(u,1,mean,na.rm=TRUE) apply(u,2,mean,na.rm=TRUE) apply(u,2,sd,na.rm=TRUE) By the way, do you understand how u is constructed? Look up the help page for the rnorm() function for generation of random numbers from a normal distribution. Again using the data frame d with the exam scores: use apply() and tapply() to calculate with a one line statement the average scores over both exams for males and females of the data used in section 8.2. Tip: perform a tapply() on an apply() on (a subset of) columns of the data frame. Use the appropriate additional arguments. Show solution tapply(apply(d[c(&#39;exam1&#39;,&#39;exam2&#39;)], 1, mean), d$gender, mean) ## female male ## 5.00 7.25 Create the list of lists flintstones &lt;- list(person1=list(name=&#39;Fred&#39;, kids=c(&#39;Pebbles&#39;), married=TRUE, pets=c(&#39;Dino&#39;,&#39;Baby Puss&#39;,&#39;Doozy&#39;)), person2=list(name=&#39;Barney&#39;, kids=&#39;Bamm-Bamm&#39;, married=TRUE, pets=&#39;Hoppy&#39;)) Using the function lapply() or sapply(), extract the names of the persons from this list. Tip: as you saw before, the bracket operators are just functions having the names “[” and “[[”. The function to apply to each element is a bracket operator. Don’t forget to quote it, otherwise the usual syntax of the bracket operator is expected. Show solution lapply(flintstones, &#39;[[&#39;, &#39;name&#39;) ## $person1 ## [1] &quot;Fred&quot; ## ## $person2 ## [1] &quot;Barney&quot; # or yielding a more compact result, if possible sapply(flintstones, &#39;[[&#39;, &#39;name&#39;) ## person1 person2 ## &quot;Fred&quot; &quot;Barney&quot; The exercise above returns a list in case of one bracket operator and a vector in case of the other when you use sapply(). Do you understand why? In case that it returns a list, you can use the unlist() function to turn the result into a character vector. Try that! In a one-line statement, calculate the mean number of pets in the flintstones list for all persons. To construct this statement work it out in stages. First extract the “pets” elements in a new list, as you did above for the names. Then apply the length() function to each of its elements, obtaining a vector (use sapply()) with numbers of pets. Finally, use the mean() function to calculate the mean number of pets. Of course, in case of a small list this effort is a bit overdone, but for large lists this is the most efficient way. Show solution mean(sapply(lapply(flintstones, &#39;[[&#39;, &#39;pets&#39;), length)) ## [1] 2 Instead of looping over a vector of integers with a for loop (for example, for (i in 1:10) {...}) you can use and lapply() or sapply() on that vector. It is used often in later chapters in this syllabus, and especially useful if you need results in a list. Use this cosntruct to do a “leave one out” analysis. In this case (admittedly, not a very useful one), to calculate the 10 different means and standard deviations of 9 out of 10 entries in the vector runif(10) by leaving each entry out once. The output should be a 10 \\(\\times\\) 2 matrix with “mean” and “sd” columns. Show solution d &lt;- runif(10) t(sapply(1:10, function(i){c(mean(d[-i]), sd(d[-i]))})) ## [,1] [,2] ## [1,] 0.3577275 0.2572868 ## [2,] 0.3593072 0.2568172 ## [3,] 0.2985961 0.1866755 ## [4,] 0.3812052 0.2383784 ## [5,] 0.3860650 0.2309436 ## [6,] 0.3520557 0.2580711 ## [7,] 0.3305451 0.2481043 ## [8,] 0.3230868 0.2395325 ## [9,] 0.3606522 0.2563303 ## [10,] 0.3680012 0.2522327 Adapt the function Rho_W(T) in section 7.2 so that it also accepts an argument T with length &gt;1. 8.4.1 A permutation function (DIFFICULT) Make a function “permute” that takes two arguments: 1) an alphabet consisting of a set of symbols and 2) a length being a single integer N. An alphabet could, for example, be the set of DNA nucleotides {“A”, “T”, “G”, “C”}. The function has as an output a character matrix containing all possible combinations of length N of the elements of the alphabet, the first column in the matrix containing the first element, the second column containing the second element, etc. The names of the rows of the output matrix should correspond to the complete string. For example, the output generating all combinations of {“a”, “b”, “c”} of length 2 would look like: alphabet &lt;- c(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;) a &lt;- permute(alphabet, 2) a ## [,1] [,2] ## aa &quot;a&quot; &quot;a&quot; ## ab &quot;a&quot; &quot;b&quot; ## ac &quot;a&quot; &quot;c&quot; ## ba &quot;b&quot; &quot;a&quot; ## bb &quot;b&quot; &quot;b&quot; ## bc &quot;b&quot; &quot;c&quot; ## ca &quot;c&quot; &quot;a&quot; ## cb &quot;c&quot; &quot;b&quot; ## cc &quot;c&quot; &quot;c&quot; The function can be written in less than 15 lines. Show solution ## Function for the generation of all permutations of ## length n of a vector v permute &lt;- function(v, n) { vl &lt;- length (v) if (n &gt; 0) { # TODO: build in safety for large n i &lt;- n - 1 r &lt;- matrix(rep(v, each=vl^i), ncol=1) while (i &gt; 0) { i &lt;- i - 1 r &lt;- cbind(r,rep(rep(v, each=vl^i), times=(vl^(n-i-1)))) } rownames(r) &lt;- apply(r, 1, paste, collapse=&#39;&#39;) return(r) } } ## Another solution uses the standard function expand.grid(), which ## generates a data frame from all combinations of a set of factor ## variables. To generate the list of factor variables to be used ## as arguments to expand.grid(), we use lapply(). Functions like ## expand.grid() or cbind(), taking indefinite length arguments can ## be called with a list of arguments using the function do.call() permute2 &lt;- function(v, n) { if (n &gt; 0) { # TODO: build in safety for large n arglist &lt;- lapply(1:n, function(x){return (v)}) r &lt;- rev(do.call(expand.grid, arglist)) # rev reverses the order of the columns rownames(r) &lt;- apply(r, 1, paste, collapse=&#39;&#39;) return(r) } } Other languages that use this property, and that you may use at some point in your career are MATLAB and Mathematica.↩ "],
["reshaping.html", "CHAPTER 9 Reshaping and manipulating complex data 9.1 The tidyr package 9.2 The dplyr package 9.3 Exercises", " CHAPTER 9 Reshaping and manipulating complex data One of the things that you almost allways have to do before analysing or plotting data is the reshaping of data tables. This as also called “data wrangling” because it can be a very tedious process, especially when the tables have a complex structure with nested levels of factors, etc. One simple type of re-shaping is the exchange of rows and columns of a table. This can be easily done in R by using the transpose function t(). However, most table manipulations are far more complex. Surprisingly, the transformations that you need to perform often boil down to a limited set of standard patterns. The packages tidyr and dplyr and their associated manuals can help you see these common manipulation patterns and can help you carry them out. A common pattern in the manipulation of data tables occurs when column names actually contain the values of a variable. It often requires some careful thinking before you realize this. For example, somebody has measured the expression of 10 genes, known to be involved in metal-stress, in three replicate “control” experiments (“control1”, “control2”, “control3”) and three replicate experiments in which samples were treated with copper (“copper1”, “copper2”, “copper3”). The expression data of the 10 genes in the 6 experiments were each put in a separate column in Excel. The rows are labeled with the gene identifier, and the columns are labeled with the experiment identifiers “control.1”, “control.2”, “control.3”, “copper.1”, “copper.2”, “copper.3”. Let’s retrieve these data from the server: file &lt;- file.path(baseurl, &#39;data&#39;, &#39;reshape&#39;, &#39;example_2.txt&#39;) d &lt;- read.table(file, header=TRUE) d ## gene control.1 control.2 control.3 copper.1 copper.2 copper.3 ## 1 gene1 3.04 2.63 2.05 0.10 -0.20 0.09 ## 2 gene2 0.31 0.91 0.55 -3.14 -2.18 -3.52 ## 3 gene3 -0.58 -0.18 -0.59 2.14 1.04 1.46 ## 4 gene4 -0.15 -0.64 -0.94 -1.35 -0.79 -0.86 ## 5 gene5 -3.15 -2.96 -2.43 2.29 1.90 2.44 ## 6 gene6 0.63 0.69 0.86 2.39 2.15 2.94 ## 7 gene7 -1.42 -1.56 -1.03 2.13 3.44 2.19 ## 8 gene8 -0.33 -0.54 -0.09 1.29 1.36 1.61 ## 9 gene9 -4.43 -3.63 -4.21 3.69 4.38 3.19 ## 10 gene10 -1.09 -1.44 -1.80 2.33 1.75 3.29 You want to compare control samples with treatment samples, for example in a boxplot. Applying the boxplot function directly to the data.frame d gives you the following plot: boxplot(d[-1], col=&#39;grey&#39;,las=2) Figure 9.1: Not quite the boxplot that we want of the gene expression data But what you actually want to compare are all control samples with all treatment samples, irrespective of their repeat number. This requires a data frame with only two columns: one (factor) column, with variable name treatment having values indicating whether the sample is a control or a copper sample, and a second column with variable name expression containing the gene expression value. So, the column names of the original data frame actually contain levels of the factor variable treatment having two levels, control or copper. Another variable name, expression, is even missing from the table. 9.1 The tidyr package The philosophy of the tidyr package is to use a limited set of functions enabling the conversion of a data object (data.frame, list or matrix) between “long” and “wide” formats. The central canonical form of data tables is the so-called tidy table, which is a table that contains a single column for every variable, and a single row for every observation (Wickham 2014). All data formats can be converted to and from this canonical form. Furthermore, this canonical form is the one that you will use most often for data analysis and plotting. The tidyr package needs only four functions to reach these goals: gather(): gather columns into less columns, converting column names into variables. spread(): the inverse of gather(). separate(): split information in one column based on a pattern of the cell content. unite(): the inverse of separate(). To manipulate our example data frame into the format that we need, we start with gathering all columns containing expression data into two columns, experiment and expression. The gather() function gathers data from columns in a data frame (the first argument) into two columns, namely a ‘key’ column containing the name of the original column and whose name is the second argument, and a ‘value’ column containing the values and whose name is the third argument. The fourth and subsequent arguments specify which columns to gather; see the help file for different ways of specifying this. The simplest way is by adding each (bare, not quoted) column name to be gathered as an additional argument. Here, however, we use the exclusion syntax -gene (notice the minus sign) because we just want to gather all columns except that single gene column. This column will then remain intact in the output, but it will be repeated for each original column. # require(tidyr) d.new &lt;- gather(d, experiment, expression, -gene) head(d.new) ## gene experiment expression ## 1 gene1 control.1 3.04 ## 2 gene2 control.1 0.31 ## 3 gene3 control.1 -0.58 ## 4 gene4 control.1 -0.15 ## 5 gene5 control.1 -3.15 ## 6 gene6 control.1 0.63 Have a carefull look at the output and make sure that you understand what the gather() function did. Superficially, this looks already like the output that we want. However, we still have 6 levels (control 1-3 and copper 1-3) in the “experiment” column, whereas we want two levels, a control and copper group. Therefore, we reshape the data by splitting the entries in the experiment column into two new factor columns, treatment, containing the part before the dot, and repeat containing the number after the dot. It will have two values: “control” and “copper”: d.new &lt;- separate(d.new, experiment, into=c(&#39;treatment&#39;,&#39;repeat&#39;), sep=&#39;[.]&#39;) head(d.new) ## gene treatment repeat expression ## 1 gene1 control 1 3.04 ## 2 gene2 control 1 0.31 ## 3 gene3 control 1 -0.58 ## 4 gene4 control 1 -0.15 ## 5 gene5 control 1 -3.15 ## 6 gene6 control 1 0.63 The argument sep='[.]' gives a so-called “regular expression” (see ?regex), which, when a match is found in a string, is used to separate the string. In this case the regular expression will match a dot. The into=c('treatment','repeat') argument indicates what the names of the newly created columns should be. The repeat column does not contain information that we will use, but is here just a by-product of splitting the content of the experiment strings, Such information may be interesting, for example when the repeats were performed on different days and you want to investigate the effect of the day on which the experiment was perfomed on thegene expression. We can now make the desired boxplot with expression plotted as a function of the treatment: boxplot(expression~treatment, data=d.new, col=&#39;grey&#39;) Figure 9.2: The correct boxplot of the gene expression data A pattern often observed often in data wrangling is one where the output of one function is the input for the next function: y &lt;- g(x, A, B, …) and z &lt;- f(y, a, …), where A, B, a etc. are additional arguments. The tidyr package, as well as a number of other packages provide the pipe operator %&gt;%, which is originallly defined in the package magrittr. It allows you to string together such functions without explicitly having to provide a variable name (y) for the intermediate result: z &lt;- x %&gt;% g(A, B, …) %&gt;% f(a, b, …) This saves some typing and makes the code better readable, which in turn makes coding less error prone. To be able to use this pipe symbol, the chained functions need to be defined in such a way that the piped output is their first argument. The code above could have been summarized as: d.new &lt;- d %&gt;% gather(experiment, expression, -gene) %&gt;% separate(experiment, into=c(‘treatment’,‘repeat’), sep=‘[.]’) Apart from being able to make the correct boxplot now, the advantage of this canonical tidy format compared to the original wide format of the data frame is that you can write model formulas (for example for linear modeling and ANOVA) in which you can investigate the effect of the treatment factor. Such modeling formulas look like expression ~ treatment, as in the command for the boxplot above. They are discussed in chapter 10. As a simple example, we do a t-test on the data: t.test(expression~treatment, data=d.new) ## ## Welch Two Sample t-test ## ## data: expression by treatment ## t = -3.9914, df = 57.626, p-value = 0.0001881 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.9566111 -0.9813889 ## sample estimates: ## mean in group control mean in group copper ## -0.7173333 1.2516667 For many analysis tasks in R, the tidy format will be optimal. If you need different formats the other functions of the tidyr package can help reshaping a tidy format into any format that you desire. 9.2 The dplyr package The dplyr package can help perform complicated data manipulations like summarizing, joining tables, filtering data, etc. Suppose that we have an additional table of gene annotations, for example a table that categorizes genes based on the biological function that they are involved in. The data are loaded from the table (note that it is tab-delimited) file &lt;- file.path(baseurl, &#39;data&#39;, &#39;reshape&#39;, &#39;example_2_annotations.txt&#39;) annot &lt;- read.table(file, header=TRUE, sep=&quot;\\t&quot;) annot ## gene biofunction ## 1 gene1 metabolism ## 2 gene2 protein synthesis ## 3 gene3 general stress ## 4 gene4 protein synthesis ## 5 gene5 metabolism ## 6 gene6 general stress ## 7 gene7 metal stress ## 8 gene8 general stress ## 9 gene9 metal stress ## 10 gene10 general stress We would like to summarize the gene expression per category (biofunction) of genes. To do this, we first have to link the data in the gene expression data table to that in the gene annotation table. This can be done using the join function from the dplyr package. The link or key-field between both tables is provided by the gene identifiers in the gene columns in both tables. This can be done with the inner_join() function5. # require(dplyr) full.table &lt;- inner_join(d.new, annot) ## Joining, by = &quot;gene&quot; head(full.table) ## gene treatment repeat expression biofunction ## 1 gene1 control 1 3.04 metabolism ## 2 gene2 control 1 0.31 protein synthesis ## 3 gene3 control 1 -0.58 general stress ## 4 gene4 control 1 -0.15 protein synthesis ## 5 gene5 control 1 -3.15 metabolism ## 6 gene6 control 1 0.63 general stress Now that we have this table, we want to know what the average expression is per treatment and gene category. To do this, we use the group_by and summarise functions from the dplyr package as follows: summary.table &lt;- full.table %&gt;% group_by(treatment, biofunction) %&gt;% summarise(meanexpr = mean(expression)) summary.table ## # A tibble: 8 x 3 ## # Groups: treatment [?] ## treatment biofunction meanexpr ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 control general stress -0.372 ## 2 control metabolism -0.137 ## 3 control metal stress -2.71 ## 4 control protein synthesis 0.00667 ## 5 copper general stress 1.98 ## 6 copper metabolism 1.10 ## 7 copper metal stress 3.17 ## 8 copper protein synthesis -1.97 This was just a very brief introduction to the capabilities of the dplyr package. If you use it you have to be aware that this package creates data objects of the type tibble. This is a child class of the data.frame frame class, but having (among other benefits) a more efficient way of displaying the data. An extensive discussion of reshaping and manipulating data with the tidyr and dplyr packages can be found on this website, and the cheatsheets page on the Rstudio website provides number of useful summaries on this subject. 9.3 Exercises What does the argument las=2 in the boxplot() command in figure 9.1 do? Show solution The argument las=2 rotates the x-axis labels so that they appear vertically Retrieve the data from data/reshape/exc_1.txt and examine its structure. The data record weight in mice that were put on different diets for 4 weeks. Which factors and factor-levels can you distinguish? Reshape the data to a data frame that is suitable for doing an ANOVA that tests whether weight depends on diet and/or gender. Use the substr() function to make the proper factor columns. Remember that the ANOVA procedure in R requires each factor to be in a different column. What other information about these mice would you have liked to have in order to be able to investigate these effects more thoroughly? Show solution We might also like to know the weight of these mice before they start the diet. It is likely to influence the final weight. file &lt;- file.path(baseurl, &#39;data/reshape/exc_1.txt&#39;) d &lt;- read.table(file, header=TRUE) d.new &lt;- d %&gt;% gather(experiment, weight, 1:6) %&gt;% separate(experiment, c(&#39;diet&#39;,&#39;gender&#39;), sep=&#39;[.]&#39;) # to illustrate its use: boxplot(weight~gender*diet, data=d.new, col=&#39;grey&#39;, las=2) References "],
["formulasyntax.html", "CHAPTER 10 Using formula syntax 10.1 Using formula syntax in plotting 10.2 Using formula syntax in model definition", " CHAPTER 10 Using formula syntax R knows a special formula syntax that is sometimes very handy or even essential in writing commands. The notation is originally described by Wilkinson and Rogers (1973). We will here discuss some of the basic uses of the formula syntax. You can find more help in the “formula” help file (type ?formula in the console). There are two areas where this formula syntax is most often used: in plotting commands and in defining statistical models. The latter are, for example, used in defining ANOVA models or when defining non-linear models to which data have to be fitted. The basic definition of a formula always contains the squiggle symbol and is written as lhs ~ rhs where the left hand side (lhs) of the formula contains a response variable and the right hand side (rhs) a predictor variable or combinations of predictor variables. The introductory R-manual by Venables and Smith gives an extensive description of formula syntax in R. See chapter 11 in that manual. 10.1 Using formula syntax in plotting To illustrate the use of formulas in plotting, take the standard iris data again, which we also used in Chapter 5. Suppose we want to plot Petal.Length as a function of Petal.Width. The way you have learned to do this was: plot(iris$Petal.Width, iris$Petal.Length) and the way to do this with a formula would be plot(Petal.Length~Petal.Width, data=iris) which could be interpreted in words as “plot Petal.Length as a function of Petal.Width, using data from the data frame iris”. The formula syntax can also contain common mathematical functions. For example, if you would want to plot the logarithm of both variables you could give the command plot(log(Petal.Length)~log(Petal.Width), data=iris) You also saw the example of using formula syntax when making a boxplot of the petal length . In addition to the squiggle, another formula syntax element is the vertical bar | which indicates evaluation with a condition. The formula syntax would be lhs ~ rhs | condition The file “weights.txt” contains body fat percentages (BFI), weights (in pounds), heights (in feet) and ages for several people. We might be interested in the fat percentage as a function of weight, differentiated by (conditioned on) gender. The usual plot() function can not handle the condition syntax, but the coplot() function (“co” for “conditional”) can: ## Gender Age Height Weight BFI ## 1 F 17 69.0 116.0 18.6 ## 2 F 18 59.5 110.2 22.0 ## 3 F 18 64.0 125.2 22.3 ## 4 F 18 63.0 115.4 35.4 ## 5 M 18 66.0 187.4 25.4 ## 6 M 18 66.0 156.2 14.8 10.2 Using formula syntax in model definition The use of formula syntax in model definition and fitting is more complex, and is most easily explained with some examples. Please execute these examples. 10.2.1 Data with a continuous explanatory variable Load the data “scatter.txt” from data/formulas. The file has a header line) in a data frame. Study the structure of the data using the head() function. head(d) ## x y ## 1 0.0 27.270772 ## 2 0.2 -2.558557 ## 3 0.4 5.884235 ## 4 0.6 21.954871 ## 5 0.8 3.539414 ## 6 1.0 8.735299 Produce a plot of the data using the formula syntax. Give the x-axis a label “Explanatory variable” and the y-axis a label “Response”. We want to fit a first order linear function (a model) to the data using the lm() function (lm stands for “linear modeling”). This would be a fit to \\(y=a_{0}+a_{1}x\\), where \\(x\\) represents the “Explanatory variable” and \\(y\\) the “Response”. We do this using the formula syntax again. Then we ask to print the data in the linear modeling object, below called linfit, in the console: linfit &lt;- lm(y~x, data=d) linfit ## ## Call: ## lm(formula = y ~ x, data = d) ## ## Coefficients: ## (Intercept) x ## 9.2365 0.8594 We see that the fit has an intercept (with the y-axis) of 9.237 and a slope of 0.8594. We can now insert a line representing the fit in the previous graph by using the predict(linfit) function, which gives the y-values predicted by the linear model, and plot these against the original x-values. This command uses the lower-level graphical lines() function. lines(d$x, predict(linfit)) You can also get graphical diagnostic information about the fit, applying the plot() method of the linfit object. If you do that you will get four graphs. Find out, using the help pages of plot.lm() and internet sources, what the first two diagnostic graphs depict. Are you satisfied with the fit? Which other fit would you like to try? Before we can try another fit on these data we have to know something more about formulas. Suppose you would want to try to fit a quadratic (second order) function in the explanatory variable x to the response y. This would also be a linear model, because this model \\(y=a_{0}+a_{1}x+a_{2}x^{2}\\), would be linear in the coefficients \\(a_{0}\\), \\(a_{1}\\), and \\(a_{2}\\) that need to be fitted. The term “linear” in “linear model” refers to the way that parameters are appearing in the formula for the fit, not to the way the variables appear! You might try lm(y ~ x+x^2, data=d), but you will see that this yields exactly the same result as lm(y ~ x, data=d). So, formula syntax in R is not the same as in arithmetic. This is because in R formulas are used for statistical modeling, where mixtures of discrete and continuous variables are possible. For a discrete variable a quadratic term \\(x^{2}\\) would be meaningless (what would \\(Gender^{2}\\) mean?), so such terms are just interpreted as linear, unless you explicitly tell R not to. This can be done using the I() syntax (see the “formula” help page), meaning that the operations within the I() brackets are interpreted as arithmetic operations. So, linfit2 &lt;- lm(y~x+I(x^2),data=d) yields the desired fit. linfit2 ## ## Call: ## lm(formula = y ~ x + I(x^2), data = d) ## ## Coefficients: ## (Intercept) x I(x^2) ## 16.863368 0.400850 0.004585 Look at the diagnostic plots of this fit, and also plot the predicted line in a plot of the original data, as you did above for the first order model. Are you satisfied with the fit? 10.2.2 Data with two discrete explanatory variables (factors) We will now fit a linear model to discrete data. The data are gene expression levels, given two variables, namely “mutation” and “temperature”. For the “mutation” variable there are two discrete levels, namely “wt” and “mut”, where “wt” stands for wild type, the most common form in a population of the gene under investigation, and “mut” for a mutant form of the gene. The “temperature” variable also has two discrete levels, namely “low” and “high”. The data are available in data/formulas in the file “generegulation.txt”. Load the data in a data frame, and study its structure. head(d) ## mutation temp response ## 1 wt low -0.5070249 ## 2 wt low 0.5846385 ## 3 wt low 0.2376765 ## 4 wt low -0.5523020 ## 5 wt low -0.5220565 ## 6 wt high 0.8200714 Before continuing, we want to order the levels of both variables in a different order than the default alphabetical order (reason explained below). By default, the levels are ordered in this way: levels(d$mutation) ## [1] &quot;mut&quot; &quot;wt&quot; levels(d$temp) ## [1] &quot;high&quot; &quot;low&quot; We re-order by explicitly calling the relevel() function (look up its help page). This function lets define one of the levels of a factor as the reference level. Behind the scenes, it just puts our desired reference level at the first position in the integer-coded levels. d$mutation &lt;- relevel(d$mutation, ref=&#39;wt&#39;) d$temp &lt;- relevel(d$temp, ref=&#39;low&#39;) levels(d$mutation) ## [1] &quot;wt&quot; &quot;mut&quot; levels(d$temp) ## [1] &quot;low&quot; &quot;high&quot; You see that the two variables are both factors, and that both factors have two levels. Make a boxplot, as below, of the data given the different variable level combinations. Use the formula syntax in the boxplot() function. From this boxplot try to answer the following questions: Does the gene expression level depend on temperature only? Does the gene expression level depend on presence or absence of the mutation only? Is the difference in gene expression level between low and high temperature the same in organisms with the wildtype and mutant genes? Now try to fit the data to 4 linear models using lm(), and relate these models to the questions that you answered above: A model1 with only mutation as explanatory variable (response ~ mutation) A model2 with only temperature as explanatory variable (temp) A model3 with both mutation and temperature as explanatory variables (mutation + temp) A model4 with both mutation and temperature and their “interaction” as explanatory variables (mutation + temp + mutation:temp, or equivalently mutation * temp) Study their diagnostic plots and their coefficients (e.g. coef(model1)). Can you interpret them? Use the predict() function to see what the predictions of the models are. Do you see how the coefficients relate to the predictions? Also try to understand why a number of models do not fit the data well. By the way, the reason to change the order of the levels above was to get the intercept and coefficients in the right order, i.e. all relative to mutation=wt and temperature=low, with the intercept corresponding to the data having with mutation=wt and temp=low. References "],
["distributions.html", "CHAPTER 11 The carnival of distributions 11.1 Distribution functions 11.2 Distribution functions in R 11.3 The exponential and Poisson distributions 11.4 The Bernoulli and binomial distributions 11.5 The normal and standard normal distribution 11.6 Student’s t-distribution 11.7 The chi-squared distribution 11.8 The F-distribution", " CHAPTER 11 The carnival of distributions 11.1 Distribution functions A distribution function is a tool used to describe the probability of finding that a random variable (\\(X\\)) has a certain number or a number within a range as its value. We say that the random variable represents a draw from this distribution. We consider two types of distribution functions: those that describe the probability of finding a certain integer (discrete distribution), which are called probability mass functions (pmf) and those describing the probabilty of finding a continuous number within a range of values (continuous distributions), called probability density functions (pdf). A distribution of the discrete type is, for example the uniform distribution with 6 levels. Its pmf is: \\[ Pr(X=i) = \\frac{1}{6} \\;\\; (i \\in 1 \\ldots 6) \\] It could describe the distribution of observing a value when throwing fair dice. The distribution function has a domain, here the numbers \\(1 \\ldots 6\\), which is the set of all values that the random variable can assume. The domain is often indicated as the set \\(\\Omega\\). For \\(P(x)\\) to be a probability, the sum of the probabilities over all possible value of \\(x \\in \\Omega\\) should add up to \\(1\\). This is a required property of a distribution function, i.e. \\[ \\sum_{X \\in \\Omega} Pr(X) = 1 \\] For a continuous random variable it is senseless to ask for the probaility of obtaining a certain value, because the set \\(\\Omega\\) will be uncaountable, and hence that propability will be infinitely close to 0. The usual question there is: what is the probability that \\(x\\) will be found in a certain interval \\([a,b)\\) \\(Pr(a \\leq X &lt; b)\\)? The distribution of \\(X\\) is in this case calculated using a probability density function \\(f(t)\\), which is linked to the requested probability as follows: \\[ Pr(a \\leq X &lt; b) = \\int_a^b f(t) dt \\] An example of such a pdf is that of the standard normal distribution: \\[ f(t|\\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\left( \\frac{t}{\\sigma} \\right)^2} \\] A normal-distributed random variable \\(X\\) can take any value in the set of real numbers, so in this case \\(\\Omega = \\mathbb{R}\\). A plot of this pdf has the well-know bell-shape of a Gauss curve. It’s important to remember that the connection between a pdf and a probability can only be made by integrating the pdf over an interval of \\(\\Omega\\). The distribution is truly that of a probability since the (limit of the) integral over the interval \\([-\\infty, \\infty]\\) equals 1. 11.2 Distribution functions in R Clearly, being a statistical environment, R has numerical methods to work with the common statistical distributions, continuous- as well as discrete-valued. These functions are encoded in the stats package. See ?Distributions for an overview. For all of the common distributions four functions are available. If the name of the distribution is distr, then there are the following functions: R function result ddistr(x,...) probability density at parameter value x (continuous) or probability of observing x (discrete) pdistr(q,...) distribution function (probability=integrated probability density) at parameter value q (continuous) or cumulative sum of probabilities of observing values up to and including q qdistr(p,...) parameter value at cumulative probability p rdistr(n,...) n parameter values drawn randomly from the distribution The dots ... provide further arguments to these functions. Most distributions are characterized by one or more parameters that you will have to provide. Below, we will use 4 of these distributions: distribution R functions characterizing parameters mean variance exponential dexp(), pexp(), etc. rate (\\(\\lambda\\)) \\(\\frac{1}{\\lambda}\\) \\(\\frac{1}{\\lambda^2}\\) Poisson dpois(), ppois(), etc. lambda (\\(\\lambda\\)) \\(\\lambda\\) \\(\\lambda\\) binomial dbinom(), pbinom(), etc. size (\\(N\\)), prob (\\(\\theta\\)) \\(N \\theta\\) \\(N \\theta (1 - \\theta)\\) normal dnorm(), pnorm(), etc. mean (\\(\\mu\\)), sd (\\(\\sigma\\)) \\(\\mu\\) \\(\\sigma^2\\) t dt(), pt(), etc. df (\\(\\nu\\)) 0 \\(\\frac{\\nu}{\\nu – 2}\\) \\(\\chi\\)-square dchisq(), pchisq(), etc. df (\\(\\nu\\)) \\(\\nu\\) \\(2 \\nu\\) F df(), pf(), etc. df1, df2 (\\(d_1\\), \\(d_2\\)) \\(\\frac{d_2}{d_2 – 2}\\) \\(\\frac{2 d_2^2 (d_1 + d_2 -2)}{d_1 (d_2 – 2)^2 (d_2 – 4)}\\) Example For example, if you wish to make 5 random draws from the normal distribution with mean 7 and standard deviation 0.6, you can use rnorm(): rnorm(n=5, mean=7, sd=0.6) ## [1] 6.819282 7.585722 7.273605 7.776645 6.320079 Clearly, if you make enough draws, and you make a histogram of these, you will reconstruct a picture of the density function of the normal distribution: hist(rnorm(10000, mean=7, sd=0.6), col=&#39;grey&#39;, nclass=50, freq=FALSE) Of course, you could have obtained an exact picture of this with dnorm(x,mean=7,sd=0.6). hist(rnorm(10000, mean=7, sd=0.6), col=&#39;grey&#39;, nclass=50, freq=FALSE) curve(dnorm(x, mean=7, sd=0.6), from=3, to=11, n=500, col=&#39;red&#39;, lwd=2, add=TRUE) The function that draw random samples from distributions all depend on a so-called random number generator, a mathematical function that computes series of apparently random numbers. A little more information about random number generators can be found in chapter 23. 11.3 The exponential and Poisson distributions The exponential distribution arises as the distribution of the intervals (time intervals, for example) of independent events that have a constant rate of occurrence. Examples are radioactive decay events among a large number of atoms, lethal traffic accidents among a large population, reacting molecules among a large number of molecules, random breaks in DNA strands (interval = size of DNA) in a large DNA molecule, customers being served at a help-desk, etc. The most characteristic property of such events is that previous events have no effect at all on future events. It is said that such a process is “memory-less”. To achieve this a large, and in principle infinite, supply of putative events is needed, otherwise their exhaustion may be noticed. When the intervals between such events are measured, they have an exponential distribution, for example with a rate \\(\\lambda\\)=0.1: curve(dexp(x, rate=0.1), from=0, to=60, n=100, col=&#39;red&#39;, lwd=2, xlab=&#39;interval&#39;, ylab=&#39;probability density&#39;) Shorter intervals are more likely to occur than long intervals. The pmf of this distribution is \\[f(x|\\lambda) = \\lambda e^{-\\lambda x}\\] The same processes generate a Poisson distribution the number of events happening within a fixed interval is counted. Below you see a simulation of 6 experiments with intervals having exponential distribution. Each event is shown as a vertical dash. The number of dashes between the red lines (displayed to the right) has a Poisson distribution, in this case with \\(\\lambda\\)=2.5. Clearly, the Poisson distribution is a discrete distribution (about counts), in contrast to the exponential distribution, which is continuous. It looks like this: plot(0:10, dpois(0:10, lambda = 2.5), type=&#39;h&#39;, xlab=&#39;number of events&#39;, ylab=&#39;probability&#39;, lwd=10) The pmf of the Poisson distribution is \\[Pr(X=k|\\lambda) = \\frac{\\lambda^k}{k!} e^{-\\lambda}\\] Exercises Show by simulation, that the Poisson distribution results from an exponential distribution of intervals. Use the cumulative sum of intervals from an exponential distribution to decide whether you have generated enough events. Show solution endtime &lt;- 50 draws &lt;- 15 # should lead in all cases to a cumulative sum &gt; endtime set.seed(7) d &lt;- t(sapply(1:10000, function(y) {cumsum(rexp(draws, rate=0.05))})) # If the following is not true, we did not make enough random draws from the exponential # distribution in at least one of the simulations. Increase &quot;draws&quot; if (!all(d[,15]&gt;endtime)) { stop(&quot;Increase draws&quot;) } pcounts &lt;- apply(d,1,function(x){sum(x &lt;= endtime)}) h &lt;- hist(pcounts, breaks=-1:12, plot=FALSE) plot(x=(0:12), y=h$density, type=&#39;h&#39;, xlab=&quot;counts&quot;, ylab=&quot;probability&quot;, lwd=2) points(x=(0:12), y=dpois(0:12,lambda=0.05*endtime), col=&quot;red&quot;, pch=&#39;-&#39;, cex=2) Deadly car accidents were counted in five consecutive years: ## [1] 926 914 922 885 914 Your government states that the average number of deadly car accidents is 850 per year. Do you agree with this hypothesis? Show solution A two sided 95% confidence interval would be qpois(c(0.025,0.975), lambda=mean(d)) ## [1] 853 972 which shows that the real number of deadly accidents occurring per year is likely to be higher than 850. One of the most notable properties of the Poisson distribution is that the variance equals the mean, and both are equal to the characterizing parameter \\(\\lambda\\) (see table above). The variance, which is the square of the standard deviation, equals \\(\\lambda\\) as well. This property is often used to find out whether a series of counts could have been drawn from a Poisson distribution. Just compare the mean of these counts to their variance. If they are almost equal, the counts could come from a Poisson distribution. 11.4 The Bernoulli and binomial distributions The event space of the Bernoulli and binomial distributions has two types of events instead of one, as for the exponential and Poisson distributions. With every trial, either one of these events occurs. Usually these two types are decribed as “success” and “failure”, “head” and “tail” or “A”, “B”, etc. One such a trial is called a Bernoulli trial. The Bernoulli distribution is characterized by a single parameter \\(\\theta\\), which is the probability that “success”, “head”, “A”, or “1” occurs. If we always use a random variable \\(X\\) that has the event space \\(\\{1,0\\}\\) (another two-valued event space can always be mapped to this one) then the Bernoulli pmf can be conveniently written as \\[Pr(X=k|\\theta) = \\theta^k (1-\\theta)^{1-k}, \\quad \\text{where }k=0\\text{ or }1\\] The binomial distribution gives the probability for the number of number of times that event “1” occurs when performing \\(N\\) Bernoulli trials, in which the single Bernoulli trial probability for event “1” equals \\(\\theta\\). The pmf of the binomial distribution equals \\[Pr(X=k|N,\\theta) = \\binom{N}{k} \\theta^k (1 - \\theta)^{N - k}\\] Exercises There is no (standard) function to draw samples from a Bernoulli distribution in R. However, it can very easily be simulated with draws from another distribution. Write a line in R that returns 100 samples from the Bernoulli distribution with \\(\\theta = 0.6\\). Note that each sample is either a 0 or a 1. Show solution Hundred draws from the uniform distribution can be used. If its outcome is less than \\(\\theta\\) yield a 1, otherwise yield a 0. ifelse(runif(100) &lt; 0.6, 1, 0) ## [1] 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 ## [36] 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 ## [71] 0 1 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 Another option would be to make 100 single draws from the binomial distribution with \\(N=1\\) and \\(\\theta=0.6\\): sapply(1:100, function(dummy) {rbinom(1,1,0.6)}) When \\(\\theta\\) is small, in the order of a few percent, the binomial distribution approaches the Poisson distribution with \\(\\lambda = N \\theta\\). Show this by simulation. Show solution n &lt;- 100 p &lt;- 0.02 set.seed(7) d &lt;- rbinom(10000, size=n, prob=p) h &lt;- hist(d, breaks=-1:15, plot=FALSE) plot(x=(0:15), y=h$density, type=&#39;h&#39;, xlab=&quot;counts&quot;, ylab=&quot;probability&quot;, lwd=2) points(x=(0:15), y=dpois(0:15, lambda=p*n), col=&quot;red&quot;, pch=&#39;-&#39;, cex=2) Using the table (above) of characteristics of these distributions, make sure that the mean and variance of the binomial distribution approach that of the Poisson distribution when \\(\\theta\\) is small Show solution Clearly, the mean \\(n \\theta\\) equals \\(\\lambda\\), even when \\(\\theta\\) is large. The variance of the binomial distribution, \\(n \\theta (1 - \\theta)\\) approaches \\(n \\theta\\) when \\(\\theta\\) is small (because \\(1 - \\theta \\approx 1\\) with small \\(\\theta\\)) and hence approaches \\(\\lambda\\), the variance of the Poisson distribution. (Difficult) Show that in the limit of \\(N \\to \\infty\\) when \\(N \\theta = \\lambda\\) is constant, the pmf of the binomial distribution approaches that of the Poisson distribution. Use the identity \\(e^{-x} = \\lim_{n \\to \\infty} \\left( 1 - \\frac{x}{n} \\right)^n\\). Show solution We have \\(\\theta = \\frac{\\lambda}{N}\\). This allows us to re-write the pmf for the binomial distribution as \\[\\lim_{N \\to \\infty} \\frac{N!}{(N-k)!k!} \\left( \\frac{\\lambda}{N} \\right)^k \\left( 1 - \\frac{\\lambda}{N} \\right)^{N - k} = \\lim_{N \\to \\infty} \\frac{N!}{(N-k)!N^k} \\cdot \\frac{\\lambda^k}{k!} \\cdot \\left( 1 - \\frac{\\lambda}{N} \\right)^{N - k} =\\] \\[\\lim_{N \\to \\infty} \\frac{N-k+1}{N} \\cdot \\frac{N-k+2}{N} \\ldots \\frac{N-k+k}{N} \\cdot \\frac{\\lambda^k}{k!} \\cdot \\left( 1 - \\frac{\\lambda}{N} \\right)^{N} \\cdot \\left( 1 - \\frac{\\lambda}{N} \\right)^{-k}\\] Since \\(k\\) is a finite quantity, terms like \\(\\frac{N-k+1}{N}\\) will approach \\(1\\) as \\(N \\to \\infty\\). Also the term \\(\\left( 1 - \\frac{\\lambda}{N} \\right)^{-k}\\) will approach \\(1\\). Therefore, in the limit this product will equal \\[\\frac{\\lambda^k}{k!} \\cdot e^{-\\lambda}\\] which is the formula for the Poisson pmf. 11.5 The normal and standard normal distribution The standard normal distribution will occur a number of times below. It is the normal distribution with mean=0 and standard deviation=1. The pdf of the normal distribution is \\[f(x|\\mu,\\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\left( \\frac{x-\\mu}{\\sigma} \\right)^2}\\] The normal distribution is best known for its occurrence in the Central Limit theorem, which says that a sum of independent random variables with finite means and standard deviations approaches the normal distribution. This property is thought to be at the heart of the reason why many experimentally measured variables have normal-distributed error: the error may be the sum of many random processes ccontributing to the measured value. It is always possible to convert a normal-distributed variable \\(x\\) with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) to a standard normal distributed variable. Namely, the variable \\(y\\) defined as \\[ y = \\frac{x - \\mu}{\\sigma} \\] will be standard normal distributed. The problem is, of course, that we ususally do not know the population mean and standard deviation, \\(\\mu\\) and \\(\\sigma\\). We can only obtain estimates of these by calculating the sample mean and standard deviations, \\(m\\) and \\(s\\) from a sample of a population. Using these instead of \\(\\mu\\) and \\(\\sigma\\) would not yield a standard normal distributed variable \\(y\\), unless the sample used for estimating mean and standard deviation would be large! Exercise Show that the quantity \\[ y = \\frac{x - mean(x)}{sd(x)} \\] does not have a standard normal distribution when \\(x\\) is a sample of 5 draws from a normal distribution. To be able to show the distribution of \\(y\\) you should calculate it 10000 times. Make draws from a normal distribution with population mean 7 and standard deviation 0.6. Can you explain why it doesn’t have the standard normal distribution? Also show that with a sample size of 50 instead of 5, the distribution of \\(y\\) does approach that of the standard normal distribution. Show solution n &lt;- 5 d &lt;- unlist(lapply(1:10000, function(x) { sample &lt;- rnorm(n, mean=7, sd=0.6) normalized &lt;- (sample - mean(sample))/sd(sample) return(normalized) })) hist(d,nclass=50,col=&#39;grey&#39;,freq=FALSE, ylim=c(0,0.45)) curve(dnorm(x,0,1),-10,10,add=TRUE,n=501,lwd=2) But with sample size 50: n &lt;- 50 d &lt;- unlist(lapply(1:10000, function(x) { sample &lt;- rnorm(n, mean=7, sd=0.6) normalized &lt;- (sample - mean(sample))/sd(sample) return(normalized) })) hist(d,nclass=50,col=&#39;grey&#39;,freq=FALSE, ylim=c(0,0.45)) curve(dnorm(x,0,1),-10,10,add=TRUE,n=501,lwd=2) 11.6 Student’s t-distribution Somebody tells you that the average hight \\(\\mu\\) of dutch men is 180 cm. You want to test this and take a small (\\(n=5\\)) random sample from the population of dutch men. Clearly the mean of your sample will not be exactly 180 cm, but how much is it allowed to deviate from this value before you conclude that the hypothesis is wrong? Let’s simulate this case. Let’s say that the standard deviation of the hight is 5 cm: set.seed(4) sample &lt;- rnorm(n=5, mean=180, sd=5) sample ## [1] 181.0838 177.2875 184.4557 182.9799 188.1781 mean(sample) ## [1] 182.797 sd(sample) ## [1] 4.032067 How do you statistically test this? For such cases the statistician William Sealy Gosset, working under the pseudonym “Student”, invented the distribution named after his pseudonym: Student’s t-distribution. If you have a sample of size \\(n\\) with a sample mean \\(m\\) and a sample standard deviation \\(s\\), and you calculate the quantity \\[ T = \\frac{m - \\mu}{s/\\sqrt{n}} \\] then the distribution of this \\(T\\) is known in case that the population mean is indeed \\(\\mu\\) and the population has a normal distribution (the null hypothesis). This distribution will be equal to the t-distribution with \\(n-1\\) degrees of freedom. In case of our sample of 5, the probability density function of T would look like the one shown in figure 11.1: curve(dnorm(x,mean=0,sd=1), from=-7, to=7, n=500, lwd=2, xlab=&#39;T or x&#39;, ylab=&#39;probability density&#39;, lty=&#39;dashed&#39;, col=&#39;grey&#39;) curve(dt(x,df=1), from=-7, to=7, n=500, lwd=2, col=&#39;red&#39;, add=TRUE) curve(dt(x,df=4), from=-7, to=7, n=500, lwd=2, col=&#39;darkgreen&#39;, add=TRUE) Figure 11.1: T-distribution with degrees of freedom = 1 (solid red line), degrees of freedom = 4 (solid green line) and standard normal distribution (dashed line). The t-distribution has heavier tails than the standard normal distribution, especially at low degrees of freedom. For reference, we draw the standard normal distribution in the same plot. Clearly, the t-distribution has “more weight” in the tails. This effect decreases with increasing sample size (degrees of freedom), mainly because s, the estimate of the standard deviation, becomes more accurate with increasing sample size. If T=0, the observed mean would be equal to that of the null hypothesis. In our case, T equals \\(\\frac{m - 180}{s/\\sqrt{5}}=\\frac{182.8 - 180}{4.0/\\sqrt{5}}=1.6\\). What is the chance of obtaining an absolute value for T that deviates by this amount or more from 0? For that, we need to know the surface of the tails \\(\\leq -1.6\\) and \\(\\geq1.6\\). Since the distribution is symmetric around 0, we can take 2 \\(\\times\\) the integral under the curve \\(\\geq1.6\\), which equals 2 * (1-pt(q=(mean(sample)-180)/(sd(sample)/sqrt(5)), df=4)) ## [1] 0.1958112 This is much larger than the usual 5% confidence level at which we refute nulll hypothesses, so we accept the null hypothesis in this case. An alternative and easier way of performing this statistical test in R is to use the function t.test(): t.test(sample, mu=180) ## ## One Sample t-test ## ## data: sample ## t = 1.5511, df = 4, p-value = 0.1958 ## alternative hypothesis: true mean is not equal to 180 ## 95 percent confidence interval: ## 177.7905 187.8035 ## sample estimates: ## mean of x ## 182.797 What we saw above is a one-sample t-test. Another way in which the t-distribution is often used is in the two-sample t-test, where we compare the means of samples of sizes \\(n_1\\) and \\(n_2\\) from two normal distributions and ask whether the population means are the same. It is similar to asking whether the difference between the population means is equal to 0. If both normal distributions have the same population standard deviation (\\(\\sigma\\)), then it can be shown that the quantity \\[ T = \\frac{m_1 -m_2}{s_{12} \\cdot \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} \\] with \\[ s_{12} = \\sqrt{\\frac{(n_1 - 1) \\cdot s_1^2 + (n_2 - 1) \\cdot s_2^2}{n_1 + n_2 -2}} \\] has a t-distribution with \\(n_1 + n_2 - 2\\) degrees of freedom. If the population standard deviations are not the same an approximation of the t-distribution is used (see for example wikipedia on this topic). These issues are taken care of by the t.test() function as long as you provide it with the correct value for the parameter var.equal parameter(TRUE for equal variances/standard deviations (variance = \\(\\sigma^2\\)) and FALSE otherwise). Exercise Yet another application of the t-test is the paired t-test. An example of an application would be when we compare measurements on subjects before and after a treatment. If the standard deviation of the effect of the treatment is expected to be much lower or in the same order of magnitude as the measurement before the treatment, then studying the individual differences (after - before) yields more accurate results than studying the avergages of the measurements before and after the treatment. For example, the effect of having spent a night awake on the intellectual performance is measured in a population of students. Clearly, students differ from the beginning in this performance, and an additive effect on this initial performance is expected: Table 11.1: Scores on tests before and after a night awake. student before after 1 6.1 5.3 2 6.7 6.8 3 8.3 8.9 4 3.4 2.0 5 6.7 5.7 6 7.5 6.7 The question in a paired t-test is whether the average difference (after - before) is significantly different from 0. Perform this test, and show the difference between the paired and unpaired version. What is the mean effect of spending a night awake on performance, and what is its 95% confidence interval? 11.7 The chi-squared distribution The \\(\\chi^2\\)-distribution with \\(k\\) “degrees of freedom” arises when we make a sum of squares of \\(k\\) independent standard normal distributed quantities \\(Z_i\\) (hence, with mean 0 and standard deviation 1). I.e. we call the distribution of \\[ \\chi^2 = \\sum_{i=1}^k Z_i^2 \\] “chi-squared distributed with \\(k\\) degrees of freedom”. The \\(\\chi^2\\)-distribution has only one characterizing parameter, namely \\(k\\). To construct this distribution, suppose we make 6 draws from a standard normal distribution, and we square and sum those quantities. How is this sum of squares distributed if we would make those 6 draws multiple times (say 10000 times)? # Using a for loop: # ssq.distr &lt;- c() # for (i in 1:10000) { # x &lt;- rnorm(n=6, mean=0, 1) # ssq.distr &lt;- c(ssq.distr, sum(x^2)) #} # Or, with a one-liner: ssq.distr &lt;- sapply(1:10000, function(x) {sum(rnorm(n=6, mean=0, sd=1)^2)}) hist(ssq.distr, col=&#39;grey&#39;, nclass=50, freq=FALSE) # for comparison, the chi-square probability density function curve(dchisq(x,df=6), from=0, to=35, n=500, col=&#39;red&#39;, lwd=2, add=TRUE) 11.7.1 Application in Pearson’s chi-square goodness of fit test Now we know where the \\(\\chi^2\\) distribution originates from. We can use it, for example, to make a goodness of fit test that tells us how well a sample of \\(k\\) data points fits to a curve (a model, expected values). If it is within the 95% quantile bounds predicted by the \\(\\chi^2\\) distribution, we could conclude that the data fit the curve very well, because the sum of squares that we obtain is close to what we expect from a normal-distributed error. If the model fits perfectly, we expect the mean of the errors (\\(\\text{measured value} - \\text{expected value}\\)) to be 0. However, we do need to make an estimate of the standard deviation of the error, because it is unlikely to be equal to 1, the standard deviation of the standard normal distribution. For continuous values you would need to estimate the standard deviation independently, for example by replicate measurements. You often don’t have these available. For discrete values, like expected counts in a bin of a histogram (\\(E_i\\)), however, we have an estimate of the standard deviation. It is equal to the square root of the expected count (\\(\\hat{\\sigma}_i=\\sqrt{E_i}\\)). Clearly, count-data, are only approximately normal-distributed. Therefore, the \\(\\chi^2\\) distribution provides only an estimate of the true distribution of these values. The number of degrees of freedom depends on the number of constraints (equations) under which the expected values are calculated. Let’s try an example with counting males and females in a sample of 20 people from a population: males &lt;- 14 females &lt;- 6 Is this sample an indication that the population average deviates from a 1:1 distribution of males and females (the null hypothesis)? In a sample of in total 20 individuals we would expect 10 males and 10 females. The \\(\\chi^2\\) is: exp.males &lt;- 10 exp.females &lt;- 10 X2 &lt;- (males - exp.males)^2/exp.males + (females - exp.females)^2/exp.females X2 ## [1] 3.2 How many degrees of freedom (\\(k\\)) do we have? Since the total sample size is fixed, the expected number of males equals the total number minus the expected number of females: we have 1 relation between expected males and females. Therefore, \\(k = 2 - 1 = 1\\). We test whether this value of \\(\\chi^2\\) is larger than expected under the null hypothesis: pchisq(X2, 1, lower.tail=FALSE) ## [1] 0.07363827 We test whether the upper tail of the distribution, beyond our \\(\\chi^2\\) value is lower than 0.05, which it is not. For the case of two classes (male, female) a more accurate estimation would be obtained by using the binomial distribution function . pbinom(min(males,females),20,0.5) ## [1] 0.05765915 The p-value using this distribution is also, but just barely, above 0.05. Exercise A researcher designs a new method to predict the binding of a compound to a protein, based on protein sequence. The predictions are ranked using an independent method as “good”, “fair” or “bad”. The current best method yields 55% good, 10% fair and 35% bad predictions. He uses the new method on a set of 230 proteins. His method yields 113 good, 35 fair and the remainder bad predictions. Does his method yield a distribution that differs significantly from the previous best method? Does his method yield better predictions? Show solution # expected fractions: frac.exp &lt;- c(0.55, 0.1, 0.35) # total sample size ntot &lt;- 230 # observed counts cnt.obs &lt;- c(113, 35, ntot-113-35) # expected counts cnt.exp &lt;- frac.exp * sum(cnt.obs) X2 &lt;- sum((cnt.obs - cnt.exp)^2/cnt.exp) pchisq(X2, df=3-1, lower.tail = FALSE) ## [1] 0.02096784 So, yes this distribution differs significantly from that of the best method. However, the ‘good’ category has a lower fraction, whereas the ‘fair’ category has a higher fraction than the best method. You could compare the total OK-ish ones (good + fair), which is 64% in the new method, compared to 65% in the previous, which is not significantly different: # expected fractions: frac.exp &lt;- c(0.55 + 0.1, 0.35) ntot &lt;- 230 cnt.obs &lt;- c(113 + 35, ntot - 113 - 35) cnt.exp &lt;- frac.exp * sum(cnt.obs) X2 &lt;- sum((cnt.obs - cnt.exp)^2/cnt.exp) pchisq(X2, df=2-1, lower.tail = FALSE) ## [1] 0.8357244 Hence, samples have moved from the good to the fair category, but not from bad to good or fair. There is no improvement compared to the previous method, rather the method performs worse. 11.8 The F-distribution The F- distribution, with \\(d_1\\) and \\(d_2\\) degrees of freedom, arises as the ratio \\[ F = \\frac{U_1/d_1}{U_2/d_2} \\] of two chi-square distributed quantities, \\(U_1=\\sum_i Z_{1i}^2\\) and \\(U_2=\\sum_j Z_{2j}^2\\), with \\(d_1\\) and \\(d_2\\) degrees of freedom. In most statistical tests, like ANOVA, the question is whether this ratio is significantly larger than 1, so whether the scaled sum of squares \\(U_1/d_1\\) is larger than the scaled sum of squares \\(U_2/d_2\\). Note that we can calculate this test statistic also when \\(Z_{1i}\\) and \\(Z_{2i}\\) are not standard normal-distributed, as long as they all have \\(\\mu=0\\), and equal but possibly unknown variances \\(\\sigma^2\\). because the random variables \\(Z_{1i}/\\sigma\\) and \\(Z_{2j}/\\sigma\\) will be standard normal-distributed. Taking the ratio: \\[ \\frac{U_1/(d_1 \\cdot \\sigma^2)}{U_2/(d_2 \\cdot \\sigma^2)} = \\frac{U_1/d_1}{U_2/d_2} = F \\] we see that we do not need to know the variances, as long as they are equal. 11.8.1 Analysis of variance The most important application of the F-distribution is in the analysis of variance (ANOVA). The null hypothesis tested in ANOVA is that the distributions of \\(U_1\\) and \\(U_2\\) are equal, with alternative hypothesis that the average of \\(U_1\\) is larger than the average of \\(U_2\\). i.e. \\(F &gt; 1\\), i.e. we perform a one-sided test in ANOVA. This is because we always calculate \\(U_2\\) as the residual sum of squares of a model with more parameters than the model that gave the residual sum of squares \\(U_1\\). So, it must be true that \\(U_2 \\leq U_1\\). The question then is whether also the residuals sum of squares scaled by the degrees of freedom are different: \\(\\frac{U_2}{d_2} &lt; \\frac{U_1}{d_1}\\), and if so, whether the difference is significant. In fact, the test used in ANOVA is a little more subtle, because there we test whether \\(\\frac{U_2}{d_2} &lt; \\frac{U_1 - U_2}{d_1 - d_2}\\), as you will see below. Comparing alternative models Suppose you are studying plant growth and want to compare the effect on crop yield of three alternative fertilization methods. With two alternative fertilization methods you could use the t-test to find out whether the average yield differs between the two alternatives. In the case of three or more alternatives you might want to know whether fertilization has any effect on crop yield. This is answered by the classical “one-way ANOVA” test. In fact, what you do here is testing whether one simple model M1, that says that all plants have equal average yield \\(\\mu\\), explains the data just as well as a more complicated model M2 that says that the average yield from treatment group \\(j\\) differs by some amount \\(a_j\\) from the average yield or from the yield of a control group. Say that we measure the yield \\(w_{ij}\\) of \\(i = 1 \\dots n\\) plants in each of \\(j = 1 \\dots k\\) treatment groups. The model formula’s would be for model M1: \\[ w_{ij} = \\mu + \\epsilon_{ij} \\] and for model M2: \\[ w_{ij} = \\mu + a_j + \\epsilon_{ij} \\] where the \\(\\epsilon_{ij}\\) are normal-distributed error terms. The \\(a_j\\) are called the ‘treatment effects’. If they were all equal to 0, model M2 would be equal to model M1. M1 and M2 are linear models which can be fitted to the data \\(w_{ij}\\) with the lm() function in R. Fitting them is actually the same as calculating means of (groups of) measurements, i.e. the best (least squares) estimator for \\(\\mu\\) in M1 and M2 is \\(\\overline{w} = \\frac{\\sum_j \\sum_i w_{ij}}{k \\cdot n}\\) (meaning that \\(\\overline{w}\\) is the average of all weights) and for the \\(a_j\\) in M2 it is \\(\\hat{a_j} = \\frac{\\sum_i w_{ij}}{n} - \\overline{w} = \\overline{w_j} - \\overline{w}\\). This says that the least squares estimate of \\(a_j\\) is the difference between \\(\\overline{w_j}\\), the average weights in treatment group \\(j\\), minus the overall average weight \\(\\overline{w}\\). Clearly, model M2, having the largest number of parameters, will yield the best fit to the data. It will have the lowest residual sum of squares (RSS). Let’s call the residual sum of squares of M1 \\(\\text{RSS}_1\\) and that of M2 \\(\\text{RSS}_2\\). For each of the models these are: \\[ \\text{RSS}_1 = \\sum_j \\sum_i (w_{ij} - \\overline{w})^2 \\] and \\[ \\text{RSS}_2 = \\sum_j \\sum_i (w_{ij} - (\\overline{w} + \\hat{a_j}))^2 = \\sum_j \\sum_i (w_{ij} - (\\overline{w} + (\\overline{w_j} - \\overline{w})))^2 = \\sum_j \\sum_i (w_{ij} - \\overline{w_j})^2 \\] The additional parameters of the full model M2 lead to a reduction of the RSS relative to the reduced model M1 of size \\(\\text{RSS}_1 - \\text{RSS}_2\\). Are the additional parameters in M2 justified to obtain this reduction? The test statistic for this question is \\[ F = \\frac{(\\text{RSS}_1 - \\text{RSS}_2)/(d_1 -d_2)}{\\text{RSS}_2/d_2} \\] where \\(d_1\\) and \\(d_2\\) are the numbers of degrees of freedom of M1 and M2. The number of degrees of freedom of M1 and M2 equals \\(n - p\\), where \\(n\\) is the number of measurements and \\(p\\) is the number of parameters in the model. For M1 this is \\(n -1\\), however for M2 it is, perhaps somewhat unexpectedly, \\(n - 3\\). That’s unexpected at first sight, because M2 has four parameters (\\(\\mu\\), \\(a_1\\), \\(a_2\\) and \\(a_3\\)). However, the model is fully determined by three parameters, meaning that a three-parameter model would yield exactly the same RSS as a four-parameter model. In fact, only the three parameters \\(b_j = \\mu + a_j\\) are fitted when performing linear regression. In the case that M1 fits the data perfectly (null hypothesis), apart from some normal-distributed error, the reduction in RSS per reduced degree of freedom will be equal to the RSS per degrees of freedom of model M2, and F should be somewhere near 1. Alternatively, a value \\(F \\gg 1\\) would indicate a significant reduction in RSS. In many statistics books you will see an alternative formulation for the test statistic for this problem. The RSS of the data is split into a “within group RSS” and “between group RSS”: \\[ \\sum_j \\sum_i (w_{ij} - \\overline{w})^2 = \\underbrace{\\sum_i \\sum_j (w_{ij} - \\overline{w_j})^2}_{\\text{Within group RSS}} + \\underbrace{\\sum_j (\\overline{w_j} - \\overline{w})^2}_{\\text{Between group RSS}} \\] Then the F-statistic for this problem (i.e. whether any of the \\(a_j\\) is significantly different from 0) is written as \\[ F = \\frac{\\text{Between group RSS}/(d1-d2)}{\\text{Within group RSS}/d2} \\] If you work out the formulas, you will see that this F-statistic is equivalent to the one formulated above. I have chosen the formulation above to demonstrate the equivalence with testing significance of parameters in regression models in the section below. Exercise You compare fertilization treatments A, B and C, and weigh three plants per treatment. The weights are: A: 1.26, 1.31, 1.07; B: 1.45, 1.79, 1.34; C:1.07, 0.409, 0.611. Put the data in a data frame that includes the groups as a factor Show solution d &lt;- data.frame(treat=factor(rep(c(&#39;A&#39;,&#39;B&#39;,&#39;C&#39;), each=3)), weight = c(1.26, 1.31, 1.07, 1.45, 1.79, 1.34, 1.07, 0.409, 0.611)) Make a boxplot (just using the plot function) Show solution plot(d) Fit the two alternative models M1 and M2 and calculate their residual sums of squares as well as the degrees of freedom Show solution m1 &lt;- lm(weight~1, d) m2 &lt;- lm(weight~treat, d) rss1 &lt;- sum(residuals(m1)^2) rss2 &lt;- sum(residuals(m2)^2) n &lt;- length(d$treat) df1 &lt;- n - 1 df2 &lt;- n - 3 Calculate the F-statistic as shown above. Is it significantly larger than 1? Show solution F &lt;- ((rss1 - rss2)/(df1 - df2))/(rss2/df2) F ## [1] 8.509283 1 - pf(F,df1-df2,df2) ## [1] 0.01771002 So, the value is significantly higher than 1 Perform the same analysis by only carrying out an ANOVA on model M2, using the anova() function. This function automatically compares it to the simpler model M1 (which is always the same for this type of problem). Show solution anova(m2) ## Analysis of Variance Table ## ## Response: weight ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## treat 2 1.0540 0.52701 8.5093 0.01771 * ## Residuals 6 0.3716 0.06193 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Comparing alternative regression models Suppose that you have a data set consisting of \\((x_i,y_i)\\) pairs, where \\(\\mathbf{x}\\) is the independent and \\(\\mathbf{y}\\) the dependent variable. You want to know whether you should fit a first or second degree polynomial to the data. I.e. model M1: \\[ y_i = a_0 + a_1 x_i + \\epsilon_i \\] or model M2: \\[ y_i = a_0 + a_1 x_i + a_2 x_i^2 + \\epsilon_i \\] where \\(\\epsilon_i\\) is the error term. We call M1 a reduced form of the full model M2, because it is equal to M2, with the constraint that \\(a_2=0\\). Clearly, the model M2 will have a better fit to the data, because it contains an additional parameter. This will be apparent from the lower sum of squared residuals for M2 than for M1, or \\(\\sum \\epsilon_i^2\\) will be smaller for M2 than for M1. The same question as above can be asked: does that additional parameter lead to a reduction in residual sum of squares (\\(\\text{RSS}_1 - \\text{RSS}_2\\)) that warrants an additional parameter? The number of degrees of freedom in \\(\\text{RSS}_1\\) and \\(\\text{RSS}_2\\) are \\(d_1=n-p_1\\) and \\(d_2=n-p_2\\), where \\(n\\) is the number of measurements and \\(p_1\\) and \\(p_2\\) are the number of parameters in M1 and M2, respectively. In the case of the comparison M1 and M2 here, \\(\\text{RSS}_2\\) will have 1 degree of freedom less than \\(\\text{RSS}_1\\). The F-test statistic for this problem is: \\[ F = \\frac{(\\text{RSS}_1 - \\text{RSS}_2)/(d_1 -d_2)}{\\text{RSS}_2/d_2} \\] It can be used to answer the question: is the “quantity of RSS” in the number of degrees of freedom (\\(d1 - d2\\)) significantly larger than in \\(\\text{RSS}_2/d_2\\), i.e. the normalized RSS in the best fitting model? If it is not, then the improvement of fit is not more than expected based just on the additional parameters of M2 when M1 would already fit perfectly well and \\(\\text{RSS}_1\\) contains only experimental noise, and is not due to lack of fit. Exercise You have measurements of some variable \\(y\\) that is a function of another variable \\(x\\): \\(x=0,1,2,3,4\\) and \\(y=-0.179,1.187,2.918,4.124,6.384\\). Plot the data and make two linear models (function lm()), one in which you fit a first-degree polynomial (line) and another where you fit a second-degree polynomial (parabola). To fit a second degree polynomial you should use the R-formula y~x + I(x^2) in the lm() function. Show solution x &lt;- c(0,1,2,3,4) y &lt;- c(-0.179,1.187,2.918,4.124,6.384) plot(x,y,pch=19) m1 &lt;- lm(y~x) m2 &lt;- lm(y~x+I(x^2)) You want to know whether the second-degree polynomial has a significantly better fit than the first-degree polynomial. Obtain the residual sums of squares from the lm objects. Also calculate their degrees of freedom (\\(n - k\\), where \\(n\\) is the number of measurements, \\(k\\) the number of fitted parameters) Show solution ss1 &lt;- sum(residuals(m1)^2) ss2 &lt;- sum(residuals(m2)^2) df1 &lt;- length(x) - 2 df2 &lt;- length(x) - 3 Calculate the F-statistic according to the formula above. Is it significantly larger than 1? Show solution F &lt;- ((ss1-ss2)/(df1-df2))/(ss2/(df2)) 1 - pf(F,(df1-df2),df2) ## [1] 0.3228608 The reduction in sum of squares is not significantly different from what would be expected by the additional parameter in M2 if M1 already fits perfectly well. Perform the same test with the anova() function (just fill in: anova(model1, model2), where model1 and model2 are the lm objects.) Show solution anova(m1,m2) ## Analysis of Variance Table ## ## Model 1: y ~ x ## Model 2: y ~ x + I(x^2) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 3 0.24850 ## 2 2 0.13456 1 0.11394 1.6936 0.3229 "],
["linearmodels.html", "CHAPTER 12 Linear models and ANOVA 12.1 Modeling with factor-type predictor variables 12.2 Two-way ANOVA/factorial ANOVA 12.3 Combinations of numerical and discrete predictors 12.4 The connection between linear models and ANOVA", " CHAPTER 12 Linear models and ANOVA This subject is very large, and this chapter can only give a brief introduction. Much more can be found in for example the book by Faraway (2005). I would also like to recommend a very good introductory book “An introduction to statistical learning” by James et al. (2013). You can obtain the pdf-file of this book for free on the author’s website: http://www-bcf.usc.edu/~gareth/ISL. A number of the techniques that are used in this syllabus are discussed extensively and in very clear language in this book. Use it! The book is accompanied by an enlightening series of weblectures by three of the authors. If you are interested in data analysis, spending 15 hours on these lectures are well worth the investment. In general, a statistical model is a function of some predictor variables \\(x_1\\), \\(x_2\\), \\(\\ldots\\) used to fit measurements of a response variable \\(y\\). The data used to fit the model is called “training data”. The goal of a statistical model is not to fit the training data as well as possible, but to find a function of the predictor variables, that predicts new data as accurately as possible. By new data we mean data that have not been used to construct the model, i.e. that are not in the training data. When using this criterion one often concludes that some or many of the predictor variables need to be discarded from the fitting function, and that the complexity of the function should not be very high. This apparent paradox can be understood from the fact that a complex function of many variables will not only fit an underlying relation between response and predictor variables, but will also fit measurement noise. When a model becomes so complex that it starts to fit noise in the training data then its predictive capacity will deteriorate. More on this subject can be found in chapter 2.1 (“What is statistical learning?”) of James et al. (2013). Linear models use a particular class of functions to fit data, namely those functions in which the parameters \\(\\beta_i\\) to be fitted occur in a linear fashion. This means that the exponents of the \\(\\beta_i\\) are equal to 1. The predictor variables can be present in the function in a linear or non-linear fashion. The most generic way to describe a function used for linear modeling is: \\[ y = \\beta_0 + \\sum \\beta_i \\phi_i(x_1, \\ldots x_n) \\] where the functions \\(\\phi_i(x_1, \\ldots x_n)\\) can be linear or non-linear functions, but they are functions of the predictor variables only. They do not contain parameters to be fitted. An example is \\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 \\] or an other \\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1^2 \\] Data sets to be modeled consist of \\(m\\)-tuples \\((y_j,x_{1j},\\ldots,x_{nj}) \\; \\text{where} \\; j=1 \\ldots m\\)), each measurement represented by one tuple. In general, selecting a good model (“model selection”) from among several functions \\(f(\\beta_0,\\ldots,\\beta_k,x_1,\\ldots,x_n)\\) consists of two parts: Fitting of the functions. This entails finding the combination of values for \\(\\beta_0,\\ldots,\\beta_k\\) that minimizes the sum of squares \\[ \\sum_{j=1}^m \\left ( y_j - f(\\beta_0,\\ldots,\\beta_k,x_1,\\ldots,x_n) \\right )^2 \\] Discarding terms of a well fitting function \\(f\\) that do not contribute to modeling the underlying relation between the response and predictor variables. In practice, this could mean that a variable is entirely left out of the function, or that higher order terms, like \\(x_1^2\\) or \\(x_1 x_2\\) are left out (equivalently, their coefficients can be set to 0). ANOVA is one of the techniques to identify these terms, as I will explain. Although apparently somewhat restricted in their flexibility, linear models are an important and versatile type of model which is used a lot in modern statistics. One reason is that the predictor variables \\(x_i\\) of linear models can be both numerical variables as well as discrete, factor-type variables. Functions with mixtures of both are also allowed. Another is that the terms and coefficients are relatively easy to interpret. Any type of linear model, whether having only factor-type predictor variables, continuous variables or mixtures of these, can be fitted in R using the function lm(). The appropriateness of the fits, i.e. the “significance” of predictive terms can then be analysed (the second phase of model selection) using the anova() function. Lastly, it is relatively easy to obtain confidence intervals for the estimated parameters. 12.1 Modeling with factor-type predictor variables To be able to use the same framework for continuous as well as factor variables, the latter are transformed into so-called “dummy” variables having discrete numerical values that correspond to the levels of a factor variable. To illustrate this, suppose that we have measured the weight \\(y\\) of a sample of males and females from a population, and want to predict the weight using the gender variable \\(x\\). Or we simply want to know whether the weight of males and females differs significantly. Clearly, \\(x\\) is a factor variable with two levels, ‘F’ and ‘M’. We can transform \\(x\\) into a dummy variable \\(x&#39;\\) having numerical levels 0 and 1, corresponding to the levels ‘F’ and ‘M’, respectively. Then we can make a plot of the weight measurements as a function of \\(x&#39;\\) and calculate the best-fitting straight line through it: Figure 12.1: Weight as a function of gender. The gender variable was converted to a numeric dummy variable. The line is a least-squares fit through the data. The data were sampled from a US population that frequented health and fitness clubs. The fitted line has the equation \\[ y = \\beta_0 + \\beta_1 x&#39; \\] The interpretation of the coefficients is as follows: \\(\\beta_0\\), or the intercept, is the average weight of females \\(\\beta_1\\), or the slope, is the difference between the average weight of males and females, since it is the increase of the fitted line over a unit interval. The fit can be performed with the lm() function which creates the dummy variable behind the scenes to calculate the coefficients. Creating the fit (the “model”) and querying its coefficients is easy: model &lt;- lm(weight~gender, d) coefficients(model) ## (Intercept) genderM ## 63.73333 16.78333 This tells us that the average weight of women is 63.7 kg and that the difference between the average weights of males and females is 16.8 kg. In the terminology of linear modeling, such a difference is called a contrast. Now we can ask three questions which are fully equivalent: Is the slope \\(\\beta_1\\) significantly different from 0? Does gender predict body weight? Is the weight of men and women significantly different? All three are answered at the same time by an ANOVA of the fit: anova(model) ## Analysis of Variance Table ## ## Response: weight ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## gender 1 1690.1 1690.08 18.033 0.0003304 *** ## Residuals 22 2061.8 93.72 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The ANOVA shows that the variable \\(x\\) (gender) leads to a sigificant reduction in the residual sum of squares, as indicated by the F-statistic and corresponding p-value (Pr(&gt;F)) of the F-test (see Chapter 11 on the F-distribution). Another way to obtain the same (and more) information is by applying the summary() method on the lm object summary(model) ## ## Call: ## lm(formula = weight ~ gender, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.733 -8.633 0.875 5.883 19.267 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 63.733 2.795 22.806 &lt; 2e-16 *** ## genderM 16.783 3.952 4.247 0.00033 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.681 on 22 degrees of freedom ## Multiple R-squared: 0.4505, Adjusted R-squared: 0.4255 ## F-statistic: 18.03 on 1 and 22 DF, p-value: 0.0003304 The summary() method not only shows the F-statistic, but also the t-statistics of the individual coefficients (i.e. how many times their standard error they differ from 0). This information can be used to judge the predictive contribution of individual coefficients, rather than predictor variables. This is particularly relevant if there are more than two levels in a factor-type predictor variable. How are factor variables translated to dummy variables if they contain more than two levels? Just by adding a dummy variable that can assume values 0 or 1. The coding scheme is worked out below for a factor variable \\(x\\) with three levels A, B and C, which is transformed into two numerical dummy variables \\(x&#39;\\) and \\(x&#39;&#39;\\) \\(x\\) \\(x&#39;\\) \\(x&#39;&#39;\\) A 0 0 B 1 0 C 0 1 Suppose that B and C represent two different fertilization strategies of crops, and that the response variable \\(y\\) is the crop yield, where A is the control treatment without fertilizer. The model equation now contains two variables and three coefficients: \\[ y = \\beta_0 + \\beta_1 x&#39; + \\beta_2 x&#39;&#39; \\] The fit can be interpreted as a plane through the points above each of the corners of a plane defined by the axes \\(x&#39;\\) and \\(x&#39;&#39;\\) Figure 12.2: A model fit on a variable with three levels. We use two dummy predictor variables to represent the levels as the vectors (0,0):A, (1,0):B, and (0,1):C. The response variable, y, is presented as the third dimension, perpendiclar to these two vectors. A model fit can be represented as a plane fitted through the data points. Due to the way that the dummy variables were coded (table above) the interpretation of the variables is as follows: \\(\\beta_0\\), or the intercept is the average yield on treatment A, the control \\(\\beta_1\\), or the slope of the plane in the direction of the \\(x&#39;\\)-axis is the difference between the average yield of treatment B and the control \\(\\beta_2\\), or the slope of the plane in the direction of the \\(x&#39;&#39;\\)-axis is the difference between the average yield of treatment C and the control So, the contrasts comprise comparisons between fertilizers and the control. The fit can again be performed with the lm() function which again implicitly performs the translation into the dummy variables: model &lt;- lm(y~x, data=d) summary(model) ## ## Call: ## lm(formula = y ~ x, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.459 -9.393 1.661 5.542 22.263 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 66.953 4.807 13.928 9.05e-09 *** ## xB 53.723 6.798 7.902 4.26e-06 *** ## xC 71.104 6.798 10.459 2.20e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.75 on 12 degrees of freedom ## Multiple R-squared: 0.9083, Adjusted R-squared: 0.8931 ## F-statistic: 59.46 on 2 and 12 DF, p-value: 5.932e-07 And performing an ANOVA on the fit yields: anova(model) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 2 13740.0 6870.0 59.457 5.932e-07 *** ## Residuals 12 1386.6 115.5 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 So, yes from the ANOVA we conclude that the variable ‘fertilizer treatment’ does have predictive value for the crop yield. The t-statistic and corresponding p-values of the parameters xB and xC suggest that both fertilizers give a higher croip yield. Note that when concluding that a multi-valued factor has an effect, we would usually perform a Tukey honest siginificant difference test, or multiple t-tests on the individual levels compared to the control with multiple-testing correction on the p-values to make a fair comparison. However, the summary() method gives a good indication of the significance of the individual coefficients. 12.1.1 Alternative dummy variable coding schemes Sometimes, an alternative dummy-variable coding scheme is desirable because you are interested in different contrasts. One desired change of coding scheme is often that you would like a different level of a factor variable to be the ofset of the fit. This can be easily achieved. By default, the level represented by the number 1 (remember, factor-vectors are a child class of integer-type vectors) is chosen as the origin for the fit. To change this level, you can use the function relevel() to convert the original factor to one with the desired level at first position. Other dummy coding schemes can be achieved by supplying the contrasts argument of the lm() function with a contrast matrix. Alternatively, many standard coding schemes can be called by using alternative formulations of the model formula. For example, suppose your lab has measured crop yield under reference conditions extremely often and has a very accurate value for the control yield. Then you might decide to report \\(y\\) as the yield difference between plants treated with fertilizers A or B and that default control yield. What you would then need is a coding scheme that looks like this: \\(x\\) \\(x&#39;\\) \\(x&#39;&#39;\\) A 1 0 B 0 1 In other words: all of the treatments are compared to yield difference = 0. The easiest way to achieve this coding or contrast scheme is to use the modeling formula y ~ 0 + x (instead of y ~ x) in the lm() call. This indicates to the lm() function that it should not fit an offset, or equivalently that it should force the fitted plane to go through \\(y=0\\) at the origin. Another contrast scheme would be the one used in Chapter 11, where yields in the fertilization groups are compared to the average yield. The corresponding dummy variable scheme for an experiment with two fertilizer treatments A and B would be (note that this is the scheme used by a t-test) \\(x\\) \\(x&#39;\\) A -1 B 1 Exercises Proper balancing in experimental design is necessary to obtain reliable results from an ANOVA. By this we mean that all levels in a factor predictor variable (or combinations of levels in case of multiple factor predictors) should be represented by approximately equal numbers of data points. Deduce from figures 12.1 and 12.2 why that is desirable. Show solution The figures show that the effects are fits of lines planes or hyperplanes throught the data. If one of the levels would be underrepresented relative to another then that would mean that the slopes of these planes would be determined by different numbers of points. In an extreme case, a slope might be determined by many points in one level and only one in another. Having equal numbers of points would make the estimates of the slopes more accurate. Note that we speak of an experimental design with unbalanced data. However, in many cases it is not a matter of design, but of availability of data. If you do have control over the number of data points per combination of factor-levels, then balancing these is important. Below I discuss another effect of unbalanced data in the context of data with two factor-type predictor variables. Suppose you want to answer the question whether fertilizers B and C have a significant, but also the same effect on crop yield. To answer this question, you need a dummy variable scheme that differs from the one used in figure 12.2. Which scheme do you need? Use figure 12.2 to ask where you should put the points for level C in order to answer that question and then define the corresponding dummy variables. Notice that you should test whether the difference between the levels A and either B or C is significant, but that at the same time that between B and C equals 0. Show solution If you would place the points of level C at (1,1), then the slope in the direction of (0,1) would represent the difference ‘C - B’. Although not part of the question, we show how the fit can be carried out by filling in the contrasts argument of the lm() function: contrast.matrix &lt;- matrix(c(0,0,1,0,1,1), nrow=3, byrow=TRUE) rownames(contrast.matrix) &lt;- c(&#39;A&#39;,&#39;B&#39;,&#39;C&#39;) colnames(contrast.matrix) &lt;- c(&#39;B - A&#39;, &#39;C - B&#39;) contrast.matrix ## B - A C - B ## A 0 0 ## B 1 0 ## C 1 1 Then use this contrast matrix in the linear modeling function: model.alt.contr = lm(y~x, data=d, contrasts = list(x=contrast.matrix)) summary(model.alt.contr) ## ## Call: ## lm(formula = y ~ x, data = d, contrasts = list(x = contrast.matrix)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.459 -9.393 1.661 5.542 22.263 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 66.953 4.807 13.928 9.05e-09 *** ## xB - A 53.723 6.798 7.902 4.26e-06 *** ## xC - B 17.381 6.798 2.557 0.0252 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.75 on 12 degrees of freedom ## Multiple R-squared: 0.9083, Adjusted R-squared: 0.8931 ## F-statistic: 59.46 on 2 and 12 DF, p-value: 5.932e-07 The summary shows that the difference ‘B - C’ that we have encoded in the contrast matrix is only just significant at 0.05 level according to its t-statistic. 12.2 Two-way ANOVA/factorial ANOVA The ANOVA’s shown in the previous examples were all one-way ANOVA’s because they concerned models with a single (factor-type) predictor variable. When models with two or more factor-type predictor variables are investigated using ANOVA, this is called two-way ANOVA in case of two predictor variables or factorial ANOVA in case of two or more predictor variables. There is, apart from the number of predictor variables used, no fundamental difference between one-, two-way and factorial ANOVA. Linear models of two or more factor variables clearly require more complex dummy encoding schemes, but the principle of dummy encoding does not differ fundamentally from that discussed above. An example of a two-way ANOVA and corresponding models is taken from the analysis of plasmid transformation in the industrially important microorganism Corynebacterium glutamicum. Plasmid transformation is an essential experimental technique used to make mutants of an organism. The efficiency of the transformation procedure was investigated under two experimental variations: growth of cells at low or room temperature (“growth temperature treatment”, variable name \\(t\\)) and the application of a heat shock or not to the cells just after the transformation (“heat shock treatment”, variable name \\(h\\)). The data are from Rest, Lange, and Molenaar (1999). Having two levels in each variable, there are four possible combinations of both treatments. A plot of the data is shown below. Due to the way the counts were obtained, the plasmid transformation count data had a proportional error. This implies that the standard deviation \\(\\sigma\\) of the count data is a fixed percentage of the counts obtained. This is a problem when applying ANOVA, because the underlying assumption for the F-test is that the error terms were all sampled from a single distribution with a single constant \\(\\sigma\\). In the count data, the \\(\\sigma\\) increases with the count value itself. Fortunately, this defect in the data can be repaired by taking the logarithm of the original count values as a new response variable. This logarithmic transformation yields a variable with a constant error. Since many experimental data have proportional error, you will find that the logarithmic transform is often applied. See for example the Wiki page on variance-stabilizing transformations. The first model is to assume that the effect of both treatments is additive (on the logarithmic scale. This means it would be multiplicative on the original plasmid transformation scale.) The formula used for this model is log.transforms ~ t + h. The corresponding mathematical formulation of the fitting function would look like: \\[\\begin{equation} y = \\beta_0 + \\beta_1 t + \\beta_2 h\\tag{12.1} \\end{equation}\\] which has three parameters, \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\beta_2\\). Since \\(t\\) and \\(h\\) are (modified to) dummy variables they can only assume the values 0 (no normal growth temperature or no heat shock) or 1 (low growth temperature or with heat shock). When fitting eq (12.1) we effectively fit the following equations in one calculation: \\[\\begin{equation*} y = \\begin{cases} \\beta_0 &amp; \\text{if } t = 0 \\text{,}\\; h = 0 \\\\ \\beta_0 + \\beta_1 &amp; \\text{if } t = 1 \\text{,}\\; h = 0 \\\\ \\beta_0 + \\beta_2 &amp; \\text{if } t = 0 \\text{,}\\; h = 1 \\\\ \\beta_0 + \\beta_1 + \\beta_2 &amp; \\text{if } t = 1 \\text{,}\\; h = 1 \\end{cases} \\end{equation*}\\] After fitting wit lm(), a summary of the additive model is given: model.additive &lt;- lm(log.transforms ~ t + h, data=transforms.x) summary(model.additive) ## ## Call: ## lm(formula = log.transforms ~ t + h, data = transforms.x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.52647 -0.31871 0.00312 0.30079 0.54141 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.9500 0.1682 11.596 3.15e-08 *** ## tlow 1.6346 0.1942 8.418 1.27e-06 *** ## hshock 2.2720 0.1942 11.701 2.83e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3884 on 13 degrees of freedom ## Multiple R-squared: 0.9411, Adjusted R-squared: 0.9321 ## F-statistic: 103.9 on 2 and 13 DF, p-value: 1.012e-08 The summary shows that all coefficients have high t-statistics. From the figure, it could be concluded that the combined effect of growth at low temperature and application of a heat shock is more than the effects of both treatments added together. Whether this is the case can be investihgated by comparintg this mode with one that includes a so-called interaction effect. The model formula that includes interaction effects as well as additive effects is log.transforms ~ t * h. It is equivalent to the formula log.transforms ~ t + h + t:h, that explicitly incorporates the interaction by the term t:h. The corresponding mathematical formulation of the fitting function would look like: \\[ y = \\beta_0 + \\beta_1 t + \\beta_2 h + \\beta_3 t h \\] which has four parameters! You can see that \\(\\beta_3\\) is only added when both \\(t\\) and \\(h\\) are equal to 1, i.e. when cells were grown at low temperature and a heat shock was applied. This leads to the following effective fitting scheme: \\[\\begin{equation*} y = \\begin{cases} \\beta_0 &amp; \\text{if } t = 0 \\text{,}\\; h = 0 \\\\ \\beta_0 + \\beta_1 &amp; \\text{if } t = 1 \\text{,}\\; h = 0 \\\\ \\beta_0 + \\beta_2 &amp; \\text{if } t = 0 \\text{,}\\; h = 1 \\\\ \\beta_0 + \\beta_1 + \\beta_2 + \\beta_3 &amp; \\text{if } t = 1 \\text{,}\\; h = 1 \\end{cases} \\end{equation*}\\] This model gives the following result. model.interaction &lt;- lm(log.transforms ~ t * h, data=transforms.x) summary(model.interaction) ## ## Call: ## lm(formula = log.transforms ~ t * h, data = transforms.x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.20294 -0.12456 -0.01459 0.12891 0.21787 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.27349 0.07717 29.459 1.46e-12 *** ## tlow 0.98757 0.10914 9.049 1.04e-06 *** ## hshock 1.62494 0.10914 14.889 4.23e-09 *** ## tlow:hshock 1.29413 0.15435 8.385 2.32e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1543 on 12 degrees of freedom ## Multiple R-squared: 0.9914, Adjusted R-squared: 0.9893 ## F-statistic: 461.9 on 3 and 12 DF, p-value: 1.17e-12 The t-statistic of the interaction effect is high (consequently, it has a low p-value of \\(2.3\\times 10^{-6}\\)), which indicates that incorporating it could have improved the fit. To be sure, we can compare the residual sums of squares of the two models using the anova() function: anova(model.additive, model.interaction) ## Analysis of Variance Table ## ## Model 1: log.transforms ~ t + h ## Model 2: log.transforms ~ t * h ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 13 1.96064 ## 2 12 0.28588 1 1.6748 70.3 2.318e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 which shows that the incorporation of the interaction term makes the fit significantly better. We can conclude that there is a synergistic effect of the treatments, having a size of approximately 1.29 on the \\(\\text{log}_{10}\\) scale, or a factor 19.7 on the original count scale. Finally, we plot the fits of the two models together with the data. Clearly, ‘fit’ here means a single number for each of the combinations of factor levels. Note that since the interaction model has four parameters, its predicted 10log(transformed cells) levels equal just the averages of the measurements at each of the four combinations. Figure 12.3: Predictions or fits by the additive model (red) and the model with interaction (blue) 12.2.1 Unbalanced data Unbalancedness of data can influence the outcome of an ANOVA for another reason than the one discussed above in an exercise. In a hypothetical firm a sample of the salaries of men and women were obtained (the example is from Zahn (2010), and the data are available in data/linearmodels). They are displayed in figure 12.4. The linear model fit (in this case just the averages of salaries of men and women) shows that there is a slight difference of 0.23 thousand Euro. The difference is not significant: an ANOVA (or t-test) yields a p-value of 0.89. The conclusion from these data could be that there is no gender bias in salary, in this firm. Figure 12.4: Salaries of men and women in a hypothetical firm in thousand Euro. The line is a linear model (salary ~ gender) fitted to the data. However, the level of education of these people was also assessed. If we plot this factor together with gender, a completely different picture arises: Figure 12.5: The same data as above but now differentiated by education. The lines are a linear model (salary ~ gender * education) fitted to the data. As you notice, a model salary ~ gender + education might be more suitable, because the slopes don’t seem to differ much (i.e. there is probably no interaction effect). The slopes in this figure are definitely positive, showing that there is a difference in salary between men and women: men are paid better than women in the same education class. The difference is approximately 2.5 thousand Euro6. Why did we not observe that in figure 12.4? The reason is that the data are unbalanced over the four classes (Female-No degree, Female-Degree, Male-No degree, Male-Degree). The females with degree are over-represented, as are the males without degree. This evens out the difference in salary paid, because people with a degree get paid better than people without degree, by approximately 7.6 thousand Euro. If the data had been balanced over all four groups, the difference in salary between men and women would have been evident from a figure like 12.4. In other words, on a balanced data set an ANOVA (or t-test) with only gender as the predictor variable a significant difference between men and women would have shown up, whereas in the current unbalanced data set it doesn’t. Exercise It appeared that the plasmid transformation efficiency depended on the way in which the plasmid was prepared. The results analysed above are those for plasmid isolated from the bacterium Escherichia coli (the xenogeneic plasmid preparation). If the plasmid was isolated from Corynebactrium glutamicum itself (a syngeneic plasmid preparation), the transformation numbers were different. The entire data set can be found in the file data/linearmodels/plasmidtransformation.tab. Investigate which model fits the data. Show solution Load and adapt the data f &lt;- file.path(baseurl, &#39;data&#39;, &#39;linearmodels&#39;, &#39;plasmidtransformation.tab&#39;) transforms &lt;- read.table(f, sep=&quot;\\t&quot;, head=TRUE) # we want &#39;normal&#39; as the reference level for temperature transforms$t &lt;- relevel(transforms$t, &#39;normal&#39;) transforms$log.transforms &lt;- log(transforms$transforms, 10) We compare a model in which plasmidprep is an additive term to a model in which it is an interaction term. model.total.sum &lt;- lm(log.transforms ~ plasmidprep + t * h, data=transforms) model.total.interact &lt;- lm(log.transforms ~ plasmidprep * t * h, data=transforms) anova(model.total.sum, model.total.interact) ## Analysis of Variance Table ## ## Model 1: log.transforms ~ plasmidprep + t * h ## Model 2: log.transforms ~ plasmidprep * t * h ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 27 11.3823 ## 2 24 1.0161 3 10.366 81.614 9.889e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Which shows that the more complex model is better able to explain the data. The model is so complex, however, that, for the sake of understanding the data, it may be better to fit the data sets for the different plasmid preparations individually. If we fit the data for the syngeneic plasmid praparation individually, we get: transforms.s &lt;- subset(transforms, plasmidprep==&#39;syngeneic&#39;) model.s.sum &lt;- lm(log.transforms ~ t + h, data=transforms.s) model.s.interact &lt;- lm(log.transforms ~ t * h, data=transforms.s) anova(model.s.sum, model.s.interact) ## Analysis of Variance Table ## ## Model 1: log.transforms ~ t + h ## Model 2: log.transforms ~ t * h ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 13 0.77907 ## 2 12 0.73024 1 0.048833 0.8025 0.388 Which shows that the simpler additive model suffices for the syngeneic data. A summary of this model yields summary(model.s.sum) ## ## Call: ## lm(formula = log.transforms ~ t + h, data = transforms.s) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.44544 -0.14049 -0.01294 0.17193 0.34447 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.21062 0.10600 58.589 &lt; 2e-16 *** ## tlow 1.86218 0.12240 15.214 1.16e-09 *** ## hshock 0.07123 0.12240 0.582 0.571 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2448 on 13 degrees of freedom ## Multiple R-squared: 0.9469, Adjusted R-squared: 0.9387 ## F-statistic: 115.9 on 2 and 13 DF, p-value: 5.169e-09 which suggests that the heat shock has no effect at all. An even simpler model then yields model.s.tonly &lt;- lm(log.transforms ~ t, data=transforms.s) anova(model.s.tonly, model.s.sum) ## Analysis of Variance Table ## ## Model 1: log.transforms ~ t ## Model 2: log.transforms ~ t + h ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 14 0.79936 ## 2 13 0.77907 1 0.020294 0.3386 0.5706 The anova confirms our suspicion: for the syngeneic DNA preparation the heat shock has no effect on plasmid DNA transformation efficiency! 12.3 Combinations of numerical and discrete predictors In the linear modeling terminology this is also called ANCOVA (analysis of covariance), because the numerical predictor variables are also called covariates. However again, there is no fundamental difference between this type of model and the previous ones, nor is there a fundamental difference between ANOVA on models with only discrete predictor variables, numerical predictor variables or mixtures of these7. As an example, let’s get back to the data that indicated that there is a relation between weight and gender. It is likely that weight is correlated to body height, and one could propose the hypothesis that the relation between gender and weight is actually an indirect one: namely through body height. On average, men are a bity taller than women which could explain the relation observed above. In fact the gender might not have any predictive value anymore if we know body height. We can investigate this hypothesis from the fulll data set found in data/linearmodels. If we plot weight as a function of height we get the following picture: It shows a clear relation between height and weight, and it seems like it could be a linear relation. A single straight line through the data was fitted as follows: onelinemodel &lt;- lm(weight~height, data=bodymetrics) summary(onelinemodel) ## ## Call: ## lm(formula = weight ~ height, data = bodymetrics) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.743 -6.402 -1.231 5.059 41.103 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -105.01125 7.53941 -13.93 &lt;2e-16 *** ## height 1.01762 0.04399 23.14 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.308 on 505 degrees of freedom ## Multiple R-squared: 0.5145, Adjusted R-squared: 0.5136 ## F-statistic: 535.2 on 1 and 505 DF, p-value: &lt; 2.2e-16 The question is whether males and females should be fitted on a single line, or on separate lines. It seems, for example, that the majority of females falls below the common line. Fitting two straight lines, one for males, another for females might yield a better fit. This can be done with one command using the lm() function with the formula weight ~ height * gender, which is equivalent to the formula weight ~ height + gender + height:gender. It includes an additive effect of the two variables gender and height as well as an interaction effect height:gender. The mathematical formula being fitted in terms of the original height variable and the dummy variable gender' is: \\[\\begin{equation} \\text{weight} = \\beta_0 + \\beta_1 \\, \\text{gender&#39;} + (\\beta_2 + \\beta_3 \\, \\text{gender&#39;}) \\, \\text{height}\\tag{12.2} \\end{equation}\\] The four parameters have the following interpretation if the dummy variable equals 0 for females and 1 for males: \\(\\beta_0\\): the offset of the line fitted through female points \\(\\beta_1\\): the difference between the offset for lines fitted through male and female points \\(\\beta_2\\): the slope of the line fitted through female points \\(\\beta_3\\): the difference between the slopes of the lines fitted through the male and female points Essentially, when fitting to eq (12.2) we make the following fits in a single calculation: \\[\\begin{equation*} \\text{weight} = \\begin{cases} \\beta_0 + \\beta_2 \\, \\text{height}, &amp; \\text{if gender&#39;} = 0 \\, \\text{(females)} \\\\ \\beta_0 + \\beta_1 + (\\beta_2 + \\beta_3) \\, \\text{height}, &amp; \\text{if gender&#39;} = 1 \\, \\text{(males)} \\end{cases} \\end{equation*}\\] The result is: twolinesmodel &lt;- lm(weight ~ height * gender, data = bodymetrics) summary(twolinesmodel) ## ## Call: ## lm(formula = weight ~ height * gender, data = bodymetrics) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.187 -5.957 -1.439 4.955 43.355 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -43.81929 13.77877 -3.180 0.00156 ** ## height 0.63334 0.08351 7.584 1.63e-13 *** ## genderM -17.13407 19.56250 -0.876 0.38152 ## height:genderM 0.14923 0.11431 1.305 0.19233 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.795 on 503 degrees of freedom ## Multiple R-squared: 0.5682, Adjusted R-squared: 0.5657 ## F-statistic: 220.7 on 3 and 503 DF, p-value: &lt; 2.2e-16 You should recognize the four parameters in the summary, and you may notice that neither of the two parameters added to the model, relative to the onelinemodel has a significant t-statistic. This may point to an over-fitted model. The ANOVA in the summary compares the residual sum of squares of the model relative to the nullmodel which is the model that predicts the weight as the mean of all weights. However, we want to compare this model with two lines to the model with one straight line. This can be done using the anova() function: anova(onelinemodel, twolinesmodel) ## Analysis of Variance Table ## ## Model 1: weight ~ height ## Model 2: weight ~ height * gender ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 505 43753 ## 2 503 38912 2 4841.5 31.292 1.553e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 which shows that the twolines model does fit significantly better on the data using the two additional degrees of freedom. The fit with two lines is shown in the figure below: It suggests why having two additional parameters might lead to over-fitting of the data: the slopes of the two lines differ only slightly, and might actually be the same. Fitting two lines with a single slope but different offsets is done with the lm() function using the formula weight ~ height + gender, i.e without the interaction effect of the previous model. The mathematical formula of this fit is \\[\\begin{equation} \\text{weight} = \\beta_0 + \\beta_1 \\, \\text{gender&#39;} + \\beta_2 \\, \\text{height}\\tag{12.3} \\end{equation}\\] which is a model with three instead of four parameters. Essentially, when fitting to eq (12.3) we make the following fits in a single calculation: \\[\\begin{equation*} \\text{weight} = \\begin{cases} \\beta_0 + \\beta_2 \\, \\text{height} &amp; \\text{if gender&#39;} = 0 \\, \\text{(females)} \\\\ \\beta_0 + \\beta_1 + \\beta_2 \\, \\text{height} &amp; \\text{if gender&#39;} = 1 \\, \\text{(males)} \\end{cases} \\end{equation*}\\] The interpretation of the parameters is: \\(\\beta_0\\): the offset of the line fitted through female points \\(\\beta_1\\): the difference between the offset for lines fitted through male and female points \\(\\beta_2\\): the slope of the lines fitted through male and female points singleslopemodel &lt;- lm(weight ~ height + gender, data = bodymetrics) summary(singleslopemodel) ## ## Call: ## lm(formula = weight ~ height + gender, data = bodymetrics) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.184 -5.978 -1.356 4.709 43.337 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -56.94949 9.42444 -6.043 2.95e-09 *** ## height 0.71298 0.05707 12.494 &lt; 2e-16 *** ## genderM 8.36599 1.07296 7.797 3.66e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.802 on 504 degrees of freedom ## Multiple R-squared: 0.5668, Adjusted R-squared: 0.5651 ## F-statistic: 329.7 on 2 and 504 DF, p-value: &lt; 2.2e-16 The addition of the gender variable no does show a high value for the t-statistic. We can compare the residual sums of squares of the two models using the anova() function: anova(singleslopemodel, twolinesmodel) ## Analysis of Variance Table ## ## Model 1: weight ~ height + gender ## Model 2: weight ~ height * gender ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 504 39043 ## 2 503 38912 1 131.84 1.7043 0.1923 It shows that the addition of an interaction effect (differen slopes for female and male points) does not significantly reduce the residual sum of squares, as expressed by the low F-statistic (1.7) and correspondingly high p-value of 0.192. Exercises A different model with only three parameters would be one with a single intercept and different slopes. Fit this model and compare it to the model with two independent lines using ANOVA. Show solution A third covariate in the dataset is age. Make a plot of weight as a function of age, possibly seprately for males and females. Try whether adding age to the model improves the predictability of weight. Start with the simplest model without interaction effects on age. Show solution 12.4 The connection between linear models and ANOVA The combination of the two techniques, Linear Modeling and Analysis of Variance, in the title of this chapter is not a coincidence. The question is justified whether ANOVA could not also be applied to residual sums of squares to compare fits of two non-linear models. The answer is that it can … in principle. However, there are two problems: The traditional ANOVA is used as a way to compare models that differ by a single predictor variable or term, to find out whether this variable or term has any predictive value. This procedure is not easily implemented in non-linear equations, where adding a term in a denominator, for example, may completely change the nature of the relation between predictor and response variables, also for the other variables or terms. In contrast to fits with linear equations, the number of degrees of freedom of a fit with a non-linear equation is ususally not well-defined. We need these degrees of freedom to be able to calculate the F-statistic. Hence, the F-statistic can only be calculated if a procedure exists that allows a reasonable estimation of the degrees of freedom of a fit. For model selection with non-linear equations, other criteria than ANOVA, like the Akaike Information Criterion (AIC) and Bayes Information Criterion (BIC) may be more suitable. These measures of statistical appropriateness of fits are not based on the relatively limited assumptions underlying the F-statistic. Consider using the broom package when analyzing statistical models. The output of the broom functions is, in contrast to the standard print and summaries of lm and other model-fit objects, always a data frame. The latter is much easier to extract data, like coefficients and p-values from. For example, instead of summary(singleslopemodel), we could use tidy(singleslopemodel) ## term estimate std.error statistic p.value ## 1 (Intercept) -56.9494889 9.42443771 -6.042747 2.948023e-09 ## 2 height 0.7129752 0.05706608 12.493852 2.149494e-31 ## 3 genderM 8.3659935 1.07295815 7.797129 3.661279e-14 to obtain the coefficients and their statistics, and glance(singleslopemodel) ## r.squared adj.r.squared sigma statistic p.value df logLik ## 1 0.5667784 0.5650593 8.801535 329.6884 2.824545e-92 3 -1820.585 ## AIC BIC deviance df.residual ## 1 3649.17 3666.084 39043.38 504 which yields several quality-of-fit metrics, including AIC and BIC. References "],
["hubble.html", "CHAPTER 13 The age of the universe 13.1 Exercises", " CHAPTER 13 The age of the universe (Techniques: linear model/regression, confidence interval of a fitted parameter) The astronomer Edwin Hubble discovered that there was a proportional relation between the distance of galaxies to Earth and the red-shift of characteristic wavelengths. Galaxies far away from Earth have a higher red-shift, meaning that they are moving away from us at higher speed than galaxies nearby. This was interpreted as an indication for a homogeneous expansion of the universe. Does this mean that we are in a special position in the universe? No, we think that an observer anywhere in the universe sees the same: compare it to dots on an inflating balloon. Every dot wil see every other dot moving away, and will, furthermore, see that dots further away move at higher speed. The balloon, i.e. space itself inflates! The relation between distance of an object and its speed relative to the observer is expressed in Hubble’s empirical law: \\[ y=\\beta x \\] where \\(y\\) is the relative velocity of two galaxies separated by distance \\(x\\), and \\(\\beta\\) is Hubble’s constant. So, when did the balloon start to inflate, i.e. when was all matter very close together and was the speed of all galaxies equal to 0? This gives us the age of the universe since the Big Bang. Assuming that the rate of expansion is constant , this age is equal to \\(1/\\beta\\)! The Hubble space telescope was used in the 1990’s to obtain an estimate of the Hubble constant (Freedman et al. 2001). The data are available from the server (data/hubble). Below, a Hubble diagram of the data from 24 galaxies is shown. Since the Hubble equation defines a linear model (the parameter \\(\\beta\\) appears in a linear fashion in the formula), we can make a least-squares fit using the lm() function. If you would like some background on this exercise, refer to chapter 3 in James et al. (2013). 13.1 Exercises Reproduce the Hubble diagram above. Show solution f &lt;- file.path(baseurl,&#39;data&#39;, &#39;hubble&#39;, &#39;hubble.txt&#39;) d &lt;- read.table(f, header=TRUE, sep=&quot;\\t&quot;) plot(y~x, data=d, xlab=&quot;Distance (Mpc)&quot;, ylab=&quot;Velocity (km/s)&quot;) Using a least squares fit, make an estimation of \\(\\beta\\) and the age of the universe. Use the formula syntax when applying the lm() function. Note that the Hubble equation does not contain an offset: the relative velocity of objects close to Earth is virtually zero! This implies that the fitted line should pass through the origin. The default behavior of lm(), however, is to include an offset. Find out from the description of the formula syntax (part of the “stats” package) or from Chapter 12 how you can force the fit to pass through the origin. One Mpc (Megaparsec) equals \\(3.09 \\times 10^{19}\\) km. Show solution We use the formula y ~ 0 + x (or, equivalently, y ~ x - 1), which forces a fit through \\((0,0)\\). Here we see that the formula syntax in R is not that of a mathematical formula: the -1 does not subtract 1, nor does 0 + add 0 to the x-values. It is just code to make clear that an intercept should not be included in the fit. hubble.fit &lt;- lm(y ~ 0 + x, data=d) summary(hubble.fit) ## ## Call: ## lm(formula = y ~ 0 + x, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -736.5 -132.5 -19.0 172.2 558.0 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## x 76.581 3.965 19.32 1.03e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 258.9 on 23 degrees of freedom ## Multiple R-squared: 0.9419, Adjusted R-squared: 0.9394 ## F-statistic: 373.1 on 1 and 23 DF, p-value: 1.032e-15 We use the coef() method to extract the fitted coefficient from the lm object: hubble.const &lt;- coef(hubble.fit)[&#39;x&#39;]/(3.09*10^19) Yielding a Hubble constant of 2.48E-18 s-1. The age of the universe in years would then be 1/(hubble.const*3600*24*365) ## x ## 12794692825 Plot the residuals and re-analyze the data after removing the putative outliers. Does it make a big difference? I should stress that it is never allowed in scientific reports or papers to leave out putative outliers without explicit mentioning of this fact, as well as of the criteria used to define outliers! The complete data set should always be reported and preferrably also used in plots. Show solution Galaxies numbers 3 and 15 seem to be outliers: plot(fitted(hubble.fit),residuals(hubble.fit)) hubble1.fit &lt;- lm(y ~ 0 + x, data=d[-c(3,15),]) summary(hubble1.fit) ## ## Call: ## lm(formula = y ~ 0 + x, data = d[-c(3, 15), ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -304.3 -141.9 -26.5 138.3 269.8 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## x 77.67 2.97 26.15 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 180.5 on 21 degrees of freedom ## Multiple R-squared: 0.9702, Adjusted R-squared: 0.9688 ## F-statistic: 683.8 on 1 and 21 DF, p-value: &lt; 2.2e-16 hubble1.const &lt;- coef(hubble1.fit)[&#39;x&#39;]/(3.09*10^19) 1/(hubble1.const*3600*24*365) ## x ## 12614854757 We see that the estimated standard error decreases and that the estimated age is a little different. We want to estimate a confidence interval of our estimator \\(\\hat{\\beta}\\) of the Hubble constant. For that, we also need the estimated standard error \\(\\hat{\\sigma}\\) of \\(\\hat{\\beta}\\) (also called \\(SE(\\hat{\\beta})\\)). To obtain \\(\\hat{\\sigma}\\) we need to apply the summary() method to the lm object8. A 95% confidence interval of the parameter \\(\\hat{\\beta}\\) can itself then be estimated as \\[ \\hat{\\beta} \\pm 1.96 \\cdot \\hat{\\sigma} \\] or, because it’s easier to remember, approximately \\[ \\hat{\\beta} \\pm 2 \\cdot \\hat{\\sigma} \\] (Chapter 3.2.1 in James et al. (2013)). Knowing this, you can estimate the 95% confidence interval for a value \\(\\beta\\) of the Hubble constant, given the estimates \\(\\hat{\\beta}\\) and \\(\\hat{\\sigma}\\) derived from the measurements and you can calculate the approximate 95% confidence range of the age of the universe. Another way to find a confidence interval for an estimated parameter is to use the function confint(). Use both methods, and compare the results. Show solution First, we obtain the summary() of the linear fit: res.hubfit &lt;- summary(hubble.fit) Let’s see what kind of information is present in this object (also see help(summary.lm)), in particular in the coefficients sub-element which is a numeric matrix with named dimensions: names(res.hubfit) ## [1] &quot;call&quot; &quot;terms&quot; &quot;residuals&quot; &quot;coefficients&quot; ## [5] &quot;aliased&quot; &quot;sigma&quot; &quot;df&quot; &quot;r.squared&quot; ## [9] &quot;adj.r.squared&quot; &quot;fstatistic&quot; &quot;cov.unscaled&quot; res.hubfit$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## x 76.58117 3.964794 19.3153 1.031907e-15 From this sub-object, which is of the class matrix, we need the “Estimate” and the “Std. Error” columns from the row with the name “x” (here there is only one row, but if there are multiple explanatory variables there will be multiple rows). With this knowledge, we can calculate the desired 95% confidence interval according to the first method: # approximate 95% confidence interval hubble.confint &lt;- (res.hubfit$coefficients[&#39;x&#39;,&#39;Estimate&#39;] + c(-1,1)*res.hubfit$coefficients[&#39;x&#39;,&#39;Std. Error&#39;])/(3.09*10^19) 1/(hubble.confint*3600*24*365) ## [1] 13493272436 12164886968 Hence, according to this appoximation, the 95% confidence interval of age of the universe is between 12.2 and 13.5 billion years. Using the second, more accurate method we obtain: # more accurate using confint() hubble.confint &lt;- confint(hubble.fit, level=0.95)/(3.09*10^19) 1/(hubble.confint*3600*24*365) ## 2.5 % 97.5 % ## x 14329359295 11556949874 so, more like between 11.6 and 14.3 billion years. This estimate is more accurate because the simple method above assumes that the quantity \\[ \\frac{\\hat{\\beta}-\\beta}{\\hat{\\sigma}} \\] is standard-normal-distributed (i.e. normal-distributed with mean=0 and standard deviation=1), whereas it will actually be t-distributed when the measurement error \\(\\epsilon\\) is normal-distributed. A t-distribution with a low number of degrees of freedom (low number of measurements) has larger “tails” than the normal distribution. Therefore we get a larger interval with the second method. The larger tails are caused by the fact that we perform a division by a quantity \\(\\hat{\\sigma}\\) that itself carries an error, and is only an estimate of the true (population) standard deviation \\(\\sigma\\). By the way, you probably noticed that confint() can be used to calculate an arbitrary confidence interval, by setting the argument level References "],
["datafabrication.html", "CHAPTER 14 A case of data fabrication 14.1 Exercises", " CHAPTER 14 A case of data fabrication (Techniques: generating random numbers from a distribution) In 2002 the physicist J.H. Schön was accused of data forgery. In the two previous years he had published 16 papers in Nature and Science, as well as many in other journals. His field of research was condensed matter physics and nanotechnology, in particular different types of conductors and electronic structures. His productivity was phenomenal and in 2001 he was listed on average every eight days as an author on a research paper. However, it soon became clear that a lot, if not most of the published data had been fabricated by Schön. As a justification of his misconduct he commented that he had “polished” or made-up figures to better illustrate the general results that he had obtained in the lab. Others, however, were unable to reproduce even the general trends of his results. To see the size of this misconduct: a wikipedia lemma lists dozens of retracted papers. Here we will investigate one of his data sets, and using a statistical method, provide strong evidence for data fabrication. The data were published online by Schön in a preprint (which is common practice in physics) and concern the so-called breakdown field strength of a type of conductor that he said he made and used for his experiments several times during the previous years. The data were said to summarize the observed breakdown field strength of a little more than 600 of these conductors. The data can be found in the server (data/Schoen_breakdown_field). 14.1 Exercises Download the data and load it in an R data structure. What was the total number of observations, the average breakdown field strength and its standard deviation in Schön’s conductors? Show solution d &lt;- read.table(file.path(baseurl, &#39;data/Schoen_breakdown_field/schoen_observations.txt&#39;), header=TRUE, sep=&quot;\\t&quot;) totalobs &lt;- sum(d$observations) d$freq &lt;- d$observations/totalobs meanbd &lt;- sum(d$bin*d$freq) sdbd &lt;- sqrt(sum(d$freq*(d$bin-meanbd)^2)) totalobs ## [1] 613 meanbd ## [1] 23.76346 sdbd ## [1] 5.796255 Draw the original data together with the graph of a normal distribution having the calculated mean and standard deviations in one figure. Use the function dnorm() to calculate the graph of the desired normal distribution. Show solution plot(observations~bin, data=d, xlab=&#39;Breakdown field (MV/cm)&#39;, ylab=&#39;Number of observations&#39;, type=&#39;h&#39;, lwd=3, main=&#39;Data by Sch\\u00F6n with fitted normal distribution&#39;) lines(1:50, dnorm(1:50, mean=meanbd, sd=sdbd)*totalobs, col=&#39;red&#39;) So far so good, and the data appear to be normal distributed. The main reason to distrust the data was the fact that the data fitted a normal distribution too well. You would not expect such a nice fit for such a low number of data. To illustrate this, draw the same number of random observations as in Schön’s data set from a normal distribution, and plot the binned results in a figure similar to that of the original data. Use the functions rnorm(), trunc(), and table(). What do you notice, if you compare this random draw to the original data? Do it a few times with different random draws. Show solution draws &lt;- table(trunc(rnorm(totalobs,mean=meanbd,sd=sdbd))) draws &lt;- data.frame(bin=as.numeric(names(draws)), observations=as.numeric(draws)) plot(observations~bin, draws, xlab=&#39;Breakdown field (MV/cm)&#39;, ylab=&#39;Number of observations&#39;, type=&#39;h&#39;, lwd=3, main=&#39;Random data from a normal distribution&#39;) lines(1:50, dnorm(1:50, mean=meanbd, sd=sdbd)*totalobs, col=&#39;red&#39;) We want to compare the counts in the bins of the histogram published by Schön to the counts that we expect when the underlying distribution is a normal distribution. Use Pearson’s \\(\\chi^{2}\\) goodness of fit test to calculate the probability that the \\(\\chi^{2}\\) statistic has a value smaller or equal to the one calculated for Schön’s data. What does the p-value of this test tell us, and how do we calculate the desired probability from this p-value? Use the chisq.test() function. In the Chi-square test we compare observed with expected counts. The observed raw counts are are entered in this function as parameter \\(x\\), and the expected values as parameter \\(p\\), a vector of expected frequencies rather than counts. The chisq.test() function calculates expected counts by multiplying the frequencies by the sum of counts in \\(x\\). Show solution chtest &lt;- chisq.test(x=d$observations, p=dnorm(d$bin, mean=meanbd, sd=sdbd), rescale.p=TRUE) ## Warning in chisq.test(x = d$observations, p = dnorm(d$bin, mean = meanbd, : ## Chi-squared approximation may be incorrect chtest ## ## Chi-squared test for given probabilities ## ## data: d$observations ## X-squared = 19.045, df = 39, p-value = 0.997 The chance that the \\(\\chi^2\\)-value is smaller or equal to one observed is 1-0.9970=0.0030, which tells us that only once in 334 cases would such an observation be made if the data were indeed normal-distributed. It is an unlikely, but certainly not impossible observation. However, in the light of the other evidence of forgery, this case is probably one too. If Schön fabricated this data, what is a likely way in which he did this? Show solution My guess is that he calculated a normal distribution with certain parameters, then calculated how many observations should be in each bin and then manually adjusted by plus or minus a few. By that, he clearly underestimated the fluctuations in real data. "],
["amt-carrier.html", "CHAPTER 15 A critical evaluation of ammonium transporter kinetics 15.1 Exercises", " CHAPTER 15 A critical evaluation of ammonium transporter kinetics (Techniques: nonlinear fitting) The water soluble gas ammonia (NH\\(_{3}\\)) is thought to diffuse easily through lipid membranes. Hence, at high ammonia concentrations this passive diffusion is more than sufficient to provide the cell with the nitrogen that it uses for protein and nucleotide biosynthesis (Boogerd et al. 2011). However, at low ammonia concentrations active, energy-driven, transport of ammonia or ammonium is necessary. Khademi et al. (2004) crystallized and characterized such a carrier from E. coli, called AmtB. According to their hypothesis, the carrier facilitates the passive diffusion of ammonia. They performed experiments with purified carrier reconstituted in lipid vesicles made from purified lipids. According to Khademi et al. (2004) the carrier led to a tenfold increase in the diffusion rate of ammonia, from 12.8 +/- 0.7 s-1 in vesicles without carrier to 115.6 +/- 13.2 s-1 in vesicles with carrier. The authors stated that these measurements were performed in six-fold. The measurements were performed with a fluorescent pH indicator present in the vesicles. Due to the influx of ammonia and the subsequent equilibration of ammonia with the ammonium species (\\(pK_{A}\\approx9.0\\)) the proton concentration inside the vesicles decreases, and this gives an increase in the fluorescence. Their figure 5A displayed the data from a “typical experiment”. Figure 15.1: A copy of Figure 5A from Khademi et al. (2004) We would like to check their assertion. Hence, I reconstructed the fluorescence data from their figure with the freely available tool ScanIt9 and put those on the server (data/Amt_carrier_kinetics/Khademi2004_Fig5A.txt). The file contains data in tab-delimited form including a header. The factor column called “expt” indicates whether the data were from the control experiment concerning vesicles without carrier, or from the experiment using vesicles with carrier. 15.1 Exercises Load and plot the data in R. Use different symbols for the different experiments. Check the reference card to choose a nice set of symbols. Show solution Loading and plotting the data f &lt;- file.path(baseurl, &#39;data/Amt_carrier_kinetics/Khademi2004_Fig5A.txt&#39;) d &lt;- read.table(f, sep=&quot;\\t&quot;, head=TRUE) plot(fluorescence~time, data=d, pch=c(2,3)[d$expt]) Although it was not explicitly stated in the paper, the most likely model to which the data were fitted is an inverted exponential decay curve. These are of the form \\[ F(t)=F_{max}(1-e^{-kt}) \\] where \\(F(t)\\) is the fluorescence at time \\(t\\), \\(F_{max}\\) is the final maximal fluorescence, and \\(k\\) is the first order decay constant. Notice that \\(\\left(\\frac{dF(t)}{dt}\\right)_{t=0}=kF_{max}\\), and hence the initial slopes, are proportional to \\(k\\). For the control experiment and the carrier experiment make fits of the data to this inverted exponential decay models. Use the non-linear least squares function nls() from the standard stats package (which is already loaded at start-up) for this. First read its help page. Why is this called non-linear fitting? You will have to give initial estimates of the parameters (\\(F_{max}\\) and \\(k\\)) to get convergence of the nls fitting algorithm. These have to be entered to the “start” argument as a list object containing the exact names of the parameters as in the formula. So, for example, start=list(Fmax=1, k=10). You can estimate start values for the parameters from the graphs. Also, do not forget to select the proper rows from the data frame corresponding to either the control or the carrier vesicles. You can do that in the conventional way by using the [ selector operator, or by using the subset argument of the nls() function. Print the nls objects to the screen and interpret the output. Also try a few different starting values for the parameters. Does the outcome of the fit differ, or the number of iterations used? Show solution We make a non-linear fit of the exponential decay equation using the function nls(). The fitting is no-linear because the parameter \\(k\\) to be fitted appears in the equation in a non-linear fashion. Displaying the fits yields: controlfit &lt;- nls(fluorescence~Fmax*(1-exp(-k*time)), data=d, start=list(Fmax=1, k=10), subset=d$expt==&#39;control&#39;) carrierfit &lt;- nls(fluorescence~Fmax*(1-exp(-k*time)), data=d, start=list(Fmax=1, k=20), subset=d$expt==&#39;carrier&#39;) controlfit ## Nonlinear regression model ## model: fluorescence ~ Fmax * (1 - exp(-k * time)) ## data: d ## Fmax k ## 0.9552 11.1905 ## residual sum-of-squares: 0.004535 ## ## Number of iterations to convergence: 4 ## Achieved convergence tolerance: 8.714e-08 carrierfit ## Nonlinear regression model ## model: fluorescence ~ Fmax * (1 - exp(-k * time)) ## data: d ## Fmax k ## 0.9413 28.1820 ## residual sum-of-squares: 0.08918 ## ## Number of iterations to convergence: 7 ## Achieved convergence tolerance: 9.992e-06 The number of iterations until convergence may differ when using different starting conditions. Plot the lines of the fits in the figure with the data. Use the predict() method for nls class objects. Show solution Plotting the fitted curves with the data: plot(fluorescence~time, data=d, pch=c(2,3)[d$expt]) lines(d$time[d$expt==&#39;control&#39;], predict(controlfit), col=&#39;red&#39;) lines(d$time[d$expt==&#39;carrier&#39;], predict(carrierfit), col=&#39;red&#39;) Do the fitted first order decay constants agree with those reported by the authors? Show solution For the control vesicles the fitted rate constant \\(k\\) roughly agrees with that of the paper, but for the carrier vesicles the fitted \\(k\\) is much lower than the reported \\(k\\). Compare your curve for the carrier vesicles with the one from the paper. Do you notice a difference? Make a residual plot (use the residuals() method for nls objects) against time for your fit of the control and carrier vesicles. Do you notice a trend in any of them? If so, what does this indicate? Show solution The main difference seems to appear at the beginning of the curve, where data points are initially located above the fit and then mainly below the fit. A residual plot also shows this: layout(matrix(1:2,nrow=1)) plot(d$time[d$expt==&#39;control&#39;], residuals(controlfit), xlab=&#39;time&#39;, ylab=&#39;residuals&#39;, main=&#39;Residuals control fit&#39;) plot(d$time[d$expt==&#39;carrier&#39;], residuals(carrierfit), xlab=&#39;time&#39;, ylab=&#39;residuals&#39;, main=&#39;Residuals carrier fit&#39;) Your first order constant for the carrier vesicles probably doesn’t agree with that of the authors. Furthermore, it appears as if the authors used a different fitting function for their carrier vesicles experiment. Thinking about it: the ammonia diffusion process in these vesicles probably consists of a mixture of two processes, namely a passive and a carrier-mediated diffusion. Both are likely to be first order processes, specifically also the carrier-mediated process, since the \\(K_{M}\\) of the carrier, estimated in other experiments, is much higher than the concentration used in this experiment. Formulate a mathematical model, just as the one above, that describes the combined behavior of both processes. Show solution A bi-exponential decay model could be used. It consists of the sum of two exponential decay models with possibly different \\(F_{max}\\)’s and certainly different values for the decay constants \\(k\\): \\[ F(t)=F_{max1}(1-e^{-k_{1}t})+F_{max2}(1-e^{-k_{2}t}) \\] Of course, \\(F_{max1}+F_{max2}\\) has to add up to the total fluorescence present at the end of the experiment. Fit your model to the carrier vesicle data. In general: the more parameters you are fitting, the better initial estimates you will have to give, so do that carefully. Show solution We try to provide good initial estimates of the parameters. For the slow process we use the \\(k\\)-value estimated earlier. For the fast process we start with an arbitrary higher value: carrierfit2 &lt;- nls(fluorescence ~ Fmax1*(1-exp(-k1*time)) + Fmax2*(1-exp(-k2*time)), data=d, ,subset=d$expt==&#39;carrier&#39;, start=list(Fmax1=0.5, Fmax2=0.5, k1=11, k2=40)) carrierfit2 ## Nonlinear regression model ## model: fluorescence ~ Fmax1 * (1 - exp(-k1 * time)) + Fmax2 * (1 - exp(-k2 * time)) ## data: d ## Fmax1 Fmax2 k1 k2 ## 0.4676 0.4964 13.6617 64.6515 ## residual sum-of-squares: 0.03784 ## ## Number of iterations to convergence: 7 ## Achieved convergence tolerance: 3.275e-06 Do your parameter estimates agree with those of the authors now? Provide some hypotheses for discrepancies between your result and those of the authors. Show solution The fast process still has an approximately two-fold lower rate than that reported in the paper. Perhaps the bi-exponential model still does not fit the data well. However, plotting the fit and the residuals shows that it is a good model: layout(matrix(1:2,nrow=1)) plot(fluorescence~time, data=d, pch=2, subset=d$expt==&#39;carrier&#39;, main=&#39;Bi-exponential fit&#39;) lines(d$time[d$expt==&#39;carrier&#39;], predict(carrierfit2), col=&#39;red&#39;) plot(d$time[d$expt==&#39;carrier&#39;], residuals(carrierfit2), xlab=&#39;time&#39;, ylab=&#39;residuals&#39;, main=&#39;Residuals&#39;) Perhaps then, the estimation of this fast process is very sensitive to the data (i.e., the error in the estimate is large). Printing the summary of the fit, however, shows no exceptionally high estimated standard deviation for \\(k2\\): summary(carrierfit2) ## ## Formula: fluorescence ~ Fmax1 * (1 - exp(-k1 * time)) + Fmax2 * (1 - exp(-k2 * ## time)) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## Fmax1 0.46761 0.06599 7.086 7.22e-10 *** ## Fmax2 0.49639 0.06882 7.212 4.20e-10 *** ## k1 13.66167 1.69721 8.049 1.14e-11 *** ## k2 64.65152 10.34699 6.248 2.49e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.02277 on 73 degrees of freedom ## ## Number of iterations to convergence: 7 ## Achieved convergence tolerance: 3.275e-06 One hypothesis to explain this controversy is that the experiment from Figure 5A was not so “typical” but rather an outlier. Considering the reported standard deviation of the first order rate constant of carrier mediated diffusion, is it likely that the experiment from figure 5 was an outlier? Show solution No this is very unlikely, given the fact that the reported standard deviation equals only 10% of the estimated rate constant. This demonstrates how bad it is not to provide exact descriptions of experimental as well as data analysis procedures. It is actually visible from the figure that the speed up of transport is less than 10-fold, and the reviewers of the paper have not paid enough attention. References "],
["bias-metabolomics.html", "CHAPTER 16 Bias in metabolomics data 16.1 Exercises", " CHAPTER 16 Bias in metabolomics data (Techniques: data de-trending, generalized additive modelling) The current case is one demonstrating the importance of measuring and saving as many variables as possible, even the ones that you think will not matter. In an experiment the production of flavour compounds by 10 strains of Lactococcus lactis strains was measured in 20 experimental microcheeses for each strain. The flavour compounds were measured with a mass-spectrometer coupled to a gas-chromatograph. The researcher was particularly interested in differences in the production of propanol, since there were earlier indications that this characteristic should differ significantly between strains. The data are present in the tab-delimited file data/metabolomics/propanol_production.txt. The file contains a header, a column called “strain” with sample names, a column stating whether the sample was a calibration point or of an unknown concentration, identified as “calibration” or “sample”, and a column with the raw propanol signals from the GC-MS system. The calibration samples contained either 0, 2, 5 or 10 \\(\\mu M\\) propanol, indicated by the sample name. The order of the samples was block-randomized, i.e. each block of 14 samples (10 cheeses and 4 calibration samples) was randomized. The researcher comes to you with the following plots: d &lt;- read.table(file.path(baseurl, &#39;data&#39;, &#39;metabolomics&#39;, &#39;propanol_production.txt&#39;), header=TRUE, sep=&quot;\\t&quot;) She was disappointed to see that the differences between the strains were hardly, if at all, measurable. Furthermore, the calibration curves looked awful, containing extremely high variance. So, she comes to you and asks what could be wrong with the measurements. Your task is to explore the data, try to correct its main defect and present a more encouraging result to her, with more accurate concentration data. 16.1 Exercises Start by loading the data in a data frame, study its structure, and reproduce exactly the plots above. Use the formula syntax in the boxplot() and plot() commands. Show solution Loading the data: d &lt;- read.table(file.path(baseurl, &#39;data&#39;, &#39;metabolomics&#39;, &#39;propanol_production.txt&#39;), header=TRUE, sep=&quot;\\t&quot;) and displaying the first few lines head(d) ## samplenr strain sampletype rawpropanol ## 1 1 strainG sample 123.5 ## 2 2 cal5 calibration 99.6 ## 3 3 strainB sample 155.1 ## 4 4 strainF sample 149.1 ## 5 5 strainE sample 99.6 ## 6 6 cal10 calibration 205.8 shows that we need to provide a new column that contains the concentration of propanol in the calibration samples. And let’s make it equal to NA in the cheese samples. d$concentration &lt;- ifelse(d$sampletype==&#39;calibration&#39;,0,NA) d$concentration[d$strain==&#39;cal2&#39;] &lt;- 2 d$concentration[d$strain==&#39;cal5&#39;] &lt;- 5 d$concentration[d$strain==&#39;cal10&#39;] &lt;- 10 To reproduce the plots it is most convenient to split the data in two data frames, one with the calibration data and another with the cheese data: cheese &lt;- d[d$sampletype==&#39;sample&#39;,] calibrations &lt;- d[d$sampletype==&#39;calibration&#39;,] If we would now make the boxplot, we would get a plot that contains also the calibration sample levels (try it). Although these are not present any more in the cheese data frame, a sub-selection of a factor type vector leaves all the original levels in that derived factor vector. To remove the unused levels, we use the droplevels() function. We are then able to reproduce the plots above. cheese$strain &lt;- droplevels(cheese$strain) layout(matrix(1:2,ncol=2)) plot(rawpropanol~concentration, data=calibrations, pch=19, xlab=&#39;Propanol [microM]&#39;,ylab=&#39;Raw propanol signal&#39;) boxplot(rawpropanol~strain, data=cheese, col=&#39;grey&#39;, las=2, ylab=&#39;Raw propanol signal&#39;) Explore the data using the plot() method of the data frame class. Do you notice the unexpected correlation? Show solution We are now ready to explore the data. If the data frame does not contain too many columns, a pairplot can be produced to detect correlations by just using the plot() method of the data frame class. Doing that (yourself!), you will see that there is an unexpected relation between the sample number and the raw propanol measurement. Such a correlation is not expected, because the cheese and calibration samples are measured in blocked random order. Apparently, there is a large instrument-trend present in the data, and the raw measurement seems to increase with time. A cause could be that the GC-MS apparatus was not properly equilibrated. Whatever the cause, we would like to know whether we can still extract the desired information (differences in propanol concentrations between strains) out of the data, even if it were only to encourage the researcher that putting more effort in solving the technical problems with the GC-MS will pay off. Re-create a single plot with the unexpected correlation. Give each strain/concentration a different symbol or color. What do you notice? What does this mean for the possibility of filtering out the trend? Show solution As a first attempt, we plot the raw propanol data as a function of the sample number, but using different symbols and colors for each of the strains. We use the original data frame including the calibration data. plot(rawpropanol~samplenr, data=d, col=d$strain, pch=c(17,18,19)[(as.numeric(d$strain)%%3) + 1]) This looks encouraging, because we see that locally, i.e. when comparing samples close to each other, the differences between the sample types seems to be more significant than globally. So, perhaps we could correct the defect by fitting a trend line through the data, and be subtracting this trend line from each of the points. Use the loess() function to create a trend line through the data. Test a few different values for the span parameter of this function. Plot these trend lines in the previous graph (without different symbols for different data types). Show solution A general trend line, whose precise shape we do not know, can be conveniently estimated using the loess() function in R. Take a look at the help file of this function to understand what it does. The loess() function contains a parameter “span” that affects the smoothness of the fit. So, let us make a few loess fits with different values for the span parameter, and plot these in the previous figure: plot(rawpropanol~samplenr, data=d, cex=0.6) trend1 &lt;- loess(rawpropanol~samplenr, data=d, span=0.35) trend2 &lt;- loess(rawpropanol~samplenr, data=d, span=0.75) trend3 &lt;- loess(rawpropanol~samplenr, data=d, span=1.5) lines(predict(trend3), col=&#39;blue&#39;, lwd=2) lines(predict(trend2), col=&#39;green&#39;, lwd=2) lines(predict(trend1), col=&#39;red&#39;, lwd=2) legend(&#39;bottomright&#39;,c(&#39;span=0.35&#39;,&#39;span=0.75&#39;,&#39;span=1.5&#39;), lwd=2, col=c(&#39;red&#39;,&#39;green&#39;,&#39;blue&#39;)) Use the trend line to correct the original data. Re-create the calibration and boxplot figures with the corrected data. Does it look better than the original data? Is there evidence for a residual trend in the corrected data? Show solution Let’s choose the smoothest trend line to subtract the trend, put the result in a column “corrawprop”, split the data in two data frames again and then reproduce the boxplot and calibration plot using the corrected data: d$corrawprop &lt;- d$rawpropanol - predict(trend3) cheese &lt;- d[d$sampletype==&#39;sample&#39;,] cheese$strain &lt;- droplevels(cheese$strain) calibrations &lt;- d[d$sampletype==&#39;calibration&#39;,] layout(matrix(1:2,ncol=2)) plot(corrawprop~concentration,data=calibrations,pch=19, xlab=&#39;Propanol [microM]&#39;,ylab=&#39;Raw propanol signal&#39;) boxplot(corrawprop~strain, data=cheese, col=&#39;grey&#39;, las=2, ylab=&#39;Raw propanol signal&#39;) A large part of the variance in the data due to the global trend has been removed. We should check whether we can still detect a residual trend in the corrected data: plot(corrawprop~samplenr,data=d) That doesn’t seem to be the case. Using a linear fit through the calibration data, calculate the propanol concentrations in the cheeses. Make a plot of the calibration data with the fitted curve, and make boxplots of the propanol concentrations in the different cheeses. Show solution To obtain propanol concentrations instead of trend-corrected raw data, we make a linear fit of the calibration data using the lm() function, and use this fit together with its predict() method to calculate the unknown sample concentrations (look up the help page of predict.lm()). calibfit &lt;- lm(concentration~corrawprop, data=calibrations) cheese$fittedconc &lt;- predict(calibfit, newdata=cheese) layout(matrix(1:2,ncol=2)) plot(corrawprop~concentration,data=calibrations,pch=19, xlab=&#39;Propanol [microM]&#39;,ylab=&#39;Raw propanol signal&#39;, main=&#39;Calibration&#39;) lines(predict(calibfit),calibrations$corrawprop, col=&#39;red&#39;, lwd=2) boxplot(fittedconc~strain, data=cheese, col=&#39;grey&#39;, las=2, ylab=&#39;Propanol [microM]&#39;,main=&#39;Deduced concentrations&#39;) I leave it up to you to produce a table with means and standard deviations for the propanol concentrations produced per strain. Why was the experimental design chosen by the scientist a good design? Which design would not have allowed us to correct the trend and calculate the propanol concentrations? Show solution With the current insight we can say that the researcher was very lucky to choose the block-randomized experimental design. Suppose she had measured all calibration samples first and then each strain 20 times in a row. Would we have been able to correct for the bias caused by the trend? This is a good lesson: randomized or designs with repeated blocks designs are always better than extremely regular designs. The latter designs always bear the danger of mixing in effects from unknown factors, like in this case a time dependent trend, which can during data analysis not be separated from the information that we are interested in. 16.1.1 An alternative solution: Generalized additive modeling (optional) There is a modern branch of statistical modeling called generalized additive modeling (GAM, see Wood (2006)) that is able to model data as sums of smooth functions of co-variates (continuous, or at least ordered explanatory variables) and of factor level coefficients. In principle, the problem above is of that type. The raw propanol readings are the sum of the trend, which we assume is a smooth function of the sample number, and of the sample class. GAM modeling has been implemented in R in the package “mgcv”, which is part of a standard R installation. The advantage of using such an approach relative to the 2-stage approach that we used above, being first estimation of the trend and correction of the data, and subsequent estimation of the sample averages is that the data are fitted in one operation. In principle this leads to more accurate results. This is especially true if the number of measurements is much smaller than we had above. If you feel inspired then try to solve the problem using this method. Read the details of the gam fitting function in its extensive help document. Show solution The problem is solved using GAM. The key is to realize that the gam functions “know” a new kind of term in the modeling formula, namely s(v), which stands for some smooth function of the variable v. Here we use s(samplenr) to model an arbitrary but smooth function of the variable samplenr. This function will represent the instrument-trend in the data. The other term in the model formula will be a constant that depends on the “strain” factor. This factor includes the calibration samples, and should perhaps actually have been called “sample.class”. # library(mgcv) gamfit &lt;- gam(rawpropanol~s(samplenr)+strain, data=d) This gives us a “gam” object containing, among other information, the fit. The details of the plot() method of the gam class can be found under “plot.gam()”. If we apply it we get the following graph plot(gamfit,rug=FALSE,residuals=TRUE,pch=1,cex=0.5) The smooth line shows the fitted trend, and an estimated region of \\(\\pm\\) one standard error of this fitted trend. The points show the residuals excluding sample class (strain) effects (so the true residuals). The trend looks quite similar to the loess fit above. We are interested in the trend-filtered data because we still have to calculate propanol concentrations using the calibration data. We can reconstruct these by setting the type='terms' argument of the predict() method for gam objects. This will yield a matrix showing per column the contribution of each of the predictive variables to the final predicted value. Since this would give each sample from a sample class the same value we still have to add the residuals to reconstruct the trend-filtered values. Let’s call these values “gamrawprop”, and let’s make the same two graphs again d$gamrawprop &lt;- predict(gamfit, type=&#39;terms&#39;)[,&#39;strain&#39;]+residuals(gamfit) cheese &lt;- d[d$sampletype==&#39;sample&#39;,] cheese$strain &lt;- droplevels(cheese$strain) calibrations &lt;- d[d$sampletype==&#39;calibration&#39;,] layout(matrix(1:2,ncol=2)) plot(gamrawprop~concentration,data=calibrations,pch=19, xlab=&#39;Propanol [microM]&#39;,ylab=&#39;GAM corrected propanol signal&#39;) boxplot(gamrawprop~strain, data=cheese, col=&#39;grey&#39;, las=2, ylab=&#39;GAM corrected propanol signal&#39;) Except for a difference in offset, this looks quite similar to what we obtained using our two-stage method. References "],
["nerve-fiber.html", "CHAPTER 17 Correlation between fiber density and episodic memory? 17.1 Exercises", " CHAPTER 17 Correlation between fiber density and episodic memory? (Techniques: correlation, outliers, influential points) In a paper in PNAS a group of scientists claimed that they found a significant relation between the nerve fiber density in certain parts of the brain and episodic memory recollection (Schott et al. 2011). The paper was immediately criticized by Rousselet and Pernet (2011) for the lack of robust support for the hypothesis. The figure demonstrating the relation is reproduced below. Figure 17.1: A copy of Figure 3 from Schott et al. (2011) You may immediately notice a problem with these data, namely that the correlation doesn’t seem very strong, and that it seems to depend on just a few influential points, namely two or perhaps three out of 28 points at high fiber density. This could be a problem if the measurements corresponding to those points are special, in the sense that they were measured by somebody else, or if there was something else erratic with those measurements. We express such concerns by saying that these measurements could be outliers. You could also argue that the scientists were very lucky to have picked those two or three subjects that were responsible for the significant correlation. So, we will try to find out which points determine the significance of the correlation. The data are available in data/nervefiber. 17.1 Exercises Reproduce the numbers in the figure, i.e. the Pearson correlation coefficient \\(r\\) and the p-value for the ERC data. Also calculate the Spearman rank correlation coefficients. The latter is a more robust correlation coefficient, i.e. it does not depend on the assumption of a linear relation and homoscedastic normal distribution of the errors. Show solution Loading the data and using the cor.test() function from R f &lt;- file.path(baseurl, &quot;data/nervefiber/nervefiber.tab&quot;) d &lt;- read.table(f, sep=&quot;\\t&quot;, header=TRUE) cor.test(d$remembered, d$PRC, method=&quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: d$remembered and d$PRC ## t = 4.744, df = 26, p-value = 6.605e-05 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.4130527 0.8406167 ## sample estimates: ## cor ## 0.6811622 cor.test(d$remembered, d$ERC, method=&quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: d$remembered and d$ERC ## t = 4.0527, df = 26, p-value = 0.0004074 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.3244537 0.8077784 ## sample estimates: ## cor ## 0.622214 cor.test(d$remembered, d$PRC, method=&quot;spearman&quot;) ## Warning in cor.test.default(d$remembered, d$PRC, method = &quot;spearman&quot;): ## Cannot compute exact p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: d$remembered and d$PRC ## S = 1770.2, p-value = 0.004988 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.5155416 cor.test(d$remembered, d$ERC, method=&quot;spearman&quot;) ## Warning in cor.test.default(d$remembered, d$ERC, method = &quot;spearman&quot;): ## Cannot compute exact p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: d$remembered and d$ERC ## S = 2544.3, p-value = 0.1162 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.3036987 Clearly, a Spearman correlation on the relation of memory performance and ERC density is not significant (at sinificance level 0.05). However, in case of the PRC density the correlation seems to hold even with a rank correlation measure. Reproduce the graphs in figure 3 of the paper. Include a linear fit for each of the graphs. Show solution We use lm() and predict.lm() to fit and plot lines: layout(matrix(1:2,nrow=1)) plot(remembered~PRC, data=d) lm.PRC &lt;- lm(remembered~PRC, data=d) lines(d$PRC,predict(lm.PRC), col=&#39;red&#39;) plot(remembered~ERC, data=d) lm.ERC &lt;- lm(remembered~ERC, data=d) lines(d$ERC,predict(lm.ERC), col=&#39;red&#39;) Plot the residuals as function of the fitted values for the PRC variable only. What do you notice? Show solution Most notable about the plot is the apparent decreasing variance with increasing predicted value, i.e. the error seems to be heteroscedastic (not drawn from a single distribution). The authors use a linear model (\\(\\text{remembered}_i = \\beta_0 + \\beta_1 \\, \\text{density}_i + \\epsilon_i\\)) to model each of the nerve density variables as a function of the fraction remembered. It is highly questionable whether the data fit to such a linear model, given the fact that most points seem to be located in a cloud below a fiber density of 100, within which there is no clear relation between the two variables. Check whether it is true that the relation is not noticeable below a fiber density of 100. Show solution subset.PRC &lt;- d[d$PRC&lt;100,] cor.test(subset.PRC$remembered, subset.PRC$PRC, method=&quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: subset.PRC$remembered and subset.PRC$PRC ## t = 1.026, df = 21, p-value = 0.3166 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.2128870 0.5785824 ## sample estimates: ## cor ## 0.2184881 subset.ERC &lt;- d[d$ERC&lt;100,] cor.test(subset.ERC$remembered, subset.ERC$ERC, method=&quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: subset.ERC$remembered and subset.ERC$ERC ## t = 0.66628, df = 22, p-value = 0.5122 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.2785609 0.5148282 ## sample estimates: ## cor ## 0.1406395 Clearly, the relation disappears, or becomes unnoticeable within the noise of the measurements. A question you could ask is how much influence each of the points has on the fitted line. The question could be made more precise by asking for their influence on the two parameters, \\(\\beta_0\\) and \\(\\beta_1\\). We are mostly interested in the influence on the slope parameter \\(\\beta_1\\) and in the influence that particular points have on significance of the slope (whether it differs from 0). The rationale behind the question is that you would (or should) want to know what the influence of a single measurement is on the conclusions from the whole study. A single, highly influential measurement deserves attention: was it measured correctly, and if so, what is the cause of its deviation from the other measurements. Since a point may have different effects on the fitted model, there are also different measures of influence. In R these can be calculated using the influence or influence.measures() functions from the standard stats package, to obtain a list or table of different measures for each of the points, or separately using the functions mentioned in the help file of the influence.measures() function. That function also yields a column in which highly influential points (by any of the measures) are marked with an asterisk. Here, we briefly discuss a few measures of influence (see for example Faraway (2005) for a more detailed discussion): Hat value: or leverage is a measure of the distance (Mahalanobis distance is used) of a point to the center of the data set. Points far from that center have a large leverage. They exert a much larger “force” on a fitted line to pass near them than points near the center (hence, the term leverage). Technically, the hat values of a data set are the diagonal elements from the so-called hat matrix \\(\\mathbf{H}\\) which is calculated from the values of the independent variables and the number of fitted parameters \\(p\\). The sum of hat values \\(h_i\\) equals the number of parameters: \\(\\Sigma h_i = p\\). So, on average, a hat value equals \\(p/n\\), where \\(n\\) is the number of measurements. A rule-of-thumb criterion to decide whether a hat value \\(h\\) is unusually large is \\[ h \\geq \\text{min} \\left[ \\frac{2p}{n}, 0.99 \\right] \\] Rstudent residual: (or jackknifed residual) If you remove a point from the data set, a model can be fitted through the remaining data. The deviation \\(\\hat{\\epsilon}_i\\) of the left-out point from the predicted value using that model, normalized by residual mean squares is a measure of its deviation from the model of the remaining data. Hence you can use this measure to detect an outlier in a data set (although it will often fail when there are multiple outliers, see Faraway (2005)). The Rstudent residual \\(t_i\\) of point \\(i\\) equals \\[ t_i = \\frac{\\hat{\\epsilon}_i}{\\sqrt{\\text{residual MS}_{(i)}} \\sqrt{1 - h_i}} \\] This residual is approximately student T-distributed with \\(n-p-1\\) degrees of freedom (if the model is correct). So, we can use this to test for significant deviation. dfbeta, dffit: These measures indicate how the parameters (\\(\\beta_0\\), \\(\\beta_1\\), etc.) (the dfbeta’s, one for each parameter) of the fit to the dependent variable \\(Y_i\\) will differ when a point \\(i\\) is left out of the data set. Rule-of-thumb cut-off criteria are \\(|\\text{dffit}| \\geq 3 \\sqrt{\\frac{p}{n-p}}\\) and \\(|\\text{dfbeta}| \\geq 1\\). Cook’s distance: Summarizes the effect on the model parameters in a single number per point. \\(D_i \\geq 1\\) and \\(D_i \\geq \\frac{4}{n}\\) are used as criteria to decide which points have unusual Cook’s distances . Consider using the augment() function from the broom package. In addition to the fitted values and residuals it yields a data frame with several influence metrics per data point. Calculate the hat values or leverages (on the PRC model) and determine whether they are unusually large for some points. Confirm that the sum of leverages equals the number of parameters of the model. Show solution h &lt;- hatvalues(lm.PRC) plot(h, pch=as.numeric(h&gt;(2*2/length(h))) + 1) # identify(h) sum(h) ## [1] 2 Points 21, 27 and 28 have a suspiciously high hat value Calculate the studentized residuals (on the model for PRC) using the rstudent() function. Determine whether they deviate significantly from the expected value (apply a Bonferroni correction for multiple hypothesis testing). Show solution rst &lt;- rstudent(lm.PRC) # calculate a cut-off value for t (two-sided), using bonferroni correction cutoff_t &lt;- qt(0.025/length(rst), df=length(rst)-2-1) # two-sided test, cutoff_t is a negative number! sign.bonf &lt;- -abs(rst) &lt; cutoff_t any(sign.bonf) ## [1] FALSE # no apparent large deviations according to this criterion Calculate Cook’s distances and indicate which points have a high influence based on this parameter. Show solution plot(cooks.distance(lm.PRC), pch=as.numeric(cooks.distance(lm.PRC) &gt; 4/length(d$PRC))+1) The conclusion from all of the tests above are that: There are a few influential points in the data set, but leaving out each individually is unlikely to change the conclusion from the study The distribution of fiber densities, with a few very high densisties and many low densisties, is such that this fact on its own deserves attention: why do some people have exceptionally high fiber density? What else is special about these people? The relation between fiber density and memory performance is only visible when people with unusually high fiber density are included The take home-message is that statistical analysis enhances our understanding of the data, but it should not replace it. There is much more to say about these data than the plain conclusion that “there is a significant correlation”. Below, we investigate to what extent the conclusion “significant effect” depends on the presence of particular points in the data set. As we saw, it completely disappears when only taking data with densities lower than 100. For each of the points, calculate the influence that it has on the p-value of the cor.test() by calculating the \\(\\log_{10}(ratio)\\) of the p-value of the data set without over that with the point (this is a positive number if the point decreases the p-value, or increases the significance of the correlation). Do this for both ERC and PRC and call the resulting values influence.PRC and influence.ERC, and add them to the data frame. The output of cor.test() is a subclass object of the list class, and the p-value can be extracted by its name p.value from such an object. Show solution influence.ERC &lt;- c() influence.PRC &lt;- c() # Reference p-values pv.ERC &lt;- cor.test(d$remembered, d$ERC, method=&quot;pearson&quot;)$p.value pv.PRC &lt;- cor.test(d$remembered, d$PRC, method=&quot;pearson&quot;)$p.value # In each iteration leave one point out and calculate its influence, d$influence.PRC = sapply(1:length(d$remembered), function(i) { log10(cor.test(d$remembered[-i], d$ERC[-i], method = &quot;pearson&quot;)$p.value/pv.ERC) }) d$influence.ERC = sapply(1:length(d$remembered), function(i) { log10(cor.test(d$remembered[-i], d$PRC[-i], method = &quot;pearson&quot;)$p.value/pv.PRC) }) Using the ggplot2 package we can also make a nice graph of the influence: # library(ggplot2) ggplot(d, aes(x=ERC, y=remembered, color=influence.ERC)) + geom_point(size=6, colour=&#39;grey20&#39;) + geom_point(size=4) + scale_colour_gradient2(low=&#39;blue&#39;,high=&quot;red&quot;) ggplot(d, aes(x=PRC, y=remembered, color=influence.PRC)) + geom_point(size=6, colour=&#39;grey20&#39;) + geom_point(size=4) + scale_colour_gradient2(low=&#39;blue&#39;,high=&quot;red&quot;) If you study the influence results, you will see that particularly the two points with the largest nerve fiber counts have a large influence on the p-value, amounting to factors of \\(10^{1.65}=45.2\\) and \\(10^{0.73}=5.4\\), respectively. Remove the one, two and three most positive influential points and re-calculate the p-values. Put them in a table and display them. On how many subjects does the significance of the correlation depend? Show solution First we calculate the order of influence, which returns an index vector, and then we can remove the 1, 2 and 3 most influential points by removing the points with the first 1 \\(\\cdots\\) 3 indices. # creating an empty table inf.table &lt;- data.frame(points.removed=0:3, ERC.pvalue=rep(NA,4), PRC.pvalue=rep(NA,4)) PRC.order &lt;- order(d$influence.PRC, decreasing=TRUE) ERC.order &lt;- order(d$influence.ERC, decreasing=TRUE) inf.table[1,&#39;PRC.pvalue&#39;] &lt;- cor.test(d$remembered, d$PRC, method=&quot;pearson&quot;)$p.value inf.table[1,&#39;ERC.pvalue&#39;] &lt;- cor.test(d$remembered, d$ERC, method=&quot;pearson&quot;)$p.value for (i in 1:3) { rd &lt;- d[PRC.order[-(1:i)],] inf.table[i+1,&#39;PRC.pvalue&#39;] &lt;- cor.test(rd$remembered, rd$PRC, method=&#39;pearson&#39;)$p.value rd &lt;- d[ERC.order[-(1:i)],] inf.table[i+1,&#39;ERC.pvalue&#39;] &lt;- cor.test(rd$remembered, rd$ERC, method=&#39;pearson&#39;)$p.value } inf.table ## points.removed ERC.pvalue PRC.pvalue ## 1 0 0.0004073759 0.000066054 ## 2 1 0.0183984872 0.002840849 ## 3 2 0.1772134107 0.029162011 ## 4 3 0.2581785086 0.036217447 It shows that significance depends on 2 or 3 subjects for ERC and PRC, respectively. Again, it shows that the conclusion from the study depends on a tiny subset at high fiber density. That requires additional research and explanation, which the authors do not supply in their paper. Perhaps, that is the most fundamental criticism of this study. The authors hide behind a statistical criterion, as often happens in the medical and psychological literature (see their response to Rousselet and Pernet (2011)). Notice how we (as well as the authors) completely ignore the fact that the measurement of the independent variable, PRC or ERC, is likely to contain an error too, and a large one probably, compared to the full range of measurements of fiber densities. The assumption behind the linear model is that the error in the independent variable is very small compared to the range. A large error in fiber density measurement might be a cause for the lack of observed correlation between fiber density and memory performance when only using low densities. A very large error in the independent variables will cause the slope to be biased towards 0. Chapter 5 in Faraway (2005) gives a discussion of this effect as well as a possible solution to, nevertheless, obtain a good (unbiased) estimate of the slope of a linear regression when the error in the independent variables is of considerable size. References "],
["classifiers.html", "CHAPTER 18 Logistic regression 18.1 Classifiers 18.2 Logistic regression", " CHAPTER 18 Logistic regression 18.1 Classifiers A classifier is a statistical model, based on a function with a specific type of output. The output of a classifier function is a class label or a distribution over class labels. Many classifiers are capable of procuding both types of output. Classifiers are used when the dependent or reponse variable is a class label, or in R-terms, when the reponse variable is a factor. A class label could be “Male”, or “Malignant” or “Iris versicolor”. Corresponding distributions over class labels would be: Male: 0.9, Female: 0.1 Benign: 0.05, Malignant: 0.95 Iris setosa: 0.01, Iris virginica: 0.24, Iris versicolor: 0.75 An alternative to the latter would be Iris setosa: 0.01, Iris virginica or Iris versicolor: 0.99 Distributions over class labels can be interpreted as probabilities. The first example would tell us that the probability that the person is a man is 90%, and that it is a female is 10%. The requirement for these distributions to be proper probabilities is that the numbers add up to \\(1\\). Logistic regression is an important type of classifier, usually applied when there are two classes, that yields a distribution over the alternative labels. Another type of classifier, the naive Bayes classifier, is discussed in chapter 20. 18.2 Logistic regression Logistic regression functions are functions that yield a distribution over class labels as an output. Logistic regression is typically used in classifications with two alternatives, i.e. when the reponse variable has two levels. We start with the example that we saw before (chapter 12), the bodymetrics data set, data/linearmodels/bodymetrics.tab. It contains measurements of height, weight and age of approximately 500 males and females. Suppose that instead of wanting to predict the height from the gender, we want to do it the other way around: predicting gender from height. Let’s make a histogram of the height differentiated by gender to see hwat the possibilities are: Figure 18.1: Histogram of body heights, using a bin width of \\(5\\) cm In this histogram I also added a smoothed version of the height distributions for both sexes. These are normal distributions (see chapter 11) with different means, but having equal variance10 (and hence, equal standard deviation). Using either the counts in the bins or the heights of the smoothed distributions we can calculate the probability that a person with a certain body height is a man or a woman. Let’s call the probability that a person with body height \\(x\\) is a male \\(p(x)\\). Then the probability that this person is a female equals \\(1 - p(x)\\). In the figure below I have plotted the probability \\(p(x)\\) calculated by using the binned counts from the histogram as well as the smoothed distibutions. It is simply the height of the male bin divided by the sum of the male and female bin, or the value of the density function for males divided by the sum of density functions of males and females. Figure 18.2: The same histogram as above. The probability \\(p (x)\\) that the person is a man at the given height \\(x\\) is added, either calculated from the histogram bins (red dots) or from the smoothed distributions in figure above (blue line). Having these estimations of the probability \\(p(x)\\) It would be straight-forward to make a classifier based on the predictor height (\\(x\\)): if the probability \\(p(x) &gt; 0.5\\) then the label is ‘male’, otherwise, when \\(p(x) \\leq 0.5\\), the label is ‘female’. Linear discriminant analysis (LDA) works exactly in this way: it obtains a description of the probability \\(p(x)\\) from smoothed normal distributions of the individual classes. However, there is another approach of obtaining a function \\(p(x)\\), which is used by logistic regression. To see how it works, we need the concept of odds11. The odds is the ratio \\(\\displaystyle \\frac{p(x)}{1-p(x)}\\), or how much more likely the person is a male than a female. This ratio equals \\(1\\) when the person is equally likely to be a man or a woman. In figure 18.2 we can see that \\(p = 1 - p = 0.5\\) at a height of a little more than \\(170\\) cm. Rather than using the odds itself, it is common to use its natural logarithm also called the log-odds \\[\\begin{equation} \\ln{\\left(\\frac{p(x)}{1-p(x)}\\right)} \\tag{18.1} \\end{equation}\\] The function \\[\\begin{equation} l(p) = \\ln{\\left(\\frac{p}{1-p}\\right)} \\tag{18.2} \\end{equation}\\] is called the logit function. Logistic regression is based on the fact that something peculiar happens if we transform the probability \\(p(x)\\) with the logit function to \\(l(p(x))\\). This is shown below for the values of \\(p(x)\\) calculated from the smoothed distributions: Figure 18.3: Logit transform of probability \\(p(x)\\) as calculated from the smooth distributions plotted as a function of the body height \\(x\\). As you will guess, this perfect straight line is not a coincidence. This is what you get if you use two normal distributions with equal standard deviation but different means to model the underlying distributions. The slope of this line will be steeper when the two distributions have less overlap, and the point where it passes through the \\(x\\)-axis is the point of equal odds. Logistic regression uses this fact. If apparently we can fit the log-odds by a straight line \\(f(x) = \\beta_0 + \\beta_1 x\\): \\[ \\ln{\\left( \\frac{p(x)}{1-p(x)} \\right) } = f(x) \\] then let’s find out how \\(p(x)\\) itself can be modeled as a function of \\(x\\): \\[\\begin{align*} \\frac{p(x)}{1-p(x)} &amp;= e^{f(x)} \\\\ p(x) &amp;= e^{f(x)} - e^{f(x)}p(x) \\\\ p(x) \\left( 1 + e^{f(x)} \\right) &amp;= e^{f(x)} \\\\ p(x) &amp;= \\frac{e^{f(x)}}{1 + e^{f(x)}} = \\frac{1}{1 + e^{-f(x)}} \\tag{18.3} \\end{align*}\\] Equation (18.3) is called the logistic equation. This is the function that is used in logistic regression to fit \\(p(x)\\) to the data. 18.2.1 Logistic Regression: a case of Generalized Linear Modeling Logistic regression is a special case of a framework more general modeling techniques called Generalized Linear Modeling or GLM. In contrast to ordinary linear models, generalized linear models have two modifications: The response variable \\(y\\) does not have to be normal-distributed. A linear function of the predictor variables is transformed through a non-linear, so-called link function to fit the sampled response variable. The second modification needs some explanation. In statistical modeling we actually predict the parameters of a distribution of the response variable rather than the response variable itself. In linear modeling, since the underlying distribution is assumed to be the normal distribution, the predicted parameter \\(\\mu\\) is the expected value of the response variable itself. However for other distributions this is not necessarily the case. For example, in logistic regression, the random variable can only have two values, 0 or 112. The variable \\(y&#39;\\) is Bernoulli-distributed, the distribution that you get with one throw of a possibly unfair coin (see chapter 11). The Bernoulli distribution has a single parameter, \\(\\theta\\), which is the probabilty that the outcome of a single draw will equal 1. In logistic regression, we try to predict this parameter \\(\\theta\\) from predictor variables. The link function referred to above in the second modification is, for the case of logistic regression, the logit function \\(l\\) in eq (18.2). The logit function links the parameter of the distribution that we want to predict (here \\(\\theta\\)) to a linear function of the predictor variable(s). Fitting of GLM’s, in contrast to ordinary LM’s or non-linear models with normal-distributed response variables, is not carried out by minimizing the sum of squared residuals. The reason is that least-squares does not yield unbiased estimates if the response variable is not normal-distributed. To fit a GLM, a technique called likelihood maximization is used. For normal-distributed response variables, likelihood maximization coincides with minimization of the sum of squared residuals. This does not hold for other distributions. Likelihood an likelihood maximization If we think a certain probability distribution applies to events that we measure, for example ‘head’ in a coin toss or ‘2.41’ for a measurement, then the likelihood of the event is the probability that it occurs according to our probability distribution or, if the variable is continuous, then it is the value of the probability density function. Examples: (Discrete variable) We believe that tosses from a coin that we have are drawn from a Bernoulli distribution with parameter \\(\\theta\\). Then likelihood of observing a ‘1’ (head) in a coin toss equals \\(l = \\theta\\). In this case it is just the probability of making this observation. (Continuous variable) We believe that measurements of suitcase weight in an airport are drawn from a normal distribution with parameters \\(\\mu\\) and \\(\\sigma\\), then the likelihood of observing weight \\(y\\) equals \\[ l = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\left( \\frac{y - \\mu}{\\sigma} \\right)^2} \\] If we make multiple independent draws \\(1 \\ldots n\\) from the distribution, then the likelihood \\(\\mathcal{L}\\) of the entire data set is the product of their individual likelihoods \\(\\mathcal{L} = l_1 \\times \\ldots \\times l_n\\). In case of discrete distributions, \\(\\mathcal{L}\\) is just the probability of the joint events, or the probability of measuring the complete data set. Since the \\(l_i\\) are numbers ususally considerably smaller than 1, \\(\\mathcal{L}\\) is an extremely small number, unpractical to use in calculations. Therefore, ususally the log-likelihood is calculated: \\[ \\ln(\\mathcal{L}) = \\ln(l_1 \\times \\ldots \\times l_n) = \\ln(l_1) + \\ldots + \\ln(l_n) = \\Sigma_i \\ln(l_i)\\] The likelihood can be used to select parameters that maximize the observation of a data set under a particular statistical model. In the case of our logistic regression example, we have a data set of indepenent response (\\(y\\), being gender, translated to 0 or 1) and predictor (\\(x\\), beign body height) pair observations \\((y_i, x_i)\\) where \\(i=1 \\ldots n\\), and we have a model that predicts the probability that the person is a man: \\[\\theta(x, \\beta_0, \\beta_1) = \\frac{e^{f(x, \\beta_0, \\beta_1)}}{1 + e^{f(x, \\beta_0, \\beta_1)}}, \\quad \\text{with } f(x, \\beta_0, \\beta_1) = \\beta_0 + \\beta_1 x\\] as a function of \\(x\\) and the parameters \\(\\beta_0\\) and \\(\\beta_1\\). These parameters still need to be determined. Since the pairs \\((y_i, x_i)\\) are numbers that we can substitute, the (log-)likelihood of the entire data set will only be a function of the two parameters. The likelihood of a single observation \\((y_i, x_i)\\) is (see chapter 11): \\[ l_i(\\beta_0, \\beta_1) = \\theta(x_i, \\beta_0, \\beta_1)^{y_i} \\times \\left( 1 - \\theta(x_i, \\beta_0, \\beta_1) \\right)^{1 - {y_i}} \\] and the log-likelihood of the entire data set will be \\[ \\ln(\\mathcal{L}(\\beta_0, \\beta_1)) = \\sum_i \\theta(x_i, \\beta_0, \\beta_1)^{y_i} \\times \\left( 1 - \\theta(x_i, \\beta_0, \\beta_1) \\right)^{1 - {y_i}} \\] Likelihood maximazation then boils down to choosing the parameters \\(\\beta_0\\) and \\(\\beta_1\\) in such a way that \\(\\mathcal{L}(\\beta_0, \\beta_1)\\) (or \\(\\ln(\\mathcal{L}(\\beta_0, \\beta_1))\\)) is maximized. In other words, the probability of observing the data set is maximized under our model. 18.2.2 Logistic regression in R In R we perform logistic regression using the glm() function for generalized linear modeling. This function is part of the standard stats package which is loaded at the start of R. The function can be called in the same way as the lm() function for the example of predicting gender from body height: log.fit &lt;- glm(gender ~ height, data=bodymetrics, family=&#39;binomial&#39;) The argument family='binomial' tells the algorithm that the response variable has a binomial distribution13. The fitted prediction of \\(\\theta(x)\\) looks as follows: Figure 18.4: Logistic regression of the bodymetrics data using the gender as the response variable and height as the predictor variable. The vertical red line is the divide between the probability of being male larger or smaller than \\(0.5\\). The fit has the following coefficients for the function \\(f(\\text{height}) = \\beta_0 + \\beta_1 \\text{height}\\) \\(\\beta_0 = -46.8\\) \\(\\beta_1 = 0.273\\) So, the fitted linear function \\(f(x)\\) equals the log-odds of the probability that the person is a man. This is equal to 0 if \\(p = 1-p = 0.5\\), and it is \\(&gt;0\\) if the probability that the person is a man is larger than that the person is a woman. The fitted \\(f(\\text{height})\\) equals zero when \\(\\text{height} = -\\frac{\\beta_0}{\\beta_1} = 171\\) cm. A vertical line is drawn at this position in figure 18.4. The fitted model can be used as a true classifier by using the rule: \\[ \\text{gender} = \\begin{cases} \\text{male}, &amp; \\text{if } p \\geq 0.5 \\\\ \\text{female}, &amp; \\text{if } p &lt; 0.5 \\end{cases} \\] which is equivalent to \\[ \\text{gender} = \\begin{cases} \\text{male}, &amp; \\text{if } \\text{height} \\geq - \\beta_0 / \\beta_1 \\\\ \\text{female}, &amp; \\text{if } \\text{height} &lt; - \\beta_0 / \\beta_1 \\end{cases} \\] One way to assess the goodness of fit of this model is to make a confusion matrix, a table that tells you per level of the response variable how well the classifier fitted the training data: ## prediction ## truth F M ## F 216 44 ## M 45 202 This shows that the prediction for males and females is approximately equally accurate, and that the overall accuracy equals 82%, which is fair, but not very good (1 in 5 is false). Can it be improved? We have other predictors as well, namely weight and age, and there is no reason to not add them to the linear predictor equation: \\(f(x) = \\beta_0 + \\beta_1 \\text{height} + \\beta_2 \\text{weight} + \\ldots\\). Let’s add weight: log.fit2 &lt;- glm(gender ~ height + weight, data=bodymetrics, family=&#39;binomial&#39;) This yields the following confusion table: ## prediction ## truth F M ## F 222 38 ## M 37 210 with an overall correct prediction of 85%. This is a little better than before but not much. We could have expected this, because it appeared earlier that height and weight are highly correlated, indicating that if you know one, the other doesn’t yield much additional information. 18.2.3 Exercises Means and variance were calculated from the data↩ Odd: in 16-th century England an “amount by which one thing exceeds or falls short of another”, but see, in the online etymology dictionary, its origin from the Norse oddi and strange development in that language from a word that is related to dutch and german oord or Ort (point, place). Odd is truly odd.↩ If the original random variable has two character labels, like ‘M’ and ‘F’, then these are first converted to a new variable \\(y&#39;\\) that has the values \\(0\\) or \\(1\\).↩ Actually, a Bernoulli distribution.↩ "],
["class-dairy-strains.html", "CHAPTER 19 Classification of dairy bacteria 19.1 Exercises", " CHAPTER 19 Classification of dairy bacteria (Techniques: hierarchical clustering, decision tree classifier) This section deals with the data from an experiment by Bachmann et al. (2009) in which the diversity of enzyme activities was assessed in different strains of the bacterial species Lactococcus lactis. A few other, independently determined characteristics of these strains are also included. Several strains of this organism are used in dairy fermentations, in particular in the production of cheese. Lactococcus lactis is the main bacterium active in ripening cheeses. During ripening these bacteria convert a small part of the milk proteins and fats to amino acids and volatile compounds. These are an important determinant of the flavour of cheese. The data is available on the server (data/strain_diversity) in the file “straindiversity.dif”. The file has a format called “DIF” (Data Interchange Format). DIF is a text-based format designed for spreadsheet-like data. You can find more information about this format on the internet. R has a standard function, read.DIF(), for reading such files. 19.1 Exercises Read the data from the server in an R data frame using the read.DIF() function. The file has a header and also contains missing data entries, indicated by the string “nd”. Use the correct settings of the read.DIF() function to automatically convert the header into the column names of the data frame and to set the missing entries to the value NA. Print a summary of the data on screen with the summary() function. Show solution Loading the data with the read.DIF() function: f &lt;- file.path(baseurl, &#39;data&#39;, &#39;strain_diversity&#39;, &#39;straindiversity.dif&#39;) d &lt;- read.DIF(f, header=TRUE, na.strings=&#39;nd&#39;) 19.1.1 Description of the data The columns “Strain” and “Other.Strain.Code” contain strains names of the NIZO (former Netherlands Institute for Dairy Research) strain collection and strain codes from other collections and publications. The “Origin” column has two levels, showing whether the strain was collected from a dairy product or was isolated from another source (plant material, for example). The “TaxonomicPosition” column gives the official taxonomic name of the strain. There are three levels in this column. The “Variant” column indicates the subspecies or variant type. There are three levels in this column. The column “Cremoris.Like.Genotype” contains logical values, coded as 1 for TRUE and 0 for FALSE. This characteristic is based on sequence information14. Some subspecies of the “lactis” type actually display more similarity to the genotype thought to be specific for the subspecies “cremoris”. They are still called lactis subspecies, because of a set of phenotypic characteristics. The subsequent ten columns contain activity measurements of five enzymes measured in the strains when the bacteria are grown either on a growth medium called GM17 or on a chemically defined growth medium called CDM. GM17 is a rich medium of largely unknown composition containing peptides, among other components. The CDM medium contains only a sugar, salts, amino acids and vitamins. The five enzymes are branched chain aminotransferase (BcaT), involved in the degradation of branched chain amino acids (ile, val, leu); alpha-hydroxyisocaproic acid dehydrogenase (HicD), involved in the degradation of aromatic amino acids; two peptidases PepN and PepXP, involved in the degradation of peptides to amino acids; finally, an esterase activity involved in the degradation of fatty acids. These enzymes were chosen because they are known to be involved in the development of flavour compounds in cheese. 19.1.2 Data conversion Convert the “Cremoris.Like.Genotype” column into a logical vector instead of a numerical vector. Remember the fact that many functions in R, including the coercion functions themselves, have a default smart way of data coercion. Show solution Using the as.logical() coercion function: d$Cremoris.Like.Genotype &lt;- as.logical(d$Cremoris.Like.Genotype) Or if you don’t want to rely on the default coercion of as.logical(): d$Cremoris.Like.Genotype &lt;- ifelse(d$Cremoris.Like.Genotype==1,TRUE,FALSE) Make two index vectors that contain the column indices of the cultures grown on GM17 on the one hand and the cultures grown on CDM on the other hand. Show solution We define GM17 and CDM as the ranges: GM17 &lt;- 8:12 CDM &lt;- 13:17 Using apply() or lapply() (remember, the data frame class is a subclass of the list class, so you can use lapply()), print the classes of the elements of the data frame that contain the enzyme activity data. Show solution We use apply() to the columns: apply(d[,c(GM17,CDM)], 2, class) ## GM17.BcaT GM17.HicD GM17.PepN GM17.PepXP GM17.Esterase ## &quot;character&quot; &quot;character&quot; &quot;character&quot; &quot;character&quot; &quot;character&quot; ## CDM.BcaT CDM.HicD CDM.PepN CDM.PepXP CDM.Esterase ## &quot;character&quot; &quot;character&quot; &quot;character&quot; &quot;character&quot; &quot;character&quot; #lapply(d[,c(GM17,CDM)], class) Among the enzyme activity columns, convert those that are character class vectors to numeric vectors using apply() and the assignment operator. Check the classes of the data frame elements again. Show solution Converting and checking the classes of the GM17 and CDM ranges: d[,c(GM17,CDM)] &lt;- apply(d[,c(GM17,CDM)], 2, as.numeric) apply(d[,c(GM17,CDM)], 2, class) ## GM17.BcaT GM17.HicD GM17.PepN GM17.PepXP GM17.Esterase ## &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ## CDM.BcaT CDM.HicD CDM.PepN CDM.PepXP CDM.Esterase ## &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; 19.1.3 Data analysis We will now explore the information present in the data. We are interested in differences in enzyme activities that we can link to the other characteristics of the strains. We are also interested in correlations between the activities, for example in cells grown in GM17 and in CDM. Make a pairplot of all the enzyme activity columns. Use the ranges defined above to select the columns, and remember that the default plot method of a data frame is a pairplot of all columns against each other. Use a plotting symbol that gives a nice graph. Since the activity range between the different strains and conditions is considerable, make the same plot using logarithmically transformed activities. This often yields a better spread of the data in a plot. Show solution Making two pairplots: plot(d[,c(GM17,CDM)], pch=20) plot(log(d[,c(GM17,CDM)]), pch=20) Calculate a correlation matrix of the enzyme activity data, using both Pearson and Spearman rank correlation definitions. There is one column causing difficulties because of missing values. Still, we want to incorporate it. Read the help file of the correlation function to learn how to do that. Show solution Calculating the Pearson and Spearman correlation coefficients with cor(): cor(d[,c(GM17,CDM)], method=&#39;pearson&#39;, use=&#39;complete.obs&#39;) cor(d[,c(GM17,CDM)], method=&#39;spearman&#39;, use=&#39;complete.obs&#39;) We are mostly interested in positively or negatively correlated activities. Pairplots and correlation coefficients yield this information, but another efficient way to summarize these is a hierarchical tree, constructed using some measure of distance between the samples. As a distance measure, we could take (1 - correlation coefficient). There is a standard clustering function in R called hclust(). Find out what it needs as an input, and make a plot of the calculated tree object using its plot() method. What is the effect of different tree agglomeration methods and of different correlation coefficients (Pearson, Spearman)? Compare the trees with the pairplots. Show solution Using Ward’s agglomeration method and Spearman’s rank correlation as the distance metric. The cluster tree nicely summarizes the information from the matrix of correlation coefficients. distance &lt;- as.dist(1-cor(d[,c(GM17,CDM)], method=&#39;spearman&#39;, use=&#39;complete.obs&#39;)) hc &lt;- hclust(distance, method=&#39;ward.D&#39;) plot(hc, hang=-1) We would also like to investigate relations between activities and the type of strains. To explore these putative relations make pairplots, using your preferred data transformation, in which you give the points different colors or symbols, depending on the type of bacteria which was contained in the cheeses. To help you get started with this: Create a vector of colors or symbol numbers having the same length as the number of levels in the factor that you want to use to distinguish points. For example, the number of levels in the “Variant” column equals 3. So, create a vector mycolors &lt;- c('red','green','blue'). These color names will be recognized by R. Then, in the plot command use the argument col=mycolors[d$Variant]. The “[” function will automatically coerce the factor to a numeric vector, containing numbers 1, 2, or 3 (check as.numeric(d$Variant)). This numeric vector is then used as an index vector to select the appropriate color from the mycolors vector. Show solution plot(log(d[,c(GM17,CDM)]), cex=0.5, col=c(&#39;red&#39;,&#39;green&#39;,&#39;blue&#39;)[d$Variant]) plot(log(d[,c(GM17,CDM)]), pch=20, col=c(&#39;red&#39;,&#39;green&#39;)[d$Origin]) plot(log(d[,c(GM17,CDM)]), pch=20, col=c(&#39;red&#39;,&#39;green&#39;)[d$Cremoris.Like.Genotype+1]) In the last command the function +() takes care of the coercion of the logical vector d$Cremoris.Like.Genotype to a numeric vector. The previous plots show that colors (or symbol types) often cluster, suggesting that we could use the measured activities to distinguish strains. To explore this possibility we are going to use decision trees. If you don’t know what these are, look it up in the internet. Install the R-package called party. Take a look at the help page of its ctree() function, in particular at the example that uses ctree() for the classification of the iris data. Also notice the special formula notation Species ~ ., where “.” stands for all variables in the data frame except those in the left hand side (“Species” here). Make and plot a classification tree for the Origin (dairy vs non-dairy) of the strain. Also make one for the Variant. You will have to supply the ctree() function with customized data frames consisting of selected columns from the original data frame to be able to use the same formula syntax as in the iris example. Which factor seems to be highly specific for the dairy origin? Show solution Using the “party” package we make decision trees that are able to distinguish strains of dairy- and non-dairy origin or strain variants: library(party) origtree &lt;- ctree(Origin~., data=d[,c(4,GM17,CDM)]) plot(origtree) vartree &lt;- ctree(Variant~., data=d[,c(4,6,GM17,CDM)]) plot(vartree) The PepXP enzyme activity is a strong determinant of the origin of strains, where dairy origin seems to be associated with high activity of this enzyme. This is not surprising, because the PepXP peptidase is involved in the degradation of milk protein to amino acids. Amino acids are needed by these bacteria to be able to grow. Compare prediction and actual values for these classification trees: note that ctree objects also have a predict() method! Show solution To compare actual and predicted classes we can make a list like: cbind(as.character(d$Origin),as.character(predict(origtree))) Can you give a critique concerning the structure of the current data set, when trying to make a decision tree differentiating the origin of the strains? Support your critique with numbers! Show solution Critique: the data is highly unbalanced with respect to origin and variant: table(d$Origin) ## ## dairy non-dairy ## 25 59 table(d$Variant) ## ## cremoris diacetylactis lactis ## 9 7 68 This leads to bias in the selection of variables used for the tree. Trees constructed using such data will probably not be good predictors for strains from unknown class. Furthermore, there is a high correlation between origin and variant types. Almost all lactis types are of non-dairy origin, all cremoris types are of dairy origin and almost all diacetylactis types are of dairy origin: table(interaction(d$Origin,d$Variant)) ## ## dairy.cremoris non-dairy.cremoris dairy.diacetylactis ## 9 0 6 ## non-dairy.diacetylactis dairy.lactis non-dairy.lactis ## 1 10 58 How could you test the accuracy of such a prediction tree with the current data set? Show solution The prediction trees could, for example, be tested with the leave-one-out procedure. Take one row out of the data set and construct a tree predictor from the remaining data. Then predict the class of the excluded row, and note whether it is correct or not. Do this for all rows. This gives an impression of the accuracy. However, the unbalancedness of the data will remain a problem. References "],
["naivebayes.html", "CHAPTER 20 Naïve Bayes classifiers 20.1 The Naive Bayes classifier 20.2 A crime scene 20.3 Reconstructing Bayes law/theorem 20.4 Deciding on the class 20.5 Continuous predicting variables 20.6 Combining information from different predictor variables 20.7 Exercises", " CHAPTER 20 Naïve Bayes classifiers Techniques: building a Naive Bayes classifier 20.1 The Naive Bayes classifier We treat the Naive Bayes classifier because it is a classifier that is popular and used often in classification tasks. Once you have internalized Bayes Law, the method is also easy to understand. Despite its simplicity and the simplifying assumptions behind it, the Bayes Classifier is very successful in many classification tasks. We explain the classifier using an example. 20.2 A crime scene At a conference for members of boards of directors a murder has taken place: the head of a company has been shot in his hotel room. The window to his room, located on the ground floor, is open. Those that arrived at the scene immediately after they heard the shot, perceived a hint of perfume in the room, but they could not remember whether it was a masculine or feminine perfume. The police found a footprint, most likely belonging to the murderer, in the flower bed below the window. The foot size was 24.3 cm. Since the conference hotel was at a strictly guarded site, only a conference participant could be the murderer. The police wants to know whether, based on current evidence, it is more likely that a man or a woman was the murderer. Sherlock lights a pipe We are going to use a naïve Bayes classifier to solve this problem. We are told that a person has a particular foot size. What does this tell us about the gender of this person? If there is a correlation between foot size and gender, then the variable foot size “carries information” about the variable gender. We know that such a correlation exists. If the footprint size is small, it tells us that the chance that it belongs to a woman is larger than that it belongs to a man, since in the entire population, women have this foot size more often than men. We were also told that the murderer wears perfume. This gives information about the gender if there is a difference in the fraction of men or women that wear perfume. How can we combine these two pieces of information to a single number that tells us whether it is more likely that the person is a woman or a man? To tackle this problem we need to recall the teachings of the reverend Thomas Bayes. Sherlock Holmes used Bayesian reasoning, we may conclude from his slogan “when you have eliminated the impossible, whatever remains, however improbable, must be the truth”. 20.3 Reconstructing Bayes law/theorem We have to use Bayes law. Bayes law is about two (or more) characteristics in a population and about whether and how much knowing one characteristic tells us about an other chracteristic. We reconstruct Bayes law using the following table: perfumed \\ gender male female yes 0.10 0.35 no 0.40 0.15 This table concerns the general population of people, not just boards of directors. Two variables are defined for each member of the population, “gender” and “perfumed” (wearing perfume). The probabilities with which the combinations of the values of these variables (male, female, yes, no) occur in the population is shown in the table. Each entry in the table represents a joint probability. The joint probability is the probability of finding this combination of properties when choosing a person at random from the population. Note that the sum of all probabilities adds up to 1, as it should. In terms of probabilities, Bayes law is about conditional probabilities, i.e. the probability of making an observation about one variable, when the value of another variable is given. Suppose you have observed that a person is a man, i.e. you know that gender=male. In this case we only have to consider the first column of the table to know something about the other variable (perfumed). From the first column of the table we can calculate the chance of observing that this man also wears perfume. It is: \\[ Pr(\\text{perfumed=yes} | \\text{gender=man}) = \\frac{Pr(\\text{perfumed=yes , gender=man})}{Pr(\\text{gender=man})} = \\frac{0.10}{0.10 + 0.40} = 0.20 \\] The terminology is as follows: \\(Pr(A|B)\\) is the probability of an observation A on an object (i.e. observing that a variable has a particular value) under the condition that observation B on this object was already made (i.e. we know that another variable has a particular value). This is \\(Pr(\\ldots|\\ldots)\\) is called a conditional probability. \\(Pr(A, B)\\) is a joint probability, i.e. a probability of making both observations A and B on an object. In what follows, we will use an abbreviated version of the terms used above. We will abbreviate perfumed to P, gender to \\(G\\), male to m and female to f. We use italics \\(G\\) to indicate gender, because it is the variable that we want to predict from other variables. So, the formula above abbreviates to: \\[ Pr(\\text{P=yes} | G=\\text{m}) = \\frac{Pr(\\text{P=yes}, G=\\text{m})}{Pr(G=\\text{m})} = \\frac{0.10}{0.10 + 0.40} = 0.20 \\] \\(Pr(G=\\text{m})\\) is also called a “marginal probability”. It can be written “in the margin” of the table above as the sum of the probabilities in the “male”-column. This relation is also often written as expressing the joint probability in terms of a conditional and a marginal probability: \\[ Pr(\\text{P=yes}, G=\\text{m}) = Pr(\\text{P=yes} | G=\\text{m}) \\cdot Pr(G=\\text{m}) \\] We can also reason the other way around. Suppose we have observed that the person wears perfume. What is the chance of this person being a man? This is \\[ Pr(G=\\text{m} | \\text{P=yes}) = \\frac{Pr(\\text{P=yes}, G=\\text{m})}{Pr(\\text{P=yes})} = \\frac{0.10}{0.10 + 0.35} = 0.22 \\] or \\[ Pr(\\text{P=yes}, G=\\text{m}) = Pr(G=\\text{m} | \\text{P=yes}) \\cdot Pr(\\text{P=yes}) \\] Clearly, the joint probability \\(Pr(\\text{P=yes}, G=\\text{m})\\) connects the two conditional probabilities, and this is what Bayes’s law states. It can be derived easily from the two previous statements (Bayes’s law or Bayes’s theorem): \\[ Pr(G=\\text{m} | \\text{P=yes}) = \\frac{Pr(G=\\text{m}) \\cdot Pr(\\text{P=yes} | G=\\text{m)}}{Pr(\\text{P=yes})} \\] \\[ \\left( = \\frac{0.50 \\cdot 0.20}{0.45} = 0.22 \\right) \\] Or, stated more generally \\[ Pr(G|\\text{P}) = \\frac{Pr(G) \\cdot Pr(\\text{P}|G)}{Pr(\\text{P})} \\] In the language of Bayesian statisticians, \\(Pr(G)\\) is called a “prior” probability (distribution) and \\(Pr(\\text{P}|G)\\) is called a “likelihood” (distribution). In the average human population, the fraction of men is approximately equal to the fraction of women, so the prior distribution \\(Pr(G)\\) is 1:1 (m:f). But, we investigate boards of directors. In this (sub) population the prior distribution \\(Pr(G)\\) is not the same as in the general population. Here it is 80:20 (m:f), i.e. \\(Pr(G=\\text{m})\\)=0.80. Among men and women in boards of directors, the use of perfume is similar to that in the general population, i.e. the likelihoods \\(Pr(\\text{P}|G)\\) are the same as in the general population, namely 70% of the women wears perfume and 20% of the men. You take a member of this population and observed that he or she uses perfume (P=yes). If we consider this population, what are the odds that this person is a man or a woman? It is \\[ Pr(G=\\text{m} | \\text{P=yes}) = \\frac{Pr(G=\\text{m}) \\cdot Pr(\\text{P=yes}|G=\\text{m})}{Pr(\\text{P=yes})} = \\frac{0.80 \\cdot 0.20}{Pr(\\text{P=yes})} = \\frac{0.16}{Pr(\\text{P=yes})} \\] and \\[ Pr(G=\\text{f} | \\text{P=yes}) = \\frac{Pr(G=\\text{f}) \\cdot Pr(\\text{P=yes}|G=\\text{f})}{Pr(\\text{P=yes})} = \\frac{0.20 \\cdot 0.70}{Pr(\\text{P=yes})} = \\frac{0.14}{Pr(\\text{P=yes})} \\] Although we do not explicitly calculate \\(Pr(\\text{P=yes})\\), we can already say that, based on this information only, it is more likely that we are dealing with a man than with a woman (\\(Pr(G=\\text{m} | \\text{P=yes}) &gt; Pr(G=\\text{f} | \\text{P=yes})\\)), because we divide both expressions above by the same marginal probabilty \\(Pr(\\text{P=yes})\\). 20.4 Deciding on the class The previous calculation demonstrates how we could use prior and likelihood distributions to make a decision on the gender classification of this person. The rule, called maximum a posteriori rule or MAP rule, is that we decide on that value of gender that maximizes the \\(Pr(G|\\text{P})\\), hence the value for gender that maximizes the numerators calculated above. Since \\[ Pr(G=\\text{m}|\\text{P=yes}) \\varpropto 0.80 \\cdot 0.20 = 0.16 \\] and (\\(\\varpropto\\) means: “is proportional to”) \\[ Pr(G=\\text{f}|\\text{P=yes}) \\varpropto 0.20 \\cdot 0.70 = 0.14 \\] using the MAP rule we would classify the person as a man. It is not a very convincing case though, because the values do not differ very much. Actually, although we do not use these numerical values, they do tell us something about the degree to which we should be convinced of a classification. It is, namely, possible to reconstruct the probability \\(Pr(G=\\text{m}|\\text{P=yes})\\) from them. The sum of the two conditional probabilities is equal to the marginal probability: \\[ Pr(G=\\text{m}|\\text{P=yes}) + Pr(G=\\text{f}|\\text{P=yes}) = Pr(\\text{P=yes}) \\] because \\(G=\\text{m}\\) and \\(G=\\text{f}\\) covers all possible combinations (the whole row in the table) corresponding to \\(\\text{P=yes}\\). Therefore, \\[ Pr(G=\\text{m}|\\text{P=yes}) = \\frac{0.80 \\cdot 0.20}{0.80 \\cdot 0.20 + 0.20 \\cdot 0.70} = 0.53 \\] and \\[ Pr(G=\\text{f}|\\text{P=yes}) = \\frac{0.20 \\cdot 0.70}{0.80 \\cdot 0.20 + 0.20 \\cdot 0.70} = 0.47 \\] showing quantitatively, in terms of probabilities that our case for the classification “male” is not a very strong one. However, we do not use these conditional probabilities in a naïve Bayesian classifier. There, we only want to know which of the numerators on the right hand side yields the maximum value. To conclude: given only the observation of perfume, and the proportion of men among boards of directors, we would, using a Bayes classifier, decide that the murderer is most likely a man. 20.5 Continuous predicting variables perfumed or P is a discrete variable, a factor in R terminology. How do we do the same analysis with a continuous variable like foot size? And furthermore, how do we combine this information with information about wearing perfume to decide on the gender classifcication of the murderer? Let’s deal with these two problems one by one. First, how do we extend our previous analysis to the case of a continuous variable like foot size? Previously we knew the discrete conditional distribution \\(Pr(\\text{P}|G)\\), i.e. the probabilities of all combinations of values of gender and perfumed, and we could make a table of these probabilities. Now, we need to have information about the (continuous) distribution of foot sizes among men and among women, i.e. probability density functions \\(f\\) of foot size conditional on gender. Suppose, for simplicity, that these are normal-distributed with means \\(\\mu_m\\) and \\(\\mu_f\\) and standard deviations \\(\\sigma_m\\) and \\(\\sigma_f\\). We have measured these means and standard deviations in the general human population and assume that they are valid for boards of directors as well. Then, abbreviating the variable “foot size” as S, we have \\[ f(\\text{S}=s | G=\\text{m}) = \\frac{1}{\\sqrt{2 \\pi \\sigma_m^2}} e^{\\left(\\frac{s-\\mu_m}{2 \\sigma_m}\\right)^2} \\] and \\[ f(\\text{S}=s | G=\\text{f}) = \\frac{1}{\\sqrt{2 \\pi \\sigma_f^2}} e^{\\left(\\frac{s-\\mu_f}{2 \\sigma_f}\\right)^2} \\] For a population in which \\(\\mu_m\\)=26.9, \\(\\mu_f\\)=24.2, \\(\\sigma_m\\)=1.4 and \\(\\sigma_f\\)=1 these would look like the figure below. The police observed that “foot size=24.3” (grey dashed line). We could make a decision rule that is based on the conditional density functions, namely that whichever density is larger at the given size corresponds to the gender that we decide is most likely. We should note that probability density is not a probability! We get a probability from a probability density function by integrating over a piece of the variable “foot size”. However, the probability density at “foot size=24.3” is almost proportional to a small integrated interval around this value. Therefore, we can use both probability density functions to obtain values that are proportional to the conditional probabilities of observing foot sizes close to 24.3. Having 80% of men again as marginal probability, we get \\[ Pr(G=\\text{m}|\\text{S}=24.3) \\varpropto f_m(24.3) \\cdot 0.80 = 0.041 \\] and \\[ Pr(G=\\text{f}|\\text{S}=24.3) \\varpropto f_f(24.3) \\cdot 0.20 = 0.079 \\] In this case we would decide that the murderer is a woman. Note again that these are not probabilities, but are only numbers proportional to the conditional probabilities \\(Pr(G|\\text{S})\\). Also in this case we can reconstruct the conditional probabilities: \\[ Pr(G=\\text{m}|\\text{S}=24.3) = \\frac{f_m(24.3) \\cdot 0.80}{f_m(24.3) \\cdot 0.80 + f_f(24.3) \\cdot 0.20} = 0.34 \\] and \\[ Pr(G=\\text{f}|\\text{S}=24.3) = \\frac{f_f(24.3) \\cdot 0.20}{f_m(24.3) \\cdot 0.80 + f_f(24.3) \\cdot 0.20} = 0.66 \\] It shows that the case is a little more, you could say almost twice as much, in favour of a woman than a man being the murderer. Using Gaussian probability density functions to model the distributions of “foot size” is only one way of modeling a distribution. Many other standard distributions can be applied in naïve Bayes classifiers. You can use whichever distribution best fits the real distribution, or fits your well-founded ideas about it. Modeling and smoothing is particularly important if the number of training samples is small compared to the “sampling space”, as is clearly the case with continuous variables. You can find a few examples of distribution models used in naive Bayes classifiers in Wikipedia. 20.6 Combining information from different predictor variables How do we combine the two pieces of information about the variables perfumed and foot size to decide whether we are dealing with a man or a woman? For this we need the Bayes chain rule, or chain rule for conditional probabilities. It says that the joint probability of observing certain values for variables \\(A_n, A_{n-1}, \\ldots, A_1\\) is \\[ Pr(A_n, A_{n-1}, \\ldots, A_1, B) = Pr(A_n | A_{n-1}, \\ldots A_1, B) \\cdot Pr(A_{n-1} \\ldots A_1, B) \\] This rule can be applied recursively to \\(Pr(A_{n-1} \\ldots A_1, B)\\) etc., to get a chain of conditional probalities on the right-hand side: \\[ \\begin{multline} Pr(A_n, A_{n-1}, \\ldots, A_1, B) = \\\\ Pr(A_n | A_{n-1}, \\ldots A_1, B) \\cdot Pr(A_{n-1} | A_{n-2} \\ldots A_1, B) \\cdot \\: \\ldots \\: \\cdot Pr(A_1|B) \\cdot Pr(B) \\end{multline} \\] For example, in case of three variables foot size, perfumed and gender, we get, using the chain rule twice \\[\\begin{equation} Pr(\\text{P},\\text{S},G) = Pr(\\text{P}|\\text{S},G) \\cdot Pr(\\text{S}|G) \\cdot Pr(G) \\tag{20.1} \\end{equation}\\] It is often difficult to obtain distributions conditional on multiple variables, like \\(Pr(\\text{P}|\\text{S},G)\\). In particular, when the number of conditional variables is large, it is practically impossible to properly sample the space of all combinations (all dimensions) of predictive variables. To simplify matter, a naïve assumption is made, giving Naïve Bayes classifiers their name. It is that the observation of every variable is conditionally independent of the observation of the other variables, when conditioned on the variable that we want to predict. In our case this means that we assume that: \\[ Pr(\\text{P}|\\text{S},G) = Pr(\\text{P}|G) \\] Or stated in words: knowing the gender gives us all information about the probability density of foot size. Having additional knowledge of wearing perfume does not change these probability densities (“perfumed carries no additional information about foot size if we know gender (condition on gender)”). This simplifies the formula (20.1) for the joint probability considerably: \\[ Pr(\\text{P},\\text{S},G) = Pr(\\text{P}|G) \\cdot Pr(\\text{S}|G) \\cdot Pr(G) \\] Instead of having to make a model of a two-dimensional distribution \\(Pr(\\text{P}|\\text{S},G)\\), we approximate the joint distribution using the one-dimensional \\(Pr(\\text{P}|G)\\). The two conditional probabilities on the right hand side were already used above when studying the individual variables. Now, we only need to calculate their product to combine the information about the two variables! The joint probability on the left hand side can also be written as \\[ Pr(\\text{P},\\text{S},G) = Pr(G|\\text{P},\\text{S}) \\cdot Pr(\\text{P}, \\text{S}) \\] Combining the two formulas we obtain Bayes law for the distribution of \\(G\\) when \\(\\text{P}\\) and \\(\\text{S}\\) are known: \\[ Pr(G|\\text{P},\\text{S}) = \\frac{Pr(\\text{P}|G) \\cdot Pr(\\text{S}|G) \\cdot Pr(G)}{Pr(\\text{P}, \\text{S})} \\] where \\(Pr(\\text{P}, \\text{S})\\) is again a marginal probability that we do not need to know when comparing the different possibilities for gender classification. We only need to know relative sizes of the numerator \\(Pr(\\text{P}|G) \\cdot Pr(\\text{S}|G) \\cdot Pr(G)\\)! In general, the naïve assumption allows us to quantify the odds having only information about the one-dimensional conditional probability distributions \\(Pr(A_i | G)\\) when \\(G\\) is the variable that we want to predict with the classifier. So let’s see what we get when combining the two pieces of information, “S=24.3” and “P=yes”. \\[ Pr(G=\\text{m}|\\text{S=24.3}, \\text{P=yes}) \\varpropto f_m(24.3) \\cdot 0.20 \\cdot 0.80 = 0.0081 \\] and \\[ Pr(G=\\text{f}|\\text{S=24.3}, \\text{P=yes}) \\varpropto f_f(24.3) \\cdot 0.70 \\cdot 0.20 = 0.0556 \\] which, using the MAP rule, leaves us no other option than to conclude that the murderer was a woman! Also in this case we can reconstruct the conditional probability itself: \\[ Pr(G=\\text{f}|\\text{S=24.3}, \\text{P=yes}) = \\frac{0.0556}{0.0081 + 0.0556} = 0.87 \\] Surprisingly, this becomes a much more convincing case than when studying either of the single pieces of evidence individually! We should note that the numerical correctness of this probability may strongly depend on the correctness of the naïve assumption! The naïve assumption of conditional independence of the observed variables seems reasonable in case of variables “foot size” and “perfumed”. However, there are cases where it is clearly incorrect. For example, suppose that we also observe “shoe brand” (B) as a variable, from the imprint of a shoe sole. The probability of observing a certain shoe brand will not only depend on the gender, the variable that we want to predict, but also on the foot size. This is the case if certain shoe brands only produce small sizes for males, for example italian brands. If you know that you’re dealing with a man, then knowing his foot size gives you additional information about the probability of observing this particular brand! Therefore \\(Pr(\\text{B}|\\text{S},G) \\neq Pr(\\text{B}|G)\\)! Nevertheless, it was shown that often, naïve Bayes classifiers perform surprisingly well, even when the assumption of conditional independence of observed variables is clearly incorrect. Below we will see an example of that case. One of the reasons is, of course, that we do not use the exact numerical outcome of the likelihood product, or the conditional probability itself, but we only want to know which one is largest. This gives the method a certain robustness against violation of the assumption of conditional independence. Another reason is that, because of the simplicity of the model (due to conditional independence we use much less parameters to fit the probability distribution that models the data), it is quite insensitive to over-fitting. It is important to realize that there is a difference between “independence of variables” and “independence of variables conditioned on an other variable”. For example, the naïve assumption which says that “foot size” and “perfumed” are independent conditioned on gender, does not mean that “perfumed” and “foot size” are independent. They clearly are not! However, what it means is that when we look at the subpopulation of men only, we do not expect a relation between “foot size” and “perfumed”, nor do we expect such a relation when we look at women only. In other words, the correlation between foot size and perfumed is fully explained by the gender. 20.7 Exercises 20.7.1 Predicting iris species Let’s try this on a little more realistic data set, namely the iris data: ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 30 4.7 3.2 1.6 0.2 setosa ## 40 5.1 3.4 1.5 0.2 setosa ## 56 5.7 2.8 4.5 1.3 versicolor ## 85 5.4 3.0 4.5 1.5 versicolor ## 131 7.4 2.8 6.1 1.9 virginica ## 134 6.3 2.8 5.1 1.5 virginica Here we have a variable, Species, that we want to predict. It has three levels: setosa, versicolor, virginica. The predictor variables are all continuous valued. We will split the data in a training set, from which we will construct the “likelihood” and “prior” distributions, and a test set on which we will apply our classifier. In the “board of directors murder” example, we estimated the likelihood distributions (concerning the use of perfume and foot size) from the general population of human beings, and adapted the prior (male/female) distribution to what we knew about it for boards of directors. Here we will estimate these from the training set. Make two index vectors, trainset and testset that can be applied, using a construct like iris[trainset,], to split the iris data frame into a set of training samples (80% of the data) and a set of test samples (20%). Use a random selection of the data to make the split. Then split the data. Show solution trainset &lt;- sample(rownames(iris), size=trunc(0.8*dim(iris)[1])) testset &lt;- rownames(iris)[!(rownames(iris) %in% trainset)] trainsamples &lt;- iris[trainset,] testsamples &lt;- iris[testset,] Note: from here on, there are two ways of carrying out this exercise: building a classifier using your own functions, in steps 2-4 as suggested below, or by using the naiveBayes function from the e1071 package. To build the classifier yourself you need somewhat advanced mastering of the R language (or: you obtain this craftsmanship by building it yourself). Calculate an 1 \\(\\times\\) 3 array of priors from the test set (you know, the expected fractions of each of the three species), or make a prior based on your belief of what fractions you could expect. The columns should carry the species names. Show solution # Either calculate a prior: prior &lt;- tapply( rep(1/length(trainsamples$Species), length(trainsamples$Species)), trainsamples$Species, sum) # Or make one based on your belief of the distribution: prior &lt;- array(1/3, dim=3, dimnames=list(levels(iris$Species))) Calculate means and standard deviations of the four predictor variables for each of the species using the training set. We will need these to model their distributions conditional on species using normal distributions. Put the means and standard deviations in a list, where each element corresponds to a predictor variable and consists of matrices with three rows (corresponding to the species) and two columns (corresponding to the mean and standard deviation conditioned on species). Below you will find out why such a list is a convenient arrangement of the data. Tip: use a tapply nested in an lapply to generate a list containig for each of the predictor variables a list of vectors of mean and standard deviations conditional on Species. Use tapply as shown in the exercises in Chapter 8. Subsequently, put the inner lists in a matrix using cbind() called with do.call(). Show solution nb &lt;- lapply( apply(trainsamples[1:4], 2, function(x){ tapply(x, trainsamples[[5]], function(x){ c(mean(x),sd(x))})} ), function(x){do.call(rbind,x)} ) Explanation: The code consists of two nested apply-like functions. The outer part (lapply) loops over each of the predictor columns and sends it to the inner function. The inner part calculates the mean and standard deviation of that column split by species (using the fifth Species column) and puts it in a vector (mean, stdev). To be able to calculate a Gaussian probability density below, we need a function that takes a value, a mean and a standard deviation and returns the corresponding probability density. The standard dnorm() function seems to do this exactly. Now comes a difficult part. For each sample in the test set you have to calculate the dnorm() using the list constructed above. Show solution posteriors &lt;- t( apply(testsamples[1:4], 1, function(y) { apply( sapply(names(y), function(varname) { apply(nb[[varname]], 1, function(x) { dnorm(y[[varname]],x[1],x[2]) } ) }), 1, prod) })) head(posteriors) ## setosa versicolor virginica ## 12 5.42363976 1.735489e-14 9.631906e-25 ## 14 0.01700747 2.757857e-18 3.329668e-30 ## 21 2.14396121 2.473334e-13 4.767107e-23 ## 22 2.91354920 1.661920e-13 1.151746e-23 ## 27 2.67218120 3.834907e-12 1.516280e-22 ## 39 0.48441983 7.361253e-16 1.804447e-27 Explanation: The nested set of apply-like functions above looks quite complicated, but it solves the problem very effectively. How do you construct such a method? I started from the center, and asked: what if I have one variable from one row of the test set, how do I then calculate the conditionals? varname &lt;- &quot;Sepal.Length&quot; # for example y &lt;- testsamples[1,1:4] # first sample, for example Then the following apply gives me a vector of conditional values for this variable: apply(nb[[varname]], 1, function(x) {dnorm(y[[varname]],mean=x[1],sd=x[2])}) ## setosa versicolor virginica ## 0.91174102 0.08842467 0.01081573 I need to calculate these conditionals for each predictor variable. So, I take a vector with the names of y (names(y)) and lapply the previous function to each of these names. I then get a list of conditionals containing an entry for each of the predictor variables: lapply(names(y), function(varname) { # now the previous piece of code apply(nb[[varname]], 1, function(x) {dnorm(y[[varname]],x[1],x[2])}) }) ## [[1]] ## setosa versicolor virginica ## 0.91174102 0.08842467 0.01081573 ## ## [[2]] ## setosa versicolor virginica ## 0.9929458 0.1316289 0.5752756 ## ## [[3]] ## setosa versicolor virginica ## 1.748784e+00 1.269976e-06 6.186876e-14 ## ## [[4]] ## setosa versicolor virginica ## 3.425766e+00 1.174091e-06 2.502125e-09 I get a list of vectors with the same length! Using sapply instead of lapply would “simplify” this list of vectors to a matrix, which is easier to use in the subsequent step: sapply(names(y), function(varname) { # now the previous piece of code apply(nb[[varname]], 1, function(x) {dnorm(y[[varname]],x[1],x[2])}) }) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## setosa 0.91174102 0.9929458 1.748784e+00 3.425766e+00 ## versicolor 0.08842467 0.1316289 1.269976e-06 1.174091e-06 ## virginica 0.01081573 0.5752756 6.186876e-14 2.502125e-09 For each of the posteriors, I need to calculate the product conditioned on species name, which means that I need to calculate the products per row of the previous matrix, using a construct apply(..., 1, prod). prod calculates the product of elements in a vector: apply( # the previous piece of code sapply(names(y), function(varname) { apply(nb[[varname]], 1, function(x) {dnorm(y[[varname]],x[1],x[2])}) }), 1, prod) ## setosa versicolor virginica ## 5.423640e+00 1.735489e-14 9.631906e-25 Now this was for one row, y, of the test data. I now make a function of the previous piece of code and apply it to all rows in the testsamples data frame: apply(testsamples[1:4], 1, function(y) { # the previous piece of code apply( sapply(names(y), function(varname) { apply(nb[[varname]], 1, function(x) {dnorm(y[[varname]],x[1],x[2])}) }), 1, prod) }) This gives the result with posteriors in rows and samples in columns. I want to have then in the transposed arrangement, so I transpose the previous matrix and get the desired result. Make a prediction of the species for each flower in the test samples based on the likelihoods. Also calculate the fraction of correct predictions. Tip: use the max.col() function. Show solution priors &lt;- c(1/3,1/3,1/3) predictions &lt;- colnames(posteriors)[max.col(t(t(posteriors)*priors))] correct &lt;- sum(predictions==as.character(testsamples[[5]]))/length(predictions) correct ## [1] 0.9666667 This shows that a naïve Bayes classifier performs very well on this data set. Is the naïve assumption valid for the predictor variables in the iris data set? Demonstrate why or why not. Show solution No, it is clearly invalid. For example, we plot the petal width against the petal length. Clearly, both variables are correlated with species (which is why we can use them to identify the species). However, in contrast to the naïve assumption, we see that within each species both variables are also correlated: if you would only draw the red points, for example, you would still observe a correlation between both variables. plot(iris$Petal.Length, iris$Petal.Width, col=iris$Species, pch=19) This means that, under the condition that we know which species we have, we can make a prediction about the petal length and width. However, we can predict the petal length even more accurately if we also know the petal width (or the other way around). 20.7.2 Predicting income The adult data set contains census data from the american population. It also contains a column that says whether a person earns more or less than 50000 dollar per year. The goal is to predict the income from the other variables. The predictor variables are continuous and discrete (which is more advanced than the previous exercise). Instead of calculating model conditional distributions using your own code, use the naiveBayes() function from the e1071 package to do that. Split the data (80:20) in a training and a test set, and predict the income (more or less than 50K) from the model distributions. How well does your predictor perform? Show solution "],
["resampling-techniques.html", "CHAPTER 21 Resampling techniques 21.1 Bootstrapping 21.2 Exercises 21.3 Permutation test", " CHAPTER 21 Resampling techniques Resampling techniques are used in statistics when parametric methods can or should not be used. For example, when the distribution of a variable is not known, it is impossible without simplification, to perform parametric significance tests. Non-parametric tests may be possible, but usually have less power, i.e. they arte less sensitive to deviations from the null hypothesis. Similarly, it is impossible to calculate parametrically, for example, the variation (precision) in the mean of a sample from a population. Resampling techniques try to accommodate those shortcomings. Resampling techniques have become popular in the computer age, because they can be so easily performed by writing a small program. And in R it is very easy to apply resampling techniques. 21.1 Bootstrapping Bootstrapping (after Wikipedia) allows one to gather many alternative versions of the single statistic that would ordinarily be calculated from one sample. For example, suppose that we are interested in the height of people worldwide, and we suspect that it is not Normal distributed. As we cannot measure the whole population, we sample only a small part of it. From that sample only one value of a statistic can be obtained, i.e one mean (\\(\\overline{x}\\)), or one standard deviation (\\(s\\)) etc., and hence we don’t see how accurate that statistic is. If the height were Normal distributed we could have estimated the standard deviation of \\(\\overline{x}\\), i.e. the “standard error of the mean” by \\(s/\\sqrt{n}\\), where \\(n\\) is the size of the sample. We could also use bootstrapping to estimate this quantity. When using bootstrapping, we randomly extract a new sample of N out of the N sampled data, with replacement. By doing this several times, we create a large number of data sets that we might have seen, and compute the statistic for each of these data sets. Thus, we obtain an estimate of the distribution of the statistic (the bootstrap distribution). The key to the strategy is to create alternative versions of data that “we might have seen”. 21.2 Exercises 21.2.1 Medical patches A company produces medical patches that are designed to increase blood levels of certain natural hormones (this example is described in Efron and Tibshirani (1993)). The hormone levels were measured in each subject after wearing three types of patches: placebo, without medicine, oldpatch, manufactured at an old factory and newpatch, manufactured in a new factory (data are available in data/resampling/medicalpatch.txt). The purpose of the experiment was to show that the new factory produces patches that are bio-equivalent to those from the old factory. The U.S. Food and Drug Administration (FDA) had approved patches from the old factory and requires those from the new factory to give an expected relative deviation of the medical effect less than or equal to 20%, in other words, the requirement is that \\[ \\frac{|E(new) - E(old)|}{E(old) - E(placebo)} \\leqq 0.20 \\] with \\(E()\\) indicating the expectation of a random variable. Define the parameter \\(\\theta\\) as \\[ \\theta = \\frac{E(new) - E(old)}{E(old)-E(placebo)} \\] The question then is: is \\(|\\theta| \\leqq 0.20\\)? Download and study the data, in particular study correlations between the variables in the data set. What do you conclude? Show solution Download and study the data d &lt;- read.table(file.path(baseurl, &#39;data/resampling/medicalpatch.txt&#39;), header=T, sep=&quot;\\t&quot;) plot(d[2:4]) Clearly, placebo and patch data are correlated, possibly because the placebo data reflect differences in basic levels of hormone in the subject. The medical patches seem to elevate that basic level. Let’s define \\(x = new - old\\) and \\(y = old - placebo\\), so that \\(\\theta = \\frac{E(x)}{E(y)}\\). As a first approach of \\(\\theta\\), calculate the so-called “plug-in” estimate of \\(\\theta\\), which equals \\(\\hat{\\theta} = \\frac{\\bar{x}}{\\bar{y}}\\). What do you conclude from this value? Show solution Then we calculate the ratio of averages and display it: theta.plugin &lt;- mean(d$newpatch - d$oldpatch)/mean(d$oldpatch - d$placebo) theta.plugin ## [1] -0.0713061 Since the estimator \\(|\\hat{\\theta}|\\) is much smaller than 0.20, it could be that \\(|\\theta|\\) itself is smaller than the required 0.20. The plugin value is subject to random fluctuations, particularly since the sample size (8) is quite small. Using bootstrapping, we can generate many similar hypothetical experiments and study the distribution of \\(\\hat{\\theta}\\). Make 1000 bootstrap replications of the experiment by sampling 8 samples each time from the original experiment with replacement, and make a histogram of the distribution of \\(\\hat{\\theta}\\), with red vertical dashed lines indicating \\(|\\hat{\\theta}|=0.20\\). How should you sample from the data set in order to generate hypothetical data sets with a correlation structure comparable to the original set? Also make a quantile-quantile plot with the Normal distribution as the reference to check whether the \\(\\hat{\\theta}\\)’s are Normal-distributed. Show solution We should sample entire rows of data from the data set, so that correlations between the three measurements are preserved in the bootstrap samples. We can do that as follows: thetas &lt;- c() for (i in 1:1000) { sel.indices &lt;- sample(1:8, replace=TRUE) dboot &lt;- d[sel.indices,] thetas &lt;- c(thetas, mean(dboot$newpatch - dboot$oldpatch)/mean(dboot$oldpatch - dboot$placebo)) } Subsequently, we make the required histogram of \\(\\hat{\\theta}\\)’s # I&#39;m using plotmath (see ?plotmath) to print mathematical symbols in the main- and axis titles hist(thetas, col=&#39;grey&#39;, main=expression(paste(&#39;Histogram of bootstrapped &#39;, hat(theta),&#39;\\&#39;s&#39;)), xlab=expression(hat(theta))) abline(v=c(-0.2,0.2), col=&#39;red&#39;, lty=&#39;dashed&#39;) Then we make a quantile-quantile plot: qqnorm(thetas); qqline(thetas, col=&#39;red&#39;, lty=&#39;dashed&#39;) which clearly shows that this is not a Normal distribution. Calculate the mean and standard deviation of estimator \\(\\hat{\\theta}\\), and calculate its 95% confidence interval based on the assumption that \\(\\hat{\\theta}\\) is Normal-distributed and based on the bootstrap distribution (the latter is called a “percentile estimate”). Discuss the results. Show solution The mean and standard deviation are: mean.th &lt;- mean(thetas) sd.th &lt;- sd(thetas) mean.th ## [1] -0.06874824 sd.th ## [1] 0.09946031 The 95% confidence intervals are: # estimated 95% confidene interval of theta.hat if theta.hat were normal distributed. mean.th + 1.96*sd.th*c(-1, 1) ## [1] -0.2636904 0.1261940 # The factor 1.96 is used as rule-of-thumb and originates from the Standard Normal distribution. # It can be calculated as qnorm(c(0.025,0.975), 0, 1), which leaves 2.5% of the data on both # tails of the Normal distribution. Hence, more exactly mean.th + sd.th*qnorm(c(0.025,0.975), 0, 1) ## [1] -0.2636869 0.1261904 # estimated 95% confidene interval of theta.hat from the bootstrap distribution quantile(thetas, c(0.025,0.975)) ## 2.5% 97.5% ## -0.2346589 0.1541064 The quantile plot shows that the assumption of Normality is wrong. The corresponding estimated 95% confidence interval clearly deviates from the percentile interval. The distribution of bootstrap \\(\\hat{\\theta}\\)’s indicates that (the true) \\(\\theta\\) is likely to be located within the requirement by the FDA. In fact, we could calculate the fraction of the distribution within the limits \\(|\\hat{\\theta}| \\leqq 0.20\\) as: sum(thetas &gt; -0.2 &amp; thetas &lt; 0.2)/length(thetas) which shows that 91.2% of the bootstrap samples is within the required limits. Whether that is acceptable is again up to the FDA. If not, we would need to do more (real) experiments. We expect that this should decrease the uncertainty in \\(\\hat{\\theta}\\). There is an R-package called boot that can do very fast bootstrapping for us and that also has a few more sophisticated confidence interval estimation methods that yield more accurate results than the simple method applied above (for example bias in the estimator \\(\\hat{\\theta}\\) for the parameter \\(\\theta\\) is removed). For the next exercise you need to install this package. Use the boot() function of the boot package to make a bootstrap sample of size 1000 and use the plot() method of a boot object to display the results. Show solution We need to define a function that calculates our statistic \\(\\hat{\\theta}\\) from the data and an index vector that selects instances from the data (see the help file of the boot() function). Below, we call this function theta.hat. We calculate 1000 bootstrap samples and plot the results # library(boot) theta.hat &lt;- function(d, i) { mean(d[i,&#39;newpatch&#39;] - d[i,&#39;oldpatch&#39;])/mean(d[i,&#39;oldpatch&#39;] - d[i,&#39;placebo&#39;]) } myboot &lt;- boot(d, statistic=theta.hat, R=1000) plot(myboot) Calculate the 95% confidence interval from the boot object obtained above using the boot.ci() function. Use the “norm”, “perc” and “bca” methods. These correspond to the intervals based on the Standard Normal distribution, the percentile bootstrap distribution (as we calculated above) and the “Bias-corrected accellerated” algorithm (Efron and Tibshirani 1993). Show solution Letting the boot.ci() function calculate the three requested types of interval estimations: boot.ci(myboot, conf=0.95, type=c(&#39;norm&#39;,&#39;perc&#39;,&#39;bca&#39;)) ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = myboot, conf = 0.95, type = c(&quot;norm&quot;, &quot;perc&quot;, ## &quot;bca&quot;)) ## ## Intervals : ## Level Normal Percentile BCa ## 95% (-0.2731, 0.1207 ) (-0.2209, 0.1674 ) (-0.2072, 0.2222 ) ## Calculations and Intervals on Original Scale It shows that the two first types of estimates are very close to what we calculated before (pfui …), and that the third bias-corrected estimate is shifted a little more with the center around 0, which is good for FDA acceptance! 21.3 Permutation test As an example of a statistic with a known generating random process but with a difficult to calculate distribution, let us devise a statistic that should be able to detect patterns on a one-dimensional grid. The grid consists of \\(N\\) bins in a row. The bins are filled with \\(n&lt;N\\) identical objects. We want to find out whether the pattern of filled bins is random or not. We could devise a statistic that we think should be able to detect certain classes of local patterns. For example, by enumerating the total number of filled neighbour bins for all filled bins. Since, if a filled bin has a filled neighbour, then that neighbour also has a filled neighbour, we divide this number by 2. Let’s call this statistic the “neighbour count” or NC. Let’s encode the filling pattern by a vector of TRUE’s and FALSE’s, TRUE standing for filled, FALSE for empty. We find the pattern TFTFTFTFTFTFTFTFTFTF (rep(c(TRUE, FALSE), 10)), let’s call it the query vector. Clearly, none of the filled cells has a filled neighbour, so \\(NC=0\\) for this pattern. What is the chance of finding a pattern with an NC value as this one or lower if the bins are filled randomly? Make a function that calculates the NC for a particular vector. Show solution Suppose we have a pattern like this: 1 2 3 4 5 6 7 8 T T F F F T T F If we shift the pattern by 1 to the right and apply the logical &amp; operation, then the result tells us which cells had a neighbour to the left: 1 2 3 4 5 6 7 8 9 T T F F F T T F . . T T F F F T T F . T F F F F T F . If we now take the sum of all TRUE values we have the NC. A function implementing this is: nc &lt;- function(x) { sum(c(x,NA) &amp; c(NA,x), na.rm=TRUE) } # test it nc(rep(c(TRUE, FALSE), 10)) ## [1] 0 nc(rep(c(TRUE, FALSE), each=10)) ## [1] 9 Make 50000 random permutations of the query vector, and calculate the NC for each. Show solution You can make permutations using the sample() function without replacement (i.e. the default mode, replace = FALSE) mypattern &lt;- rep(c(TRUE, FALSE), 10) ncs &lt;- c() for (i in 1:50000) { ncs &lt;- c(ncs, nc(sample(mypattern))) } Make a histogram of the distribution of the NC. Which end of the histogram corresponds to improbable patterns? Show solution hist(ncs,nclass=9) Both ends correspond to improbable patterns (those with very low and very high neighbour counts). From the distribution of the NC, make an estimation of the significance of finding the query vector by chance. Show solution Is any of the NC values in the re-sampled case equal to 0? any(ncs==0) ## [1] TRUE I haven’t used set.seed(), so, if not then the chance must be smaller than \\(\\frac{1}{\\text{sample size}}\\) which equals \\(2\\times 10^{-5}\\). Else, this probability is approximately equal to: sum(ncs==0)/length(ncs) What about the vector TTFFTTFFTTFFTTFFTTFF (rep(c(TRUE, TRUE, FALSE, FALSE), 5)). Do we have a satisfying statistic? Show solution nc(rep(c(TRUE, TRUE, FALSE, FALSE), 5)) ## [1] 5 sum(ncs&lt;=5)/length(ncs) ## [1] 0.8149 which is large, although the pattern is clearly not random. Would it be possible to devise a more sensitive statistic than the neighbour count that detects such patterns too (open question)? References "],
["numerical-differentiation.html", "CHAPTER 22 Numerical differentiation and smoothing 22.1 Exercises", " CHAPTER 22 Numerical differentiation and smoothing (Techniques: de-noising, Savitzky-Golay filter) Figure 22.1: Three traces of enzyme activity measurements (right panel), a growth curve (left upper panel) of a bacterial culture and a bioluminescence trace of a growing culture (left lower panel). Often, the slope of an experimental trace (a time-series) like the ones in figure 22.1 is needed. For example, if you are interested in rates rather than in the absolute read-out of an experiment. If you are interested in the slope at a particular point of the curve, one way of calculating that slope is to take a linear part of a trace around the point of interest and to fit a straight line or a higher order polynomial through the data. The first order derivative of the fit is an estimation of the rate of change of the signal at the particular point. For example, for the enzyme kinetic curves in figure 22.1 we are interested in the initial rates, i.e. the rates at \\(t=0\\). For the growth curve in figure 22.1 we are interested in changes in the growth rate over the whole curve. And for the bioluminescence curve, we are interested in the maximal bioluminescence but also in the bioluminescence normalized to (divided by the) amount of bacteria (or optical density = OD). So, how can you calculate the first order (or higher order) derivative if you’re interested in the rate of change over the whole time series, and when you do not know the underlying mathematical shape of the curve? Or how do you calculate the maximal bioluminescence when the curve contains a lot of noise, as in the figure? We will try to answer these questions in the following exercises. 22.1 Exercises The data can be found in data/numdif. We start with the initial rates problem. For each of the curves determine by eye which of the points belong to an approximately linear part of the curve near the point \\(t=0\\), or at least to a portion of the curve around \\(t=0\\) where the slope doesn’t change too much, and we can still expect a reasonable fit. Then fit first-, second- and third-order polynomials through those points, and calculate the initial rates. How many points do you need at least for each of the polynomials, and what do you notice about those initial rate estimates? Which polynomial gives the right answer? Make detailed plots of the curves and fits to support the answer. Show solution Clearly, we need at least a number of points equal to the number of parameters of the fitted curve, i.e. 2 for a straight line, 3 for a 2-nd order polynomial and 4 for a 3-rd order polynomial. But taking that bare minimum doesn’t leave any room for filtering out measurement error. We notice that the curves start to bend off already close to the first point, especially for E3, so we should not take too many points for the straight line fits, i.e. let’s take a minimum of three points there. And let’s take the minimum + 1 point for the higher order polynomials. Below, I put the results in a table. Note that I am using the poly() function to define polynomials in the formulas. You could also yse a formula like E1 ~ I(time) + I(time^2) + I(time^3) to define a third order polynomial, for example. However, poly() from the stats package makes it easier to formulate higher order polynomials. f &lt;- file.path(baseurl,&#39;data/numdif/enzymekinetics.tab&#39;) d &lt;- read.table(f, sep=&quot;\\t&quot;, head=TRUE) fits &lt;- c() for (deg in 1:3) { fits &lt;- cbind(fits, c( lm(E1~stats::poly(x=time, degree=deg, raw=TRUE), d[1:(deg+2),])$coefficients[2], lm(E2~stats::poly(x=time, degree=deg, raw=TRUE), d[1:(deg+2),])$coefficients[2], lm(E3~stats::poly(x=time, degree=deg, raw=TRUE), d[1:(deg+2),])$coefficients[2]) ) } # Above: fully qualifying the stats::poly function, because I have loaded the &quot;signal&quot; # package which contains another &quot;poly&quot; function rownames(fits) &lt;- paste(&#39;E&#39;, 1:3, sep=&#39;&#39;) colnames(fits) &lt;- c(&#39;poly1&#39;,&#39;poly2&#39;,&#39;poly3&#39;) fits ## poly1 poly2 poly3 ## E1 0.007116667 0.008396667 0.009251587 ## E2 0.014750000 0.016965000 0.018663889 ## E3 0.033250000 0.041141667 0.043980159 It shows that the initial rate estimates increase with the order of the fitted polynomial, and that this can make a dramatic difference. Which approach is right? That depends on the noise in the data. Higher order polynomials adapt better to high curvature, but also to noise in the data. A detailed plot of the fits to E1 is shown below plot(E1~time, d[1:5,]) coefs &lt;- lm(E1~stats::poly(time,1,raw=TRUE),d[1:3,])$coefficients curve(coefs[1]+coefs[2]*x, add=TRUE, col=&#39;red&#39;) coefs &lt;- lm(E1~stats::poly(time,2,raw=TRUE),d[1:4,])$coefficients curve(coefs[1]+coefs[2]*x+coefs[3]*x^2, add=TRUE, col=&#39;darkgreen&#39;) coefs &lt;- lm(E1~stats::poly(time,3,raw=TRUE),d[1:5,])$coefficients curve(coefs[1]+coefs[2]*x+coefs[3]*x^2+coefs[4]*x^3, add=TRUE, col=&#39;blue&#39;) It shows that a straight line is likely to underestimate the initial rate, and that it fits badly to the three points. However, whether a third order polynomial is a better fit than a second order polynomial can not be answered without additional data. For that we would need to obtain more information about the technical error in the measurement. We will now attack the second problem. The first thing we need to know is that we are actually not interested in the absolute rate of biomass growth, but in the specific growth rate (called \\(\\mu\\)), i.e. the rate of biomass growth per amount of biomass. This is a more general measure of growth of a microbial culture, because all individuals in a culture contribute to growth at any moment in time. So clearly, the growth rate increases with the amount of cells, or the amount of biomass \\(x\\). Hence, we are actually interested in the quantity \\[ \\mu(t) = \\frac{1}{x(t)} \\cdot \\frac{d\\,x(t)}{d\\,t} \\] We still need to estimate \\(dx(t)/dt\\), but have to divide this quantity by \\(x(t)\\) to obtain an estimate of \\(\\mu(t)\\) at any time of the culture. A naive approach to this problem of numerical differentiation is to apply a discretized version of the mathematical operation of differentiation. Suppose you have a series of measurements of a variable \\(x\\) at different, regularly spaced time points, i.e. you have a series of ordered pairs \\((t_i, x_i)\\). Because of the regular interval, there is a constant difference \\(\\delta t\\) between consecutive time points. To calculate the rate at a certain time point \\(t_i\\), you could calculate \\[ \\frac{\\delta x}{\\delta t} = \\frac{x_{i+1} - x_i}{\\delta t} \\] To be more concrete, suppose we are interested in the first order derivative of the growth curve shown in . Apply the above formula to calculate the first order derivative of the curve and estimate \\(\\mu(t)\\). Use the function diff(). Also make a line-plot of the derivative (use plot() with argument type='l'). Show solution We can calculate the difference between consecutive OD measurements using the expression diff(d$OD). However, the resulting vector has one point less than the original data, hence, I chose to arbitrarily leave out the first data point to calculate \\(\\mu\\): f &lt;- file.path(baseurl, &quot;data/numdif/growth.tab&quot;) d &lt;- read.table(f, sep=&quot;\\t&quot;, head=TRUE) delta.x &lt;- diff(d$OD) delta.t &lt;- diff(d$time) plot(d$time[-1], (1/d$OD[-1])*(delta.x/delta.t), type=&#39;l&#39;, xlab=&quot;Time (h)&quot;, ylab=&quot;Specific growth rate (1/h)&quot;, ylim=c(-0.25,1.25)) There are two complications with this approach. The smaller complication is that the resulting first order derivative has one point less than the original data set. Hence, we are throwing away one OD measurement, which is arbitrarily the first or the last measurement. The quotient \\(\\delta x/\\delta t\\) is actually an estimate of the derivative half-way between the two bordering time-points. So, instead of throwing away an OD measurement, we could take the average between the two bordering measurements. Carry out this solution. Show solution Calculating the average of the two bordering OD measurements and using this as an estimate of the OD halfway between two time points: OD.halfway &lt;- (d$OD[-1] + d$OD[-length(d$OD)])/2 plot(d$time[-1], (1/OD.halfway)*(delta.x/delta.t), type=&#39;l&#39;, xlab=&quot;Time (h)&quot;, ylab=&quot;Specific growth rate (1/h)&quot;, ylim=c(-0.25,1.25)) The second, more serious complication is that the noise in \\(\\delta x/\\delta t\\) tends to be twice as large as the noise in \\(x\\), because we are subtracting two measurements of \\(x\\), each with their own measurement error. This is even more pronounced if you would calculate higher order derivatives. The noise becomes even worse if we divide \\(\\delta x/\\delta t\\) by \\(x\\) to obtain an estimate of the specific growth rate \\(\\mu\\). Then the error is amplified at low values of \\(x\\). A way to avoid amplifying noise is by calculating a smooth version of the growth curve before numerical differentiation. Fifty years ago Savitzky and Golay (1964) and Golay invented a method to carry out smoothing and calculate the first and higher order derivatives of experimental data in one go. The method is still applied a lot in data analysis. The idea is simple: take a window with an odd number of data points (say \\(N_{odd}\\) points) and calculate a polynomial fit through those points. The fitted position, first- and higher-order derivatives of the polynomial are an estimate of the value of the central measurement (at position \\(1+\\frac{N_{odd}-1}{2}\\) in the window) and of the first and higher-order derivatives at that point. By moving the window over all data points we can can calculate a smoothed curve and smoothed versions of first- and higher-order derivatives. For the right and left \\(\\frac{N_{odd}-1}{2}\\) points of the time-series that can not be in the centre of a window we can just take the first and last polynomial fits as estimates. Install the package signal, and study its sgolayfilt function. You see that this function does not require a time vector because it assumes that measurements are equally spaced. Under this assumption the calculations can be performed extremely efficiently using a single matrix multiplication. Apply the function to calculate and plot smoothed versions of the growth curve as well as of the specific growth rate. Experiment with the window size and degree of the polynomial fit. You have to apply the function twice, once with m=0 and once with m=1 to obtain smoothed fits of the original curve and of the first order derivative. Also realize that \\(\\delta t = 0.25\\) hours, so the time scaling factor ts should equal \\(0.25\\). Show solution # library(signal) layout(matrix(1:2, nrow=2)) sg0 &lt;- sgolayfilt(d$OD, p=3, n=9, m=0, ts=0.25) plot(OD~time, d) lines(d$time, sg0, col=&#39;red&#39;) sg1 &lt;- sgolayfilt(d$OD, p=3, n=9, m=1, ts=0.25) plot(d$time, sg1/sg0, type=&#39;l&#39;, xlab=&quot;Time (h)&quot;, ylab=&quot;Specific growth rate (1/h)&quot;, ylim=c(-0.25,1.25)) To estimate the maximum of bioluminescence, it is not sufficient to take the maximum of the measured values, because that would be equel to the true maximum plus the largest incidence of noise. Apply the same smoothing technique as above to obtain a robust estimate of the maximum of bioluminescence. Show solution Generally, we want to use a Savitzky-Golay filter with low degree of the polynomial, because a high degree polynomial will over-fit the data, i.e. it will also fit the noise. Trying a few window sizes and a polynomial of degree 2, I found the following compromise: b &lt;- read.table(file.path(baseurl, &quot;data/numdif/bioluminescence.tab&quot;), sep=&quot;\\t&quot;, head=TRUE) sg2 &lt;- sgolayfilt(b$luminescence, p=2, n=21, m=0, ts=0.167) plot(luminescence~time, b) lines(b$time, sg2, col=&#39;red&#39;) The robust maximum bioluminescence equals the maximum of the smoothed curve. We can compare that to the maximum measured value: max(b$luminescence) ## [1] 10065 max(sg2) ## [1] 9089.126 You may have noticed that bioluminescence and OD were not measured at the same frequency. Therefore, we have to interpolate either of the variables at the time point of the other variable to be able to calculate a correct ratio. Here, interpolate the Savitzky-Golay smoothed OD at the time points of the bioluminescence trace, using the functionapproxfun(). This is a special function that returns a function as its result. Entering a time value in this function will then return the interpolated OD value. After creating this function, calculate and plot the specific bioluminescence, i.e. the bioluminescence per amount of cells (here measured in OD units). Show solution # creating a function that returns an interpolated OD value based on the Savitzky-Golay smoothed OD time-series interpOD &lt;- approxfun(d$time,sg0) # the object interpOD is a function: class(interpOD) ## [1] &quot;function&quot; # calculating and plotting the ratio of the snoothed bioluminescence and the interpolated smoothed OD: plot(b$time, sg2/interpOD(b$time), type=&#39;b&#39;, xlab=&#39;Time (h)&#39;, ylab=&#39;Specific bioluminescence&#39;) References "],
["randomnumbers.html", "CHAPTER 23 Random numbers", " CHAPTER 23 Random numbers Random numbers are extremely important in current statistical and data analysis as well as in society. For example: Lotteries depend on random numbers. Nobody should want to participate in lotteries that are biased. Cryptographical algorithms are based on random numbers. Distribution sampling functions and randomization tests depend on random numbers. The research and characterization of complex distributions (Monte Carlo simulation) depends on random numbers. It is said that some professors determine grades using random numbers. Random number generators (RNG’s) are an extensive branch of research in computer science. A whole 200-page chapter of Knuth (2011), websites, books and papers are devoted to it. There are two types of RNG’s: programmable RNG’s (PRNG) and true RNG’s (TRNG). TRNG’s are based on physical measurements of processes that are thought to have a random nature, like radioactive decay, whereas PRNG’s are deterministic algorithms that calculate a sequence of numbers that have the appearance of randomness. An RNG should pass a number of randomness tests. For example, they should yield a uniform distribution of numbers within their number domain, and the sequence in which the random numbers appear should not have predicable characteristics, given that the observer is unaware of the underlying generating algorithm. The most practical RNG’s for daily work are PRNG’s. The default RNG used by R is currently (R version 3.4.3) the so-called “Mersenne Twister” by Matsumoto and Nishimura (1998). It is a state-of-the-art PRNG and considered to be among the best. It has passed all standard tests of randomness. This RNG is called by every function in R that generates random samples. This includes all the rdist() type functions that produce random draws from particular distributions. There is no guarantee that random numbers from an PRNG will show no regularities when performing particular tests. If you ever doubt the randomness of a random number generator, try a different algorithm or a TRNG, like the one from random.org that uses atmosperic noise to generate random numbers (can we trust atmospheric noise to be random?), or fourmilab’s hotbits, that uses a radioactive source. These TRNG sources are not suitable for regular use when you need many random numbers, but for making checks their throughput might suffice. The difficulty with TRNG’s is that technical problems causing their putative non-randomness are often extremely difficult to pinpoint, hence also here, there is no guarantee that these numbers will be truly random. The quality of these TRNG’s depends on regular testing, in principle using the same techniques as for PRNG’s. So, you should not see TRNG sources as the ultimate truth, but as another source of random numbers, at worst with a different bias than your PRNG. Because PRNG’s are deterministic, they generate a fixed sequence of numbers. To avoid generating the same sequence over and over again upon calling, programs that use PRNG’s keep track of the state of the RNG (the current random number) and use this as input to generate a new sequence upon the next call. In this way, every time you call for example the function sample(1:100,2) to choose two random integers from 1..100, you will obtain a different set. Often, you want to generate exactly the same sequence of random numbers again, for example to exactly reproduce a result in different R-sessions. The way to do that is by (re-)setting the so-called seed of the random number generator to a particular value, for example set.seed(23). This must be done before calling the function that uses the global random number generator function. The sequence set.seed(3) sample(1:100, 2) ## [1] 17 80 will generate the same set over and over again, because it starts at the same state of the RNG over and over again. It should (if you have the same version of R, in particular of the RNG algorithm) even give the same result on your as on my PC. Note that the state of the RNG funcion working at the background is changed every time that a sampling function calls it. So, calling the same function twice, without resetting the seed, generates different samples: set.seed(726) a &lt;- sample(1:100, 2) b &lt;- sample(1:100, 2) identical(a, b) yields FALSE, but set.seed(726) a &lt;- sample(1:100, 2) set.seed(726) b &lt;- sample(1:100, 2) identical(a, b) yields TRUE, and clearly, set.seed(726) a &lt;- sample(1:100, 2) set.seed(24) b &lt;- sample(1:100, 2) identical(a, b) must yield FALSE. References "],
["references.html", "References", " References "]
]
