---
title: "Assignment_2"
author: "Clara Ferrer Castellà"
date: "4 de mayo de 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Before start:

* Clean the Environment
```{r}
rm(list=ls())
```

* Load Packages
```{r}

library("ggplot2")
library("grid")
library("gridExtra")
library("scales")
library("plyr")
library("corrplot")
library("gbm")
library("caret")
library("tidyr")
library("dplyr")
library("unbalanced")
library("randomForest")


```


* Functions definition
```{r}
# Function to create multiplots
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}




#Scoring and Ranking functions
rank_prediction <- function(df){
    "
    Ranks search IDs according to prediction scores

    The dataframe given as a parameter 
    must contain the following columns
    in order to rank:
        - srch_id (the search IDs)
        - prediction (the prediction per row (higher score -> better rank))

    Other important columns are:
        - prop_id (for sorting the property IDs for the prediction file)
        - relevance (relevance score for calculating the NDCG of the test set)
    "
    
    search_ids <- sort(unique(df[,"srch_id"]))
    searches <- df[,"srch_id"]
    
    res <- data.frame()
    for (i in search_ids){
        group <- df[searches == i,]
        res <- rbind(res,group[order(group$prediction,decreasing=TRUE),])
    }
    
    #rownames(res) <- NULL
    
    return(res)
}

ndcg_score <- function(df){
    "
    Calculates the average ndcg score for a 
    given ensemble of ranked searches.

    The dataframe given as a parameter 
    must contain the following columns
    in order to calculate the ndcg score:
        - srch_id (the search IDs)
        - relevance (relevance score for calculating the NDCG of the test set)
    "
    
    search_ids <- sort(unique(df[,"srch_id"]))
    searches <- df[,"srch_id"]
    
    ndcg_scores <- rep(0,length(search_ids))
    for (i in 1:length(search_ids)){
        group <- df[searches == i,]
        
        dcg <- group[,"relevance"] / log2((1:nrow(group))+1)
        norm <- group[order(group$relevance,decreasing=TRUE),"relevance"] / log2((1:nrow(group))+1)
        
        ndcg_scores[i] <- sum(dcg) / sum(norm)
    }

    return(mean(ndcg_scores))
}




```


* Set path and load the data
```{r}
# Stablishing the working directory and the file paths 
Working_directory <- setwd("~/DM_Basic_Assignment/Assignment_2")
d <- unzip("Assignment2_data.zip")
file_training <- file.path(Working_directory, "Data Mining VU data" , "full_train.csv", "full_train.csv")
file_test <- file.path(Working_directory, "Data Mining VU data" , "test_set_VU_DM_2014.csv")



# Opening the files 
all_train <- read.csv(file_training, header = TRUE)
#test <- read.csv(file_test, header = TRUE)


```


<!-- * Factorize variables -->
<!-- ```{r} -->
<!-- factor_cols <- unlist(lapply(all_train, is.factor)) -->
<!-- factor_cols["date_time"] <- FALSE -->

<!-- #train[factor_cols] <- as.numeric(as.character(train[factor_cols])) -->

<!-- for (name in colnames(all_train[factor_cols])){ -->
<!--     all_train[name] <- as.numeric(sub('NULL',NA,as.character(all_train[name][,1]))) -->
<!-- } -->
<!-- ``` -->



##Visualize the Data

* Visualization of the NA Data of the new hotels 

```{r}
# Change the NAs to character to be taken into account in the plot
all_train$prop_review_score[is.na(all_train$prop_review_score)] <- "NA"

# Plot of the Hotel rating score for all hotels
p1 <- ggplot(all_train, aes(x=prop_review_score)) + geom_bar() + ggtitle("A") + xlab("Hotel Review Score")

# Plot of the score of the booked hotels, to see if the users prefer a new hotel (NA) or a hotel with the woorst case scenario (0)
pre_NA_visual <-table(Rating=all_train$prop_review_score, Booked= as.logical(all_train$booking_bool))
pre_NA_visual <- as.data.frame(prop.table(pre_NA_visual, 1)) # row percentages 
NA_visual <- pre_NA_visual[pre_NA_visual$Rating=="NA"| pre_NA_visual$Rating=="0"| pre_NA_visual$Rating=="1",]
NA_visual <- NA_visual[NA_visual$Booked==TRUE,]

p2 <- ggplot(NA_visual, aes(x = Rating, y = Freq)) + 
  geom_bar(stat = "identity", position = position_dodge()) + 
  ggtitle("B") + xlab("Hotel Review Score")

multiplot(p1, p2, cols=2)

# Remove useless Data from Environment
rm(NA_visual, p1, p2, pre_NA_visual)
```



* Visualization of the unbalanced data
```{r}
# Create the ignored column
all_train$ignored_bool <- ifelse(all_train$click_bool == 0 & all_train$booking_bool == 0, 1, 0)

balance_visual <- data.frame(Type = c("click","book", "ignored"), Value=c(sum(all_train$click_bool==1), sum(all_train$booking_bool==1), sum(all_train$ignored_bool==1)))

ggplot(balance_visual, aes(x=Type, y=Value, fill=Type)) + 
  geom_histogram(stat="identity")

# balance_visual <- data.frame(Yes=c(sum(train$click_bool==1), sum(train$booking_bool==1), sum(train$ignored_bool==0)), No = c(sum(train$click_bool==0), sum(train$booking_bool==0), sum(train$ignored_bool==1)), row.names = c("click","book", "ignored"))
# 
# balance_visual <- as.matrix(balance_visual)
# balance_visual <- as.table(balance_visual)
# 
# percentage_balance <- as.data.frame(prop.table(balance_visual, 1)*100) # row percentages 
# 
# 
# ggplot(percentage_balance, aes(x = Var2, y = Freq, fill = Var1)) + 
#   geom_bar(stat = "identity", position = position_dodge())

# Remove useless Data from Environment
rm(balance_visual,NA_visual,pre_NA_visual)
```

<!-- ## Add relevance targets -->
<!-- ```{r} -->
<!-- all_train$relevance <- numeric(nrow(all_train)) -->

<!-- booked <- all_train[,"booking_bool"] == 1 -->
<!-- clicked <- !booked & (all_train[,"click_bool"] == 1) -->

<!-- all_train$relevance[booked] <- 5 -->
<!-- all_train$relevance[clicked] <- 1 -->

<!-- ``` -->


## Calculate the percentage of NAs in a column
```{r}
# Get NA value booleans
nas <- is.na(all_train)

len.nas <- length(nas[,1])

# Initialize NA vector
na.vec <- vector(mode="integer",length=length(colnames(all_train)))

# Sum the NA values and fill NA vector
for (i in 1:length(na.vec)){
    na.vec[i] <- sum(nas[,i]) / len.nas
}

# Create bar plot Data Frame
na.df <- data.frame(colnames(all_train),na.vec)
colnames(na.df) <- c("attribute","missing_values")

# Sort by number of missing values
position <- arrange(na.df,missing_values)["attribute"][,1]



```


##Split of the data 
```{r}
#Group by search ID to then split the test set
all_train <- all_train %>%
  group_by(srch_id)

#Split of the test set
all_length <- nrow(all_train)
splity <- all_length*0.15
splity <- 743764 # Adjust cutoff depending on the srch_id 
test <- all_train[1:splity,]
splited_train <- all_train[(splity+1):all_length,]
rm(all_train) 

#Split the validation set
dt <- sort(sample(nrow(splited_train), nrow(splited_train)*.82353)) #to have 15% of the real_train
train <- splited_train[dt,]
validation <- splited_train[-dt,]

# Remove useless Data from Environment
rm(splited_train, dt)
rm(splity, all_length)
```



##Import data with imputed NA values 
```{r}
# Stablishing the working directory and the file paths 
Working_directory <- setwd("~/DM_Basic_Assignment/Assignment_2")
mod_train <- file.path(Working_directory, "modified_splits", "mod_train.csv")
mod_test <- file.path(Working_directory, "modified_splits", "mod_test.csv")
mod_validation <- file.path(Working_directory, "modified_splits", "mod_valid.csv")


# Opening the files 
train <- read.csv(mod_train, header = TRUE)
test <- read.csv(mod_test, header = TRUE)
validation <- read.csv(mod_validation, header = TRUE)

```



## Data Balance 
```{r}

# Balance the Train
train$ignored_bool <- ifelse(train$click_bool == 0 & train$booking_bool == 0, 0, 1) # Change previous 1 to 0, then this will be changed again, it's only in order to be able to run the function ubBalance()

output <- as.factor(train$ignored_bool) 
input <- train

data <- ubBalance(X=input, Y=output, type = "ubUnder", perc = 40) #apply undersampling
underTrain <- data.frame(data$X) #undersampled dataset





#Visualizations to compare data unbalanced and balanced
unbalance_visual <- data.frame(Type = c("Click","Book", "Ignored"), Value=c(sum(train$click_bool==1), sum(train$booking_bool==1), sum(train$ignored_bool==0)))

p1 <- ggplot(unbalance_visual, aes(x=Type, y=Value, fill=Type)) + 
  geom_histogram(stat="identity", show.legend = FALSE) +
  ggtitle("Unbalanced Data")

balance_visual <- data.frame(Type = c("Click","Book", "Ignored"), Value=c(sum(underTrain$click_bool==1), sum(underTrain$booking_bool==1), sum(underTrain$ignored_bool==0)))  # We'll only do one plot since it's already representative of how the other splits(validation and test) are balanced. 

p2 <- ggplot(balance_visual, aes(x=Type, y=Value, fill=Type)) + 
  geom_histogram(stat="identity", show.legend = FALSE) +
  ggtitle("Balanced Data")

multiplot(p1, p2, cols=2)





# Validation Balance 
validation$ignored_bool <- ifelse(validation$click_bool == 0 & validation$booking_bool == 0, 0, 1) # Change previous 1 to 0, then this will be changed again, it's only in order to be able to run the function ubBalance()
output <- as.factor(validation$ignored_bool) 
input <- validation

data <- ubBalance(X=input, Y=output, type = "ubUnder", perc = 40) #apply undersampling
underValidation <- data.frame(data$X) #undersampled dataset





# Test Balance
test$ignored_bool <- ifelse(test$click_bool == 0 & test$booking_bool == 0, 0, 1) # Change previous 1 to 0, then this will be changed again, it's only in order to be able to run the function ubBalance()
output <- as.factor(test$ignored_bool) 
input <- test

data <- ubBalance(X=input, Y=output, type = "ubUnder", perc = 40) #apply undersampling
underTest <- data.frame(data$X) #undersampled dataset





# Remove useless Data from Envirinment
rm(balance_visual, data, input, unbalance_visual, p1, p2, output, train, validation, test)





# Change values 0 to 1 of the "ignored_bool" variable in the new balanced data sets
underTrain$ignored_bool <- ifelse(underTrain$click_bool == 0 & underTrain$booking_bool == 0, 1, 0) 
underTest$ignored_bool <- ifelse(underTest$click_bool == 0 & underTest$booking_bool == 0, 1, 0) 
underValidation$ignored_bool <- ifelse(underValidation$click_bool == 0 & underValidation$booking_bool == 0, 1, 0)





# Create csv files of the balanced data sets (underTrain, underTest, underValidation)

write.csv(underTrain, file = "underTrain.csv", col.names = TRUE, row.names=FALSE)
write.csv(underTest, file = "underTest.csv", col.names = TRUE, row.names=FALSE)
write.csv(underValidation, file = "underValidation.csv", col.names = TRUE, row.names=FALSE)





```




# Set path and load the balanced data sets (underTrain, underTest, underValidation)
```{r}

file_underTrain <- file.path(Working_directory, "underTrain.csv")
underTrain <- read.csv(file_underTrain, header = TRUE)

file_underTest <- file.path(Working_directory, "underTest.csv")
underTest <- read.csv(file_underTest, header = TRUE)
 
# file_underValidatioin <- file.path(Working_directory, "underValidation.csv")
# underValidation <- read.csv(file_underValidatioin, header = TRUE)

file_na_df <- file.path(Working_directory, "na_df.csv")
na.df <- read.csv(file_na_df, header = TRUE)

```



##Data Processing for the Random Forest
```{r}
#Select columns with less than a 75% of NAs
variables_without_nas <- na.df$attribute[na.df$missing_values<=0.75]
no.na_train <- underTrain[as.character(variables_without_nas)]

#Delete variables that are not in the test set
rf_train <- subset(no.na_train, select=c(-srch_id,-position,-click_bool,-booking_bool, -ignored_bool))
rf_train <- as.matrix(rf_train[1:100000,])
#rf_train2 <- rf_train[1:100,]

```



##Random Forest model
```{r}

# Set seed for reproducibility
set.seed(111)

# Apply the Random Forest Algorithm
rf_model <- randomForest(relevance ~ ., data=rf_train, importance=TRUE, ntree = 700)
varImpPlot(rf_model)
pred_forest_vali <- predict(rf_model, newdata = underTest, type = "class")


df <- data.frame(srch_id= underTest$srch_id, prop_id=underTest$prop_id, prediction=pred_forest_vali, relevance=underTest$relevance)

ranking <- rank_prediction(df)

ndcg_score(ranking)





```

##Data Processing for Logistic regression
```{r}
#Convert variables to type factor 
rf_train2$relevance <- as.factor(rf_train2$relevance)
```


#Logistic regression
```{r}
# Logistics Regression
#glm.fit <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Smarket, family = binomial)
glm.fit <- glm(relevance ~ . , data = rf_train2, family = binomial)

summary(glm.fit)

pred_logi <- predict(glm.fit, newdata = underTest, type = "response")

df <- data.frame(srch_id= underTest$srch_id, prop_id=underTest$prop_id, prediction=pred_logi, relevance=underTest$relevance)

ranking <- rank_prediction(df)

ndcg_score(ranking)

```



